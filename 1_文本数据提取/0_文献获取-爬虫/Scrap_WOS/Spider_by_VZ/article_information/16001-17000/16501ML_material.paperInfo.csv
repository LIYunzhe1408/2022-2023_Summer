,title,Volume,Issue,Pages,whole__author_name,simply_author_name,reprint author,DOI,reprint address,Abstract,Keywords,Document Type,Publisher,Research Domain,Published Date,impact_factor,Keywords_plus,joural,pdf_link,Download_SuccessOrDefeat
1,Few-shot hypercolumn-based mitochondria segmentation in cardiac and outer hair cells in focused ion beam-scanning electron microscopy (FIB-SEM) data,128,,521-528,"Dietlmeier Julia,McGuinness Kevin,Rugonyi Sandra,Wilson Teresa,Nuttall Alfred,O'Connor Noel E.","Dietlmeier J,McGuinness K,Rugonyi S,Wilson T,Nuttall A,O'Connor NE",Dietlmeier J,10.1016/j.patrec.2019.10.031,Dublin City University,"We present a novel AI-based approach to the few-shot automated segmentation of mitochondria in large-scale electron microscopy images. Our framework leverages convolutional features from a pre-trained deep multilayer convolutional neural network, such as VGG-16. We then train a binary gradient boosting classifier on the resulting high-dimensional feature hypercolumns. We extract VGG-16 features from the first four convolutional blocks and apply bilinear upsampling to resize the obtained maps to the input image size. This procedure yields a 2688-dimensional feature hypercolumn for each pixel in a 224 x 224 input image. We then apply L-1-regularized logistic regression for supervised active feature selection to reduce dependencies among the features, to reduce overfitting, as well as to speed-up gradient boosting-based training. During inference we block process 1728 x 2022 large microscopy images. Our experiments show that in such a formulation of transfer learning our processing pipeline is able to achieve high-accuracy results on very challenging datasets containing a large number of irregularly shaped mitochondria in cardiac and outer hair cells. Our proposed few-shot training approach gives competitive performance with the state-of-the-art using far less training data. (C) 2019 Elsevier B.V. All rights reserved.","Deep learning,Few-shot learning,Cardiac and outer hair cells,Gradient boosting,Mitochondria segmentation,Transfer learning",Article,"ELSEVIER, RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS",Computer Science,,3.615,IMAGES,PATTERN RECOGNITION LETTERS,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7450765,
2,Quantification of Soil Organic Carbon in Biochar-Amended Soil Using Ground Penetrating Radar (GPR),11,23,,"Shen Xiaoqing,Foster Tyler,Baldi Heather,Dobreva Iliyana,Burson Byron,Hays Dirk,Tabien Rodante,Jessup Russell","Shen XQ,Foster T,Baldi H,Dobreva I,Burson B,Hays D,Tabien R,Jessup R",Jessup R,10.3390/rs11232874,Texas A&M University System,"The application of biochar amendments to soil has been proposed as a strategy for mitigating global carbon (C) emissions and soil organic carbon (SOC) loss. Biochar can provide additional agronomic benefits to cropping systems, including improved crop yield, soil water holding capacity, seed germination, cation exchange capacity (CEC), and soil pH. To maximize the beneficial effects of biochar amendments towards the inventory, increase, and management of SOC pools, nondestructive analytical methods such as ground penetrating radar (GPR) are needed to identify and quantify belowground C. The use of GPR has been well characterized across geological, archaeological, engineering, and military applications. While GPR has been predominantly utilized to detect relatively large objects such as rocks, tree roots, land mines, and peat soils, the objective of this study was to quantify comparatively smaller, particulate sources of SOC. This research used three materials as C sources: biochar, graphite, and activated C. The C sources were mixed with sand-12 treatments in total-and scanned under three moisture levels: 0%, 10%, and 20% to simulate different soil conditions. GPR attribute analyses and Naive Bayes predictive models were utilized in lieu of visualization methods because of the minute size of the C particles. Significant correlations between GPR attributes and both C content and moisture levels were detected. The accuracy of two predictive models using a Naive Bayes classifier for C content was trivial but the accuracy for C structure was 56%. The analyses confirmed the ability of GPR to identify differences in both C content and C structure. Beneficial future applications could focus on applying GPR across more diverse soil conditions.","ground penetrating radar,biochar,attribute analysis,machine learning",Article,"MDPI, ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND","Environmental Sciences & Ecology,Geology,Remote Sensing,Imaging Science & Photographic Technology",,5.353,MORPHOLOGY,REMOTE SENSING,https://www.mdpi.com/2072-4292/11/23/2874/pdf,
3,Heterogeneous sensor-based condition monitoring in directed energy deposition,30,,,"Montazeri Mohammad,Nassar Abdalla R.,Stutzman Christopher B.,Rao Prahalada","Montazeri M,Nassar AR,Stutzman CB,Rao P",Rao P,10.1016/j.addma.2019.100916,University of Nebraska System,"The objective of this work is to detect in situ the occurrence of lack-of-fusion defects in titanium alloy (Ti-6Al-4V) parts made using directed energy deposition (DED) additive manufacturing (AM). We use data from two types of in-process sensors, namely, a spectrometer and an optical camera which are integrated into an Optomec MR-7 DED machine. Both sensors are focused on capturing the dynamic phenomena around the melt pool region. To detect lack-of-fusion defects, we fuse (combine) the data from the in-process sensors invoking the concept of Kronecker product of graphs. Subsequently, we use the features derived from the graph Kronecker product as inputs to a machine learning algorithm to predict the severity (class or level) of average length of lack-of-fusion defects within a layer, which is obtained from offline X-ray computed tomography of the test parts. We demonstrate that the severity of lack-of-fusion defects is classified with statistical fidelity (F-score) close to 85% for a two-level classification scenario, and approximately 70% for a three-level classification scenario. Accordingly, this work demonstrates the use of heterogeneous in-process sensing and online data analytics for in situ detection of defects in DED metal AM process.","Directed energy deposition,Lack-of-fusion defects,In-process optical emission spectroscopy,Plume imaging,Kronecker graph product,Sensor data fusion",Article,"ELSEVIER, RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS","Engineering,Materials Science",,12.363,"DIRECT,LASER,DEPOSITION,REGRESSION,SIGNAL",ADDITIVE MANUFACTURING,,
4,Learning image-based spatial transformations via convolutional neural networks: A review,64,,142-153,"Tustison Nicholas J.,Avants Brian B.,Gee James C.","Tustison NJ,Avants BB,Gee JC",Tustison NJ,10.1016/j.mri.2019.05.037,University of Virginia,"Recent methodological innovations in deep learning and associated advancements in computational hardware have significantly impacted the various core subfields of quantitative medical image analysis. The generalizability, computational efficiency and open-source availability of deep learning algorithms and related software, particularly those utilizing convolutional neural networks, have produced paradigm shifts within the field. This impact is evident from topical prevalence in the literature, conference and workshop themes and winning methodologies in relevant competitions. In this work, we review the various state-of-the-art approaches to learning and prediction and/or optimizing image transformations using convolutional neural networks. Although of primary importance within the quantitative imaging domain, image registration algorithmic development, in the context of these deep learning strategies, has received comparatively less attention than its counterparts (e.g., image segmentation). Nevertheless, significant progress has been made in this particular subfield which has been presented in various research venues. We contextualize these contributions within the broader scope of deep learning advancements and, in so doing, attempt to facilitate the leveraging and further development of such techniques within the medical imaging research community.","ConvNets,Deep learning,Diffeomorphisms,Image registration,Spatial normalization",Review,"ELSEVIER SCIENCE INC, STE 800, 230 PARK AVE, NEW YORK, NY 10169 USA","Radiology, Nuclear Medicine & Medical Imaging",,2.608,"PATTERN-RECOGNITION,REGISTRATION,DIFFEOMORPHISMS,MECHANISM,FRAMEWORK,AFFINE,MODEL",MAGNETIC RESONANCE IMAGING,,
5,Multi-distance support matrix machines,128,,237-243,"Ye Yunfei,Han Dong","Ye YF,Han D",Ye YF,10.1016/j.patrec.2019.09.004,Shanghai Jiao Tong University,"Real-world data such as digital images, MRI scans and electroencephalography signals are naturally represented as matrices with structural information. Most existing classifiers aim to capture these structures by regularizing the regression matrix or introducing factorization technique. In this paper, we propose a multi-distance support matrix machine (MDSMM), which formulates the optimization problem by introducing the concept of multi-distance. Unlike traditional matrix-based classifiers, the proposed approach uses a vector-based distance to quantify the cost function and penalty function. We further study the generalization bounds for i.i.d. processes and non i.i.d. processes based on different classifiers. For typical hypothesis classes where matrix norms are constrained, MDSMM achieves a faster learning rate than conventional methods. We demonstrate the merits of the proposed approach by conducting comparative experiments on both simulation study and a number of real-world datasets. (C) 2019 Elsevier B.V. All rights reserved.","Multi-distance support matrix machine,Generalization bounds,Rademacher complexity,Vapnik-Chervonenkis dimension",Article,"ELSEVIER, RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS",Computer Science,,3.615,,PATTERN RECOGNITION LETTERS,http://arxiv.org/pdf/1807.00451,
6,Automatic depression classification based on affective read sentences: Opportunities for text-dependent analysis,115,,1-14,"Stasak Brian,Epps Julien,Goecke Roland","Stasak B,Epps J,Goecke R",Stasak B,10.1016/j.specom.2019.10.003,University of New South Wales Sydney,"In the future, automatic speech-based analysis of mental health could become widely available to help augment conventional healthcare evaluation methods. For speech-based patient evaluations of this kind, protocol design is a key consideration. Read speech provides an advantage over other verbal modes (e.g. automatic, spontaneous) by providing a clinically stable and repeatable protocol. Further, text-dependent speech helps to reduce phonetic variability and delivers controllable linguistic/affective stimuli, therefore allowing more precise analysis of recorded stimuli deviations. The purpose of this study is to investigate speech disfluency behaviors in non-depressed/depressed speakers using read aloud text containing constrained affective-linguistic criteria. Herein, using the Black Dog Institute Affective Sentences (BDAS) corpus, analysis demonstrates statistically significant feature differences in speech disfluencies, whereby when compared to non-depressed speakers, depressed speakers show relatively higher recorded frequencies of hesitations (55% increase) and speech errors (71% increase). Our study examines both manually and automatically labeled speech disfluency features, demonstrating that detailed disfluency analysis leads to considerable gains, of up to 100% in absolute depression classification accuracy, especially with affective considerations, when compared with the affect-agnostic acoustic baseline (65%).","Digital phenotyping,Digital medicine,Paralinguistics,Machine learning,Speech elicitation,Valence",Article,"ELSEVIER, RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS","Acoustics,Computer Science",,2.039,"PSYCHOMOTOR,RETARDATION,COGNITIVE-PROCESSES,PAUSE-TIME,SPEECH,INFORMATION,INDICATORS,EFFORTFUL,SEVERITY",SPEECH COMMUNICATION,,
7,Revisiting heater size sensitive pool boiling critical heat flux using neural network modeling: Heater length of the half of the Rayleigh-Taylor Instability Wavelength maximizes CHF,14,,,"He Mingfu,Lee Youho","He MF,Lee Y",Lee Y,10.1016/j.tsep.2019.100421,Seoul National University (SNU),"This study extensively collected pool boiling CHF data of atmospheric water on smooth square surfaces. General Regression Neural Network (GRNN) was applied to investigate heater size and material property sensitive CHF. It was confirmed that if the side length is sufficiently large, CHF becomes independent of the surface area of plate. For all materials, the heater side length of similar to 8 mm, which is equal to the half of the Rayleigh-Taylor Instability Wavelength, gives the maximum CHF. This implies that the optimal surface size for CHF maximization is dominantly affected by the far-field fluid properties, instead of the near field surface characteristics. Application of GRNN for the collected data confirms the sanity of the asymptotically increasing CHF behavior with respect to increasing Thermal Activity in pool boiling.","Critical heat flux,Pool boiling,Rayleigh-Taylor instability wavelength,Machine learning,Neural network",Article,"ELSEVIER, RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS","Thermodynamics,Energy & Fuels,Engineering,Mechanics",,,"PIN-FINNED,SURFACES,HYDRODYNAMIC,MODEL,CONTACT-ANGLE,ENHANCEMENT,WATER,THICKNESS,GRAVITY,EFFUSIVITY,NANOFLUIDS,PREDICTION",THERMAL SCIENCE AND ENGINEERING PROGRESS,,
8,Predictive biomarkers of colorectal cancer,83,,,"Ding Di,Han Siyu,Zhang Hui,He Ye,Li Ying","Ding D,Han SY,Zhang H,He Y,Li Y",Li Y,10.1016/j.compbiolchem.2019.107106,Jilin University,"Colorectal cancer is one of the top leading causes of cancer mortality worldwide, especially in China. However, most of the current treatments are invasive and can only be applied to very few cancers. The earlier a malignant tumor is diagnosed, the higher the patient's survival rate. In this study, we proposed a computational framework to identify highly-reliable and easierly-detectable biomarkers capable of secreting into blood, urine and saliva by integrating transcriptomics and proteomics data at the system biology level. First, a large number of transcriptome data were processed to identify candidate biomarkers for colorectal cancer. Second, three classified models are constructed to predict biomarkers for colorectal cancer capable of secreting into blood, urine and saliva, which are effective disease diagnosis media to facilitate clinical screening. Then biological functions and molecular mechanisms of the candidate biomarkers of colorectal cancer are inferred utilizing multi-source biological knowledge and literature mining. Furthermore, the classification power of different combinations of candidate biomarkers is verified by machine learning models. In addition, the targeted drugs of the predicted biomarkers are further analyzed to provide assistance for clinical treatment of colorectal cancer. In this paper, our proposed computational model not only provides the effective candidate biomarkers ESM1, CTHRC1, AZGP1 for colorectal cancer capable of secreting into blood, urine and saliva, but also helps to understand the molecular mechanism of colorectal cancer. This computational framework can span the huge gap between transcriptome and proteomics, which can easily be applied to the biomarker research for other types of tumor.","Cancer biomarkers,Colorectal cancer,Differential expressed genes,Saliva,Blood,Urine,Enrichment analysis",Article,"ELSEVIER SCI LTD, THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND","Life Sciences & Biomedicine - Other Topics,Computer Science",,,"EXPRESSION,PROTEINS,DATABASE",COMPUTATIONAL BIOLOGY AND CHEMISTRY,,
9,EMG-based lumbosacral joint compression force prediction using a support vector machine,74,,115-120,"Li Simon S. W.,Chu Carlin C. F.,Chow Daniel H. K.","Li SSW,Chu CCF,Chow DHK",Chow DHK,10.1016/j.medengphy.2019.09.009,Education University of Hong Kong (EdUHK),"Electromyography-assisted optimization (EMGAO) approach is widely used to predict lumbar joint loads under various dynamic and static conditions. However, such approach uses numerous anthropometric, kinematic, kinetic, and electromyographic data in the computation process, and thus makes data collection and processing complicated. This study developed an electromyography-based support vector machine (EMGB_SVM) approach for predicting lumbar spine load during walking with backpack loads. The EMGB_SVM is simple and uses merely the electromyographic data. Anthropometric information of 10 healthy male adults as well as their kinematic, kinetic, and electromyographic data acquired during walking exercises with no-load and with various backpack loads (5%, 10%, 15%, and 20% of their body weight) were used as the inputs of a biomechanical model, which was then used for predicting the lumbosacral joint compression force. The efficacy of the EMGB(-)SVM was investigated by comparing the force profiles obtained using this model with those obtained using the current EMGAO approach. On average, the EMGB_SVM obtained deviations in the peak and minimum forces of -3.3% and 5.1%, respectively, and a root mean square difference in the force profile of 7.5%. The EMGB(-)SVM is a comparable estimator in terms of its slight bias, favourable consistency, and efficiency at predicting the lumbosacral joint compression force. (C) 2019 IPEM. Published by Elsevier Ltd. All rights reserved.","Electromyography,Walking,Machine learning,Spinal loads,Trunk muscle",Article,"ELSEVIER SCI LTD, THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND",Engineering,,,"TRUNK,MUSCLE,FORCES,LUMBAR,SPINE,ASSISTED,OPTIMIZATION,LOADS,MODEL",MEDICAL ENGINEERING & PHYSICS,,
10,First-principles and machine learning predictions of elasticity in severely lattice-distorted high-entropy alloys with experimental validation,181,,124-138,"Kim George,Diao Haoyan,Lee Chanho,Samaei A. T.,Tu Phan,de Jong Maarten,An Ke,Ma Dong,Liaw Peter K.,Chen Wei","Kim G,Diao HY,Lee C,Samaei AT,Phan T,de Jong M,An K,Ma D,Liaw PK,Chen W",Chen W,10.1016/j.actamat.2019.09.026,Illinois Institute of Technology,"Stiffness usually increases with the lattice-distortion-induced strain, as observed in many nanostructures. Partly due to the size differences in the component elements, severe lattice distortion naturally exists in high entropy alloys (HEAs). The single-phase face-centered-cubic (FCC) Al0.3CoCrFeNi HEA, which has large size differences among its constituent elements, is an ideal system to study the relationship between the elastic properties and lattice distortion using a combined experimental and computational approach based on in-situ neutron-diffraction (ND) characterizations, and first-principles calculations. Analysis of the interatomic distance distributions from calculations of optimized special quasi random structure (SQS) found that the HEA has a high degree of lattice distortion. When the lattice distortion is explicitly considered, elastic properties calculated using SQS are in excellent agreement with experimental measurements for the HEA. The calculated elastic constant values are within 5% of the ND measurements. A comparison of calculations from the optimized SQS and the SQS with ideal lattice sites indicate that the lattice distortion results in the reduced stiffness. The optimized SQS has a bulk modulus of 177 GPa compared to the ideal lattice SQS with a bulk modulus of 194 GPa. Machine learning (ML) modeling is also implemented to explore the use of fast, and computationally efficient models for predicting the elastic moduli of HEAs. ML models trained on a large dataset of inorganic structures are shown to make accurate predictions of elastic properties for the HEA. The ML models also demonstrate the dependence of bulk and shear moduli on several material features which can act as guides for tuning elastic properties in HEAs. (C) 2019 Acta Materialia Inc. Published by Elsevier Ltd. All rights reserved.","First-principles calculation,Elastic constants,In situ tension test,Neutron diffraction,Machine learning",Article,"PERGAMON-ELSEVIER SCIENCE LTD, THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND","Materials Science,Metallurgy & Metallurgical Engineering",,9.277,"TOTAL-ENERGY,CALCULATIONS,CRACK,GROWTH-BEHAVIOR,FATIGUE,BEHAVIOR,MECHANICAL-PROPERTIES,TENSILE-STRENGTH,POISSONS,RATIO,SINGLE-CRYSTAL,AL,ADDITION,MICROSTRUCTURE,TEMPERATURE",ACTA MATERIALIA,,
11,Unsupervised learning of dislocation motion,181,,510-518,"Pagan Darren C.,Phan Thien Q.,Weaver Jordan S.,Benson Austin R.,Beaudoin Armand J.","Pagan DC,Phan TQ,Weaver JS,Benson AR,Beaudoin AJ",Pagan DC,10.1016/j.actamat.2019.10.011,Cornell University,"The unsupervised learning technique, locally linear embedding (LLE), is applied to the analysis of X-ray diffraction data measured in-situ during the uniaxial plastic deformation of an additively manufactured nickel-based superalloy. With the aid of a physics-based material model, we find that the lower-dimensional coordinates determined using LLE appear to be physically significant and reflect the evolution of the defect densities that dictate strength and plastic flow behavior in the alloy. The implications of the findings for future constitutive model development are discussed, with a focus on wider applicability to microstructure evolution and phase transformation studies during in-situ materials processing. (C) 2019 Acta Materialia Inc. Published by Elsevier Ltd. All rights reserved.","Plasticity,Machine learning,Additive manufacturing,X-ray diffraction,Nickel-based superalloy",Article,"PERGAMON-ELSEVIER SCIENCE LTD, THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND","Materials Science,Metallurgy & Metallurgical Engineering",,9.277,"NONLINEAR,DIMENSIONALITY,REDUCTION,X-RAY-DIFFRACTION,POWDER-BED,FUSION,STRAIN,EIGENMAPS,FLOW",ACTA MATERIALIA,https://www.sciencedirect.com/science/article/am/pii/S1359645419306718,
12,Machine learning models for the lattice thermal conductivity prediction of inorganic materials,170,,,"Chen Lihua,Huan Tran,Batra Rohit,Kim Chiho,Ramprasad Rampi","Chen LH,Tran H,Batra R,Kim C,Ramprasad R",Ramprasad R,10.1016/j.commatsci.2019.109155,University System of Georgia,"The lattice thermal conductivity (kappa(L)) is a critical property of thermoelectrics, thermal barrier coating materials and semiconductors. While accurate empirical measurements of kappa(L) are extremely challenging, it is usually approximated through computational approaches, such as semi-empirical models, Green-Kubo formalism coupled with molecular dynamics simulations, and first-principles based methods. However, these theoretical methods are not only limited in terms of their accuracy, but sometimes become computationally intractable owing to their cost. Thus, in this work, we build a machine learning (ML)-based model to accurately and instantly predict kappa(L) of inorganic materials, using a benchmark data set of experimentally measured kappa(L) of about 100 inorganic materials. We use advanced and universal feature engineering techniques along with the Gaussian process regression algorithm, and compare the performance of our ML model with past theoretical works. The trained ML model is not only helpful for rational design and screening of novel materials, but we also identify key features governing the thermal transport behavior in non-metals.","Lattice thermal conductivity,Inorganic materials",Editorial Material,"ELSEVIER, RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS",Materials Science,,3.222,"THERMOELECTRIC,PROPERTIES,ANHARMONICITY,MG2SI",COMPUTATIONAL MATERIALS SCIENCE,http://arxiv.org/pdf/1906.06378,
13,Prediction models for macro shrinkage of aluminum alloys based on machine learning algorithms,21,,,"Liao Hengcheng,Zhao Baojun,Suo Xiaojin,Wang Qigui","Liao HC,Zhao BJ,Suo XJ,Wang QG",Liao HC,10.1016/j.mtcomm.2019.100715,Southeast University - China,"Macro shrinkage percentage is one of the most important measures of the castability of alloys. To quickly and accurately predict the macro shrinkage of aluminum alloys is greatly meaningful before developing new components or new alloys. In this study, based on the dataset of macro shrinkage percentage of aluminum alloys obtained by our experiments, three algorithms, Support Vector Regression (SVR), General Regression Neural Network (GRNN) and Random Forest (RF), were adopted to construct the prediction models for macro shrinkage, respectively. K-fold cross-validation and analysis on stability and reliability of models indicate the RF model is the best one. Compared the prediction values of macro shrinkage percentage in binary, ternary and multicomponent aluminum alloys by RF model with the experiment data of this study and the data from literatures, the constructed model has been verified to have high prediction accuracy and reliability.","Aluminum alloys,Macro shrinkage,Prediction model",Article,"ELSEVIER, RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS",Materials Science,,3.145,"SOLIDIFICATION,MICROPOROSITY,REFINEMENT",MATERIALS TODAY COMMUNICATIONS,,
14,Classification of alveolar bone density using 3-D deep convolutional neural network in the cone-beam CT images: A 6-month clinical study,148,,,"Sorkhabi Majid Memarian,Khajeh Maryam Saadat","Sorkhabi MM,Khajeh MS",Sorkhabi MM,10.1016/j.measurement.2019.106945,"Roshdiyeh Higher Educ Inst, Fahmideh Sq, Tabriz, East Azarbaijan, Iran.","Background: Computer-based diagnoses are a crucial study in the medical image analyzing and machine learning technologies. The cone beam computed tomography (CBCT) modality provides three-dimensional bone models to extract an interactive treatment plan at relatively low radiation dose and cost. For the first time in this study, the evaluation of alveolar bone density was performed by a 3-D deep convolutional neural network (CNN) at the CBCT images. The trabecular pattern of the bone was recognized and classified.
Method: This study aimed to present a methodology which was implementing 3D voxel-wise feature evaluation within a convolutional neural network. We presented a three-dimensional CNN method that evaluated the alveolar bone density from CBCT volumetric data which could efficiently capture the trabecular pattern. In clinical trials, 207 surgery target areas of 83 patients have been selected. Clinical parameters were measured and evaluated during the surgery and a 6-month follow-up. These parameters were used to database labeling and evaluate the performance of the proposed technique.
Results: Our method achieved the average precision score of 84.63% and 95.20% in the hexagonal prism and the cylindrical voxel shapes respectively. Furthermore, the alveolar bone classification was performed in 76 ms. In comparison to the state-of-art approaches, the efficiency of the suggested algorithm was proved.
Conclusion: An automatic classification can improve the proficiency and certainty of the radiologic evaluation. The outcome of this research may help the dentists in the implant treatment from diagnosis to surgery. (C) 2019 Elsevier Ltd. All rights reserved.","3-D deep CNN,Alveolar bone classification,Dental implant,Computer-aided diagnosis",Article,"ELSEVIER SCI LTD, THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND","Engineering,Instruments & Instrumentation",,3.778,"RESONANCE,FREQUENCY-ANALYSIS,COMPUTED-TOMOGRAPHY,IMPLANT,STABILITY,DENTAL,IMPLANTS,IV,BONE,QUALITY,PERCEPTION,INSERTION,ACCURACY,CBCT",MEASUREMENT,,
15,Application of Artificial Neural Networks for Yield Modeling of Winter Rapeseed Based on Combined Quantitative and Qualitative Data,9,12,,"Niedbala Gniewko,Piekutowska Magdalena,Weres Jerzy,Korzeniewicz Robert,Witaszek Kamil,Adamski Mariusz,Pilarski Krzysztof,Czechowska-Kosacka Aneta,Krysztofiak-Kaniewska Anna","Niedbala G,Piekutowska M,Weres J,Korzeniewicz R,Witaszek K,Adamski M,Pilarski K,Czechowska-Kosacka A,Krysztofiak-Kaniewska A",Niedbala G,10.3390/agronomy9120781,Poznan University of Life Sciences,"Rapeseed is considered as one of the most important oilseed crops in the world. Vegetable oil obtained from rapeseed is a valuable raw material for the food and energy industry as well as for industrial applications. Compared to other vegetable oils, it has a lower concentration of saturated fatty acids (5%-10%), a higher content of monounsaturated fatty acids (44%-75%), and a moderate content of alpha-linolenic acid (9%-13%). Overall, rapeseed is grown in all continents on an industrial scale, so there is a growing need to predict yield before harvest. A combination of quantitative and qualitative data were used in this work in order to build three independent prediction models, on the basis of which yield simulations were carried out. Empirical data collected during field tests carried out in 2008-2015 were used to build three models, QQWR15_4, QQWR31_5, and QQWR30_6. Each model was composed of a different number of independent variables, ranging from 21 to 27. The lowest MAPE (mean absolute percentage error) yield prediction error corresponded to QQWR31_5, it was 6.88%, and the coefficient of determination R-2 was 0.69. As a result of the sensitivity analysis of the neural network, the most important independent variable influencing the final rapeseed yield was indicated, and for all the analyzed models it was ""The kind of sowing date in the previous year"" (KSD_PY).","winter rapeseed,artificial neural networks,machine learning,yield prediction,yield simulation,MLP (multi-layer perceptron) network,sensitivity analysis,precision agriculture,sustainable agriculture,sustainability",Article,"MDPI, ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND","Agriculture,Plant Sciences",,3.64,"MULTIPLE,LINEAR-REGRESSION,SEED,YIELD,SOWING,DATE,INDIAN,MUSTARD,OILSEED,RAPE,WHEAT,YIELD,PREDICTION,CANOLA,GROWTH",AGRONOMY-BASEL,https://www.mdpi.com/2073-4395/9/12/781/pdf,
16,The Relative Importance of Race Compared to Health Care and Social Factors in Predicting Prostate Cancer Mortality: A Random Forest Approach,202,6,1209-1215,"Hanson Heidi A.,Martin Christopher,O'Neil Brock,Leiser Claire L.,Mayer Erik N.,Smith Ken R.,Lowrance William T.","Hanson HA,Martin C,O'Neil B,Leiser CL,Mayer EN,Smith KR,Lowrance WT",Hanson HA,10.1097/JU.0000000000000416,Utah System of Higher Education,"Purpose: There is a differential in prostate cancer mortality between black and white men. Advances in precision medicine have shifted the research focus toward underlying genetic differences. However, nonbiological factors may have a large role in these observed disparities. Therefore, we sought to measure the relative importance of race compared to health care and social factors on prostate cancer specific mortality.
Materials and Methods: Using the SEER (Surveillance, Epidemiology, and End Results) database we identified 514,878 men diagnosed with prostate cancer at age 40 years or greater between 2004 and 2012. We also selected a subset of black and white men matched by age, stage and birth year. We stratified patients by age 40 to 54, 55 to 69 and 70 years or older and disease stage, resulting in 18 groups. By applying random forest methods with variable importance measures we analyzed 15 variables and interactions across 4 categories of factors (tumor characteristics, race, and health care and social factors) and the relative importance for prostate cancer specific mortality.
Results: Tumor characteristics at diagnosis were the most important factors for prostate cancer mortality. Across all groups race was less than 5% as important as tumor characteristics and only more important than health care and social factors in 2 of the 18 groups. Although race had a significant impact, health care and social factors known to be associated with racial disparities had greater or similarly important effects across all ages and stages.
Conclusions: Eradicating disparities in prostate cancer survival will require a multipronged approach, including advances in precision medicine. Disparities will persist unless health care access and social equality are achieved among all populations.","prostatic neoplasms,SEER program,race factors,mortality,health services accessibility",Article,"LIPPINCOTT WILLIAMS & WILKINS, TWO COMMERCE SQ, 2001 MARKET ST, PHILADELPHIA, PA 19103 USA",Urology & Nephrology,,6.413,"AFRICAN-AMERICANS,SURVIVAL,MEN,STATISTICS,TRENDS,BLACK,AGE",JOURNAL OF UROLOGY,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8276188,
17,A frame semantic overview of NLP-based information extraction for cancer-related EHR notes,100,,,"Datta Surabhi,Bernstam Elmer V,Roberts Kirk","Datta S,Bernstam EV,Roberts K",Roberts K,10.1016/j.jbi.2019.103301,"7000 Fannin St 600, Houston, TX 77030 USA.","Objective: There is a lot of information about cancer in Electronic Health Record (EHR) notes that can be useful for biomedical research provided natural language processing (NLP) methods are available to extract and structure this information. In this paper, we present a scoping review of existing clinical NLP literature for cancer.
Methods: We identified studies describing an NLP method to extract specific cancer-related information from EHR sources from PubMed, Google Scholar, ACL Anthology, and existing reviews. Two exclusion criteria were used in this study. We excluded articles where the extraction techniques used were too broad to be represented as frames (e.g., document classification) and also where very low-level extraction methods were used (e.g. simply identifying clinical concepts). 78 articles were included in the final review. We organized this information according to frame semantic principles to help identify common areas of overlap and potential gaps.
Results: Frames were created from the reviewed articles pertaining to cancer information such as cancer diagnosis, tumor description, cancer procedure, breast cancer diagnosis, prostate cancer diagnosis and pain in prostate cancer patients. These frames included both a definition as well as specific frame elements (i.e. extractable attributes). We found that cancer diagnosis was the most common frame among the reviewed papers (36 out of 78), with recent work focusing on extracting information related to treatment and breast cancer diagnosis.
Conclusion: The list of common frames described in this paper identifies important cancer-related information extracted by existing NLP techniques and serves as a useful resource for future researchers requiring cancer information extracted from EHR notes. We also argue, due to the heavy duplication of cancer NLP systems, that a general purpose resource of annotated cancer frames and corresponding NLP tools would be valuable.","Cancer,Natural language processing,Electronic health records,Deep phenotyping,Frame semantics,Scoping review",Review,"ACADEMIC PRESS INC ELSEVIER SCIENCE, 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA","Computer Science,Medical Informatics",,5.221,"ELECTRONIC,HEALTH,RECORD,LANGUAGE,PROCESSING,SYSTEM,BREAST-CANCER,PATHOLOGY,REPORTS,TEXT,CLASSIFICATION,IDENTIFICATION,PHENOTYPES,KNOWLEDGE,ONCOLOGY",JOURNAL OF BIOMEDICAL INFORMATICS,https://doi.org/10.1016/j.jbi.2019.103301,
18,Deep learning predicts extreme preterm birth from electronic health records,100,,,"Gao Cheng,Osmundson Sarah,Edwards Digna R. Velez,Jackson Gretchen Purcell,Malin Bradley A.,Chen You","Gao C,Osmundson S,Edwards DRV,Jackson GP,Malin BA,Chen Y",Chen Y,10.1016/j.jbi.2019.103334,Vanderbilt University,"Objective: Models for predicting preterm birth generally have focused on very preterm (28-32 weeks) and moderate to late preterm (32-37 weeks) settings. However, extreme preterm birth (EPB), before the 28th week of gestational age, accounts for the majority of newborn deaths. We investigated the extent to which deep learning models that consider temporal relations documented in electronic health records (EHRs) can predict EPB.
Study design: EHR data were subject to word embedding and a temporal deep learning model, in the form of recurrent neural networks (RNNs) to predict EPB. Due to the low prevalence of EPB, the models were trained on datasets where controls were undersampled to balance the case-control ratio. We then applied an ensemble approach to group the trained models to predict EPB in an evaluation setting with a nature EPB ratio. We evaluated the RNN ensemble models with 10 years of EHR data from 25,689 deliveries at Vanderbilt University Medical Center. We compared their performance with traditional machine learning models (logistical regression, support vector machine, gradient boosting) trained on the datasets with balanced and natural EPB ratio. Risk factors associated with EPB were identified using an adjusted odds ratio.
Results: The RNN ensemble models trained on artificially balanced data achieved a higher AUC (0.827 vs. 0.744) and sensitivity (0.965 vs. 0.682) than those RNN models trained on the datasets with naturally imbalanced EPB ratio. In addition, the AUC (0.827) and sensitivity (0.965) of the RNN ensemble models were better than the AUC (0.777) and sensitivity (0.819) of the best baseline models trained on balanced data. Also, risk factors, including twin pregnancy, short cervical length, hypertensive disorder, systemic lupus erythematosus, and hydroxychloroquine sulfate, were found to be associated with EPB at a significant level.
Conclusion: Temporal deep learning can predict EPB up to 8 weeks earlier than its occurrence. Accurate prediction of EPB may allow healthcare organizations to allocate resources effectively and ensure patients receive appropriate care.","SCHOOL-AGE OUTCOMES,27 WEEKS GESTATION,PERINATAL REGIONALIZATION,DEPRESSIVE SYMPTOMS,WEIGHT CHILDREN,INFANTS BORN,RISK-FACTORS,MORTALITY,PREGNANCY,DELIVERY",Article,"ACADEMIC PRESS INC ELSEVIER SCIENCE, 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA","Computer Science,Medical Informatics",,5.221,"SCHOOL-AGE,OUTCOMES,27,WEEKS,GESTATION,PERINATAL,REGIONALIZATION,DEPRESSIVE,SYMPTOMS,WEIGHT,CHILDREN,INFANTS,BORN,RISK-FACTORS,MORTALITY,PREGNANCY,DELIVERY",JOURNAL OF BIOMEDICAL INFORMATICS,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6899197,
19,Quantifying semantic similarity of clinical evidence in the biomedical literature to facilitate related evidence synthesis,100,,,"Hassanzadeh Hamed,Nguyen Anthony,Verspoor Karin","Hassanzadeh H,Nguyen A,Verspoor K",Hassanzadeh H,10.1016/j.jbi.2019.103321,Royal Brisbane & Women's Hospital,"Objective: Published clinical trials and high quality peer reviewed medical publications are considered as the main sources of evidence used for synthesizing systematic reviews or practicing Evidence Based Medicine (EBM). Finding all relevant published evidence for a particular medical case is a time and labour intensive task, given the breadth of the biomedical literature. Automatic quantification of conceptual relationships between key clinical evidence within and across publications, despite variations in the expression of clinically-relevant concepts, can help to facilitate synthesis of evidence. In this study, we aim to provide an approach towards expediting evidence synthesis by quantifying semantic similarity of key evidence as expressed in the form of individual sentences. Such semantic textual similarity can be applied as a key approach for supporting selection of related studies.
Material and methods: We propose a generalisable approach for quantifying semantic similarity of clinical evidence in the biomedical literature, specifically considering the similarity of sentences corresponding to a given type of evidence, such as clinical interventions, population information, clinical findings, etc. We develop three sets of generic, ontology-based, and vector-space models of similarity measures that make use of a variety of lexical, conceptual, and contextual information to quantify the similarity of full sentences containing clinical evidence. To understand the impact of different similarity measures on the overall evidence semantic similarity quantification, we provide a comparative analysis of these measures when used as input to an unsupervised linear interpolation and a supervised regression ensemble. In order to provide a reliable test-bed for this experiment, we generate a dataset of 1000 pairs of sentences from biomedical publications that are annotated by ten human experts. We also extend the experiments on an external dataset for further generalisability testing.
Results: The combination of all diverse similarity measures showed stronger correlations with the gold standard similarity scores in the dataset than any individual kind of measure. Our approach reached near 0.80 average Pearson correlation across different clinical evidence types using the devised similarity measures. Although they were more effective when combined together, individual generic and vector-space measures also resulted in strong similarity quantification when used in both unsupervised and supervised models. On the external dataset, our similarity measures were highly competitive with the state-of-the-art approaches developed and trained specifically on that dataset for predicting semantic similarity.
Conclusion: Experimental results showed that the proposed semantic similarity quantification approach can effectively identify related clinical evidence that is reported in the literature. The comparison with a state-of-the-art method demonstrated the effectiveness of the approach, and experiments with an external dataset support its generalisability.","Clinical semantic similarity,Clinical evidence,Evidence based medicine,Systematic review",Article,"ACADEMIC PRESS INC ELSEVIER SCIENCE, 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA","Computer Science,Medical Informatics",,5.221,"SCIENTIFIC,ARTIFACTS,SYSTEM,FRAMEWORK",JOURNAL OF BIOMEDICAL INFORMATICS,https://doi.org/10.1016/j.jbi.2019.103321,
20,Learning magnetization dynamics,491,,,"Kovacs Alexander,Fischbacher Johann,Oezelt Harald,Gusenbauer Markus,Exl Lukas,Bruckner Florian,Suess Dieter,Schrefl Thomas","Kovacs A,Fischbacher J,Oezelt H,Gusenbauer M,Exl L,Bruckner F,Suess D,Schrefl T",Schrefl T,10.1016/j.jmmm.2019.165548,Danube University Krems,Deep neural networks are used to model the magnetization dynamics in magnetic thin film elements. The magnetic states of a thin film element can be represented in a low dimensional space. With convolutional autoencoders a compression ratio of 1024:1 was achieved. Time integration can be performed in the latent space with a second network which was trained by solutions of the Landau-Lifshitz-Gilbert equation. Thus the magnetic response to an external field can be computed quickly.,"Micromagnetics,Magnetic sensors,Machine learning,Model order reduction",Article,"ELSEVIER, RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS","Materials Science,Physics",,2.836,,JOURNAL OF MAGNETISM AND MAGNETIC MATERIALS,http://arxiv.org/pdf/1903.09499,
21,Global discovery of stable and non-toxic hybrid organic-inorganic perovskites for photovoltaic systems by combining machine learning method with first principle calculations,66,,,"Wu Tianmin,Wang Jian","Wu TM,Wang J",Wang J,10.1016/j.nanoen.2019.104070,University of Hong Kong,"Traditional trial-and-error methods seriously restrict and hinder the searching of high-performance functional materials, especially when the search space is large. Rapid searching for advanced functional materials has always been a hot research topic, and attracted a lot of experimental and theoretical research attention. Here, by combining machine learning method with density functional theory (DFT) calculations, a target-driven method is proposed here to speed up the discovery of hidden hybrid organic-inorganic perovskites (HOIPs) for photovoltaic applications from 230808 HOIPs candidates which is almost two orders larger than previous studied. After imposing two criterions, i.e., charge neutrality condition and stability condition, on potential HOIPs candidates, followed by a machine learning (ML) screening, 686 orthorhombic-like HOIPs with proper bandgap are selected. In machine learning screening, ensemble learning using three ML models, including gradient boosting regression (GBR), supporting vector regression (SVR) and kernel ridge regression (KRR), are applied to predict the bandgap of 38086 HOIPs candidates. 132 stable and non-toxic (Cd-, Pb- and Hg-free) orthorhombiclike HOIPs are finally verified by DFT calculations with appropriate band gap for solar cells. In the present study, not only a series of unexplored stable and non-toxic HOIPs are discovered for further experimental synthesis, a new HOIPs database is constructed as well, thus beneficial to future functional material design.","Machine learning,Hybrid organic-inorganic perovskites,First principle calculations,Photovoltaics",Article,"ELSEVIER, RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS","Chemistry,Science & Technology - Other Topics,Materials Science,Physics",,17.631,"TOTAL-ENERGY,CALCULATIONS,EFFECTIVE,IONIC-RADII,SOLAR-CELLS,LEAD-FREE,HALIDE,PEROVSKITES,EFFICIENT,LIGHT,IODIDE,APPROXIMATION,PERFORMANCE",NANO ENERGY,,
22,Optofluidic label-free SERS platform for rapid bacteria detection in serum,300,,,"Hunter Robert,Sohi Ali Najafi,Khatoon Zohra,Berthiaume Vincent R.,Alarcon Emilio I,Godin Michel,Anis Hanan","Hunter R,Sohi AN,Khatoon Z,Berthiaume VR,Alarcon EI,Godin M,Anis H",Hunter R,10.1016/j.snb.2019.126907,University of Ottawa,"The prevalence of hospital acquired infections and antibiotic resistant pathogens necessitates the development of bacteria sensing systems that do not require sample amplification via conventional cell culturing, which can be prohibitively time-consuming. To meet this need, we designed an optofluidic Raman detection platform which utilized a microfluidic driven hollow-core photonic crystal fiber, which in combination with silver nanoparticles, provides a large enhancement to the Raman signal. By confining both light and cells within this fiber, spectral events generated by the flowing cells facilitates a novel method of cell counting to simultaneously quantify and qualify infections. Counting is performed automatically by a genetically optimized support vector machine learning algorithm that was previously developed by our group. The microfluidic system can be regenerated multiple times, and allows for online detection of planktonic bacteria to levels as low as 4 CFU/mL in 15 min. This compares favourably to other methods currently under development such as qPCR and biosensing techniques. Furthermore, Raman spectral differences between bacteria allow for inherent multiplexed detection in serum, by adding another layer to the learning algorithm. Further development of this device has promising potential as a rapid point-of-care system for infection management in the clinic.","Surface enhanced Raman scattering,Optofluidic,Label-free detection,Machine learning,Hospital acquired infection",Article,"ELSEVIER SCIENCE SA, PO BOX 564, 1001 LAUSANNE, SWITZERLAND","Chemistry,Electrochemistry,Instruments & Instrumentation",,6.743,"ENHANCED,RAMAN-SCATTERING,STAPHYLOCOCCUS-AUREUS,ESCHERICHIA-COLI,BIOSENSOR,NANOPARTICLES,DIAGNOSIS,CELLS",SENSORS AND ACTUATORS B-CHEMICAL,https://doi.org/10.1016/j.snb.2019.126907,
23,A Convolutional Neural Network for the automatic diagnosis of collagen VI-related muscular dystrophies,85,,,"Bazaga Adrian,Roldan Monica,Badosa Carmen,Jimenez-Mallebrera Cecilia,Porta Josep M.","Bazaga A,Roldan M,Badosa C,Jimenez-Mallebrera C,Porta JM",Porta JM,10.1016/j.asoc.2019.105772,Consejo Superior de Investigaciones Cientificas (CSIC),"The development of machine learning systems for the diagnosis of rare diseases is challenging, mainly due to the lack of data to study them. This paper surmounts this obstacle and presents the first Computer-Aided Diagnosis (CAD) system for low-prevalence collagen VI-related congenital muscular dystrophies. The proposed CAD system works on images of fibroblast cultures obtained with a confocal microscope and relies on a Convolutional Neural Network (CNN) to classify patches of such images in two classes: samples from healthy persons and samples from persons affected by a collagen VI-related muscular distrophy. This fine-grained classification is then used to generate an overall diagnosis on the query image using a majority voting scheme. The proposed system is advantageous, as it overcomes the lack of training data, points to the possibly problematic areas in the query images, and provides a global quantitative evaluation of the condition of the patients, which is fundamental to monitor the effectiveness of potential therapies. The system achieves a high classification performance, with 95% of accuracy and 92% of precision on randomly selected independent test images, outperforming alternative approaches by a significant margin. (C) 2019 Elsevier B.V. All rights reserved.","Convolutional neural networks,Deep learning,Classification,Computer aided diagnosis,Confocal microscopy images",Article,"ELSEVIER, RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS",Computer Science,,6.607,"CLASSIFICATION,PREDICTION",APPLIED SOFT COMPUTING,https://digital.csic.es/bitstream/10261/201984/3/A%20Convolutional%20Neural%20Network_Bazaga.pdf,
24,Physical model of scour at the toe of rock armoured structures,154,,,"den Bieman Joost P.,van Gent Marcel R. A.,Hoonhout Bas M.","den Bieman JP,van Gent MRA,Hoonhout BM",den Bieman JP,10.1016/j.coastaleng.2019.103572,Deltares,"Toe structures are a common feature in coastal rock armoured structures, both safeguarding the structure against scour at the toe and providing support to the armour layer. The toe structure itself is often comprised of rocks as well. Hence, generally some form of scour protection, such as a filter layer or a geotextile, is applied underneath the toe structure. Currently, there is a lack of available methods to judge the necessity of these kinds of costly scour protection measures underneath the toe structure. In this paper, a unique data set from physical model tests of scour at the toe of rock armoured structures is presented. The bed level change is measured by a laser scanner across the entirety of the sand bed, including underneath the structure. Additionally, the temporal development of the scour at the glass wall of the flume is measured using an innovative combination of video imagery and a deep learning algorithm. The sand diameter is not varied in the physical model tests. In all tests, scour was observed both in front of the structure and underneath the structure. Two expressions have been derived that give estimates for both kinds of scour, and thus can give insight into the necessity of scour protection underneath the toe structure.","Toe structures,Scour,Physical modelling,Breakwaters,Deep learning,Automated video analysis,Rubble mound,Open filters",Article,"ELSEVIER, RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS",Engineering,,,"SUCTION,REMOVAL,OPEN,FILTERS,SEDIMENT,PROTECTION,DESIGN",COASTAL ENGINEERING,,
25,EMG-based online classification of gestures with recurrent neural networks,128,,45-51,"Simao Miguel,Neto Pedro,Gibaru Olivier","Simao M,Neto P,Gibaru O",Neto P,10.1016/j.patrec.2019.07.021,Universidade de Coimbra,"Online gesture classification can rely on unsupervised segmentation in order to divide the data stream into static and dynamic segments for individual classification. However, this process requires motion detection calibration and adds complexity to the classification, thus becoming an additional failure point. An alternative is the sequential (dynamic) classification of the data stream. In this study we propose the use of recurrent neural networks (RNNs) to improve the online classification of hand gestures with Electromyography (EMG) signals acquired from the forearm muscles. The proposed methodology was evaluated on the UC2018 DualMyo and the NinaPro DB5 data set. The performance of a Feed-Forward Neural Network (FFNN), a Recurrent Neural Network (RNN), a Long Short-Term Memory network (LSTM) and a Gated Recurrent Unit (GRU) are compared and discussed. Additionally, an alternative performance index, the gesture detection accuracy, is proposed to evaluate the performance of the model during online classification. It is demonstrated that the static model (FFNN) and the dynamic models (LSTM, RNN and GRU) achieve similar accuracy for both data sets, i.e., about 95% for the DualMyo and about 91% for the NinaPro DB5. Although both models had similar accuracies, the dynamic models (LSTM and GRU) have a third of the parameters, presenting smaller training and inference times. + + Long Short-Term Memory (LSTM). (C) 2019 Elsevier B.V. All rights reserved.","Online gesture recognition,long short-term memory networks,Recurrent neural networks,Time series classification,LSTM,EMG",Article,"ELSEVIER, RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS",Computer Science,,3.615,,PATTERN RECOGNITION LETTERS,,
26,Dynamically enhanced static handwriting representation for Parkinson's disease detection,128,,204-210,"Diaz Moises,Angel Ferrer Miguel,Impedovo Donato,Pirlo Giuseppe,Vessio Gennaro","Diaz M,Ferrer MA,Impedovo D,Pirlo G,Vessio G",Vessio G,10.1016/j.patrec.2019.08.018,Universita degli Studi di Bari Aldo Moro,"Computer aided diagnosis systems can provide non-invasive, low-cost tools to support clinicians. These systems have the potential to assist the diagnosis and monitoring of neurodegenerative disorders, in particular Parkinson's disease (PD). Handwriting plays a special role in the context of PD assessment. In this paper, the discriminating power of ""dynamically enhanced"" static images of handwriting is investigated. The enhanced images are synthetically generated by exploiting simultaneously the static and dynamic properties of handwriting. Specifically, we propose a static representation that embeds dynamic information based on: (i) drawing the points of the samples, instead of linking them, so as to retain temporal/velocity information; and (ii) adding pen-ups for the same purpose. To evaluate the effectiveness of the new handwriting representation, a fair comparison between this approach and state-of-the-art methods based on static and dynamic handwriting is conducted on the same dataset, i.e. PaHaW. The classification workflow employs transfer learning to extract meaningful features from multiple representations of the input data. An ensemble of different classifiers is used to achieve the final predictions. Dynamically enhanced static handwriting is able to outperform the results obtained by using static and dynamic handwriting separately. (C) 2019 Elsevier B.V. All rights reserved.","Parkinson's disease,e-Health,Computer aided diagnosis,Dynamically enhanced static handwriting,Convolutional neural networks",Article,"ELSEVIER, RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS",Computer Science,,3.615,,PATTERN RECOGNITION LETTERS,,
27,Mesoscale characterization of continuous fiber reinforced composites through machine learning: Fiber chirality,181,,447-459,"Sherman Samuel,Simmons Jeff,Przybyla Craig","Sherman S,Simmons J,Przybyla C",Simmons J,10.1016/j.actamat.2019.10.001,United States Department of Defense,"A method of quantifying fiber chirality, the twist of continuous fibers through a volume, is defined and applied to both phantom data and real data. Specifically, a field quantity termed the fiber chirality based on the anti-symmetric part of the gradient of the fiber orientation is introduced. For this method of estimation, the input is the set of fiber positions gathered from the stack of images which represent the sample volumes. The phantom sample is generated and several different real continuous fiber reinforced matrix composites are experimentally characterized. Each phantom dataset contains a bundle of fibers that rotate about a center axis with a user-defined angle at each step in the z-direction. The chirality of the fibers is calculated based on the pre-characterized positions using a machine learning algorithm. To validate the method of quantification, our chirality estimation method results in a colormap with an angle of rotation that becomes increasingly more similar to the user-defined angle with decreasing interstice distance. Fiber positions from real data are then input into the estimation method and the results are compared. Published by Elsevier Ltd on behalf of Acta Materialia Inc.","3D characterization,Microstructure,Ceramic matrix composites,Fibers",Article,"PERGAMON-ELSEVIER SCIENCE LTD, THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND","Materials Science,Metallurgy & Metallurgical Engineering",,9.277,"TEXTILE,COMPOSITES",ACTA MATERIALIA,,
28,Deep Learning for EEG motor imagery classification based on multi-layer CNNs feature fusion,101,,542-554,"Amin Syed Umar,Alsulaiman Mansour,Muhammad Ghulam,Mekhtiche Mohamed Amine,Hossain M. Shamim","Amin SU,Alsulaiman M,Muhammad G,Mekhtiche MA,Hossain MS",Muhammad G,10.1016/j.future.2019.06.027,King Saud University,"Electroencephalography (EEG) motor imagery (MI) signals have recently gained a lot of attention as these signals encode a person's intent of performing an action. Researchers have used MI signals to help disabled persons, control devices such as wheelchairs and even for autonomous driving. Hence decoding these signals accurately is important for a Brain-Computer interface (BCI) system. But EEG decoding is a challenging task because of its complexity, dynamic nature and low signal to noise ratio. Convolution neural network (CNN) has shown that it can extract spatial and temporal features from EEG, but in order to learn the dynamic correlations present in MI signals, we need improved CNN models. CNN can extract good features with both shallow and deep models pointing to the fact that, at different levels relevant features can be extracted. Fusion of multiple CNN models has not been experimented for EEG data. In this work, we propose a multi-layer CNNs method for fusing CNNs with different characteristics and architectures to improve EEG MI classification accuracy. Our method utilizes different convolutional features to capture spatial and temporal features from raw EEG data. We demonstrate that our novel MCNN and CCNN fusion methods outperforms all the state-of-the-art machine learning and deep learning techniques for EEG classification. We have performed various experiments to evaluate the performance of the proposed CNN fusion method on public datasets. The proposed MCNN method achieves 75.7% and 95.4% on the BO Competition IV-2a dataset and the High Gamma Dataset respectively. The proposed CCNN method based on autoencoder cross-encoding achieves more than 10% improvement for cross-subject EEG classification. (C) 2019 Elsevier B.V. All rights reserved.","EEG motor imagery classification,Deep learning,Convolution neural network,Multi-layer CNNs feature fusion",Article,"ELSEVIER, RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS",Computer Science,,6.644,"CONVOLUTIONAL,NEURAL-NETWORKS",FUTURE GENERATION COMPUTER SYSTEMS-THE INTERNATIONAL JOURNAL OF ESCIENCE,,
29,Corneal thickness measurement by secondary speckle tracking and image processing using machine-learning algorithms,24,12,,"Bennett Aviya,Davidovitch Elnatan,Beiderman Yafim,Agadarov Sergey,Beiderman Yevgeny,Moshkovitz Avital,Polat Uri,Zalevsky Zeev","Bennett A,Davidovitch E,Beiderman Y,Agadarov S,Beiderman Y,Moshkovitz A,Polat U,Zalevsky Z",Zalevsky Z,10.1117/1.JBO.24.12.126001,Bar Ilan University,"Corneal thickness (CoT) is an important tool in the evaluation process for several disorders and in the assessment of intraocular pressure. We present a method enabling high-precision measurement of CoT based on secondary speckle tracking and processing of the information by machine-learning (ML) algorithms. The proposed configuration includes capturing by fast camera the laser beam speckle patterns backscattered from the corneal-scleral border, followed by ML processing of the image. The technique was tested on a series of phantoms having different thicknesses as well as in clinical trials on human eyes. The results show high accuracy in determination of eye CoT, and implementation is speedy in comparison with other known measurement methods. (C) The Authors. Published by SPIE under a Creative Commons Attribution 4.0 Unported License.","secondary speckle patterns,machine learning,lasers,optics,imaging,corneal thickness",Article,"SPIE-SOC PHOTO-OPTICAL INSTRUMENTATION ENGINEERS, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98225 USA","Biochemistry & Molecular Biology,Optics,Radiology, Nuclear Medicine & Medical Imaging",,2.993,"ULTRASOUND,PACHYMETRY,APPLANATION,TONOMETRY,SPECULAR,MICROSCOPY,ANTERIOR-CHAMBER,PENTACAM,REFLECTOMETRY,TOMOGRAPHY",JOURNAL OF BIOMEDICAL OPTICS,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7005539,
30,Modeling oil-brine interfacial tension at high pressure and high salinity conditions,183,,,"Amar Menad Nait,Shateri Mohammadhadi,Hemmati-Sarapardeh Abdolhossein,Alamatsaz Alireza","Amar MN,Shateri M,Hemmati-Sarapardeh A,Alamatsaz A",Hemmati-Sarapardeh A,10.1016/j.petrol.2019.106413,Shahid Bahonar University of Kerman (SBUK),"Accurate estimation of interfacial tension (IFT) in crude oil/brine system is of great importance for many processes in petroleum and chemical engineering. The current study plays emphasis on introducing the ""Gradient Boosting Decision Tree (GBDT)"" and ""Adaptive Boosting Support Vector Regression (AdaBoost SVR)"" as novel powerful machine learning tools to determine the IFT of crude oil/brine system. Two sorts of models have been developed using each of these two data-driven methods. The first kind includes six inputs, namely pressure (P), temperature (T) and four parameters describing the proprieties of crude oil (total acid number (TAN) and specific gravity (SG) and brine (NaCl equivalent salinity (S-eq) and pH), while the second kind deals with four inputs (without including pH and TAN). To this end, an extensive databank including 560 experimental points was considered, in which 80% of the points were employed for the training phase and the remaining part was utilized as blind test data. Results revealed that the proposed approaches provide very satisfactory predictions, and the implemented GBDT model with six inputs is the most accurate model of all with an average absolute relative error of 1.01%. Moreover, the outcomes of the GBDT model are better than literature models. Finally, outlier diagnostic using Leverage approach was performed to investigate the applicability domain of the GBDT model and to evaluate the quality of employed data.","Interfacial tension (IFT),Crude oil,Brine,Gradient boosting trees,AdaBoost SVR",Article,"ELSEVIER, RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS","Energy & Fuels,Engineering",,,"COMPUTATIONAL,INTELLIGENCE,SCHEME,SPONTANEOUS,IMBIBITION,TEMPERATURE,WATER,SURFACTANT,PREDICTION,BEHAVIOR,SYSTEMS,RECOVERY",JOURNAL OF PETROLEUM SCIENCE AND ENGINEERING,,
31,Machine learning methods applied to drilling rate of penetration prediction and optimization - A review,183,,,"Barbosa Luis Felipe F. M.,Nascimento Andreas,Mathias Mauro Hugo,de Carvalho Joao Andrade Jr.","Barbosa LFFM,Nascimento A,Mathias MH,de Carvalho JA",Barbosa LFFM,10.1016/j.petrol.2019.106332,Universidade Estadual Paulista,"Drilling wells in challenging oil/gas environments implies in large capital expenditure on wellbore's construction. In order to optimize the drilling related operation, real-time decisions making have been put in place, so that prediction of rate of penetration (ROP) with accuracy is essential. Despite many efforts (theoretical and experimental) throughout the years, modeling the ROP as a mathematical function of some key variables is not so trivial, due to the highly non-linearity behavior experienced. Therefore, several researches in the recent years have been proposing to use data-driven models from artificial intelligence field for ROP prediction and optimization.
This paper presents an extensive review of the literature on ROP prediction, especially, with machine learning techniques, as well as how these models can be used to optimize the drilling activities. The ROP models are classified as traditional models (based on physics-models), statistical models (e.g. multiple regression), or machine learning methods. This review enables to see that machine learning techniques can potentially outperform in terms of ROP-prediction accuracy on top of traditional or statistical models. Throughout this work, an extensive analysis of different ways of obtaining ROP models is carried out, concluding with different strategies adopted in literature to perform data-driven model optimization.
Despite the saving potential which can be achieved with real-time optimization based on data-driven ROP models, it is noticeable that there is a lack of implementation of those techniques in the industry, as per literature review. To take a step forward in real implementations, the petroleum industry must be aware that yet no rule of thumb already exists on this specific area, but still, good and very reasonable results can be achieved by following the best practices identified in this review. In addition, the modern practices of machine learning provide promising guidelines for implementing projects in oil and gas industry.","Data-driven model,Artificial intelligence,Multi-objective optimization,ROP,Drilling optimization",Review,"ELSEVIER, RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS","Energy & Fuels,Engineering",,4.376,"SUPPORT,VECTOR,REGRESSION,ARTIFICIAL,NEURAL-NETWORKS,FUZZY,INFERENCE,SYSTEM,MULTIOBJECTIVE,OPTIMIZATION,FEEDFORWARD,NETWORKS,INTELLIGENCE,TECHNIQUES,ROP,PREDICTION,RATE,MODEL,ALGORITHMS,OIL",JOURNAL OF PETROLEUM SCIENCE AND ENGINEERING,,
32,A spectroscopic method based on support vector machine and artificial neural network for fiber laser welding defects detection and classification,108,,,"Chen Yuanhang,Chen Bo,Yao Yongzhen,Tan Caiwang,Feng Jicai","Chen YH,Chen B,Yao YZ,Tan CW,Feng JC",Chen B,10.1016/j.ndteint.2019.102176,Harbin Institute of Technology,"Diverse welding processes have been utilized in manufacturing industry for years. But up to date, welding quality still cannot be guaranteed, due to the lack of an efficient and on-line welding defects monitoring method, and this leads to increased manufacturing costs. In this paper, a method based on feature extraction and machine learning algorithm for on-line quality monitoring and defects classification was presented. Plasma radiation was captured by an optical fiber probe, and delivered by an optical fiber to the spectrometer. The captured spectral signal was processed by selecting sensitive emission lines and extracting features of spectral data's evolution, which realized spectral data compression with low computational cost. After selecting the proper training data set, the designed ANN and SVM allows automatic detection and classification of welding defects. The validity of proposed method was successfully approved by test data set in welding experiments. Welding experiments on galvanized steel sheets showed the corresponding relationship between the output of classifiers and welding defects. Finally, the two classifiers were compared. Experiments indicated the performance of ANN is slightly better than that of SVM, while both of them have its own advantages.","Laser welding,Plasma spectral analysis,Support vector machine,Artificial neural network",Article,"ELSEVIER SCI LTD, THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND",Materials Science,,3.719,"OPTICAL,SENSOR,PENETRATION,DEPTH,WIDTH",NDT & E INTERNATIONAL,,
33,Observation of Electron Shakeup in CdSe/CdS Core/Shell Nanoplatelets,19,12,8495-8502,"Antolinez Felipe V.,Rabouw Freddy T.,Rossinelli Aurelio A.,Cui Jian,Norris David J.","Antolinez FV,Rabouw FT,Rossinelli AA,Cui J,Norris DJ",Norris DJ,10.1021/acs.nanolett.9b02856,ETH Zurich,"While ensembles of CdSe nanoplatelets (NPLs) show remarkably narrow photoluminescence line widths at room temperature, adding a CdS shell to increase their fluorescence efficiency and photostability causes line width broadening. Moreover, ensemble emission spectra of CdSe/CdS core/shell NPLs become strongly asymmetric at cryogenic temperatures. If the origin of these effects were understood, this could potentially lead to stable core/shell NPLs with narrower emission, which would be advantageous for applications. To move in this direction, we report time-resolved emission spectra of individual CdSe/CdS core/shell NPLs at 4 K. We observe surprisingly complex emission spectra that contain multiple spectrally narrow emission features that change during the experiment. With machine-learning algorithms, we can extract characteristic peak energy differences in these spectra. We show that they are consistent with electron ""shakeup lines"" from negatively charged trions. In this process, an electron-hole pair recombines radiatively but gives part of its energy to the remaining electron by exciting it into a higher single-electron level. This ""shakeup"" mechanism is enabled in our NPLs due to strong exciton binding and weak lateral confinement of the charge carriers. Time-resolved single-photon-counting measurements and numerical calculations suggest that spectral jumps in the emission features originate from fluctuations in the confinement potential caused by microscopic structural changes on the NPL surface (e.g., due to mobile surface charges). Our results provide valuable insights into line width broadening mechanisms in colloidal NPLs.","CdSe/CdS nanoplatelets,core/shell nanoplatelets,shakeup line,trion emission,weak confinement,machine learning",Article,"AMER CHEMICAL SOC, 1155 16TH ST, NW, WASHINGTON, DC 20036 USA","Chemistry,Science & Technology - Other Topics,Materials Science,Physics",,12.777,"AUGER,RECOMBINATION,QUANTUM,SPECTROSCOPY,EXCITON,DECAY,BLINKING,NANOCRYSTALS,SUPPRESSION,DIFFUSION,DYNAMICS",NANO LETTERS,https://www.research-collection.ethz.ch/bitstream/20.500.11850/386327/1/Antolinez_etal_NanoLetters_Shakeup.pdf,
34,,,,,,,,,,,,,,,,,,,,
35,Dual-domain convolutional neural networks for improving structural information in 3 T MRI,64,,90-100,"Zhang Yongqin,Yap Pew-Thian,Qu Liangqiong,Cheng Jie-Zhi,Shen Dinggang","Zhang YQ,Yap PT,Qu LQ,Cheng JZ,Shen DG",Shen DG,10.1016/j.mri.2019.05.023,University of North Carolina,We propose a novel dual-domain convolutional neural network framework to improve structural information of routine 3 T images. We introduce a parameter-efficient butterfly network that involves two complementary domains: a spatial domain and a frequency domain. The butterfly network allows the interaction of these two domains in learning the complex mapping from 3 T to 7 T images. We verified the efficacy of the dual-domain strategy and butterfly network using 3 T and 7 T image pairs. Experimental results demonstrate that the proposed framework generates synthetic 7 T-like images and achieves performance superior to state-of-the-art methods.,"Image synthesis,Image super-resolution,Magnetic resonance imaging,Deep learning,Convolutional neural network",Article,"ELSEVIER SCIENCE INC, STE 800, 230 PARK AVE, NEW YORK, NY 10169 USA","Radiology, Nuclear Medicine & Medical Imaging",,2.608,"IMAGE,SUPERRESOLUTION,7T-LIKE,IMAGES,RECONSTRUCTION,REGISTRATION,ENHANCEMENT",MAGNETIC RESONANCE IMAGING,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6874896,
36,Temporal and spatial deep learning network for infrared thermal defect detection,108,,,"Luo Qin,Gao Bin,Woo W. L.,Yang Yang","Luo Q,Gao B,Woo WL,Yang Y",Gao B,10.1016/j.ndteint.2019.102164,University of Electronic Science & Technology of China,"Most common types of defects for composite are debond and delamination. It is difficult to detect the inner defects on a complex shaped specimen by using conventional optical thermography nondestructive testing (NDT) methods. In this paper, a hybrid of spatial and temporal deep learning architecture for automatic thermography defects detection is proposed. The integration of cross network learning strategy has the capability to significantly minimize the uneven illumination and enhance the detection rate. The probability of detection (POD) has been derived to measure the detection results and this is coupled with comparison studies to verify the efficacy of the proposed method. The results show that visual geometry group-Unet (VGG-Unet) cross learning structure can significantly improve the contrast between the defective and non-defective regions. In addition, investigation of different feature extraction methods in which embedded in deep learning is conducted to optimize the learning structure. To investigate the efficacy and robustness of the proposed method, experimental studies have been carried out for inner debond defects on both regular and irregular shaped carbon fiber reinforced polymer (CFRP) specimens.","Deep learning,Segmentation,Thermography defect detection,Nondestructive testing",Article,"ELSEVIER SCI LTD, THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND",Materials Science,,3.719,"IMAGE,SEGMENTATION,DAMAGE,DETECTION,THERMOGRAPHY,COMPOSITES",NDT & E INTERNATIONAL,http://nrl.northumbria.ac.uk/id/eprint/41092/1/Temporal%20Spatial%20Deep%20Learning%20Infrared%20Thermal%20Detection_author%20version.pdf,
37,An efficient cervical disease diagnosis approach using segmented images and cytology reporting,58,,265-277,"Chen Hao,Yang Liyu,Li Ling,Li Ming,Chen Zhen","Chen H,Yang LY,Li L,Li M,Chen Z",Li L,10.1016/j.cogsys.2019.07.008,Nanchang Hangkong University,"Cervical cancer is the second most common cancer in women globally. A computer aided cervical disease diagnosis system that can relieve pressure on medical experts and save the cost is proposed. To implement our approach in the reality of cervical diseases diagnosis, a multi-modal framework is designed for three kinds of cervical diseases diagnosis that integrates uterine cervix images, Thinprep Cytology Test, human papillomavirus test, and patients' age. However, too many features increase memory storage costs and computational costs, and it affects the spread of this system in poor areas. Feature selection not only eliminates redundant or irrelevant features but also finds the factors that influence the disease most first is performed in multi-modal frameworks for cervical diseases diagnosis. The detailed process of the method is as follows: first, according the representative color, an efficient image segmentation algorithm is developed; then from three different types of segmented images, we extract color features and texture features for interpreting uterine cervix images; next, Boruta algorithm is applied to feature selection; finally, the performance of Random Forests that utilizes selected features for cervical disease diagnosis is investigated. In the experiment, the proposed multi-modal diagnostic approach gives the final diagnosis for three different kinds of cervical diseases with 83.1% accuracy, which significantly outperforms methods using any single source of information alone. The validation cohort is applied to validate the efficiency of our method, and the performance of random forest obtained by using only 1.2% of features is like or even better than using 100% of features. (C) 2019 Elsevier B.V. All rights reserved.","Cervical cancer screening,Uterine cervix image segmentation,Multimodal,Feature selection,Disease classification",Article,"ELSEVIER, RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS","Computer Science,Neurosciences & Neurology,Psychology",,3.134,"FEATURE-SELECTION,CANCER,CLASSIFICATION,NEOPLASIA,LESIONS",COGNITIVE SYSTEMS RESEARCH,,
38,Open-source pipeline for multi-class segmentation of the spinal cord with deep learning,64,,21-27,"Paugam Francois,Lefeuvre Jennifer,Perone Christian S.,Gros Charley,Reich Daniel S.,Sati Pascal,Cohen-Adad Julien","Paugam F,Lefeuvre J,Perone CS,Gros C,Reich DS,Sati P,Cohen-Adad J",Cohen-Adad J,10.1016/j.mri.2019.04.009,Ecole Centrale de Lyon,"This paper presents an open-source pipeline to train neural networks to segment structures of interest from MRI data. The pipeline is tailored towards homogeneous datasets and requires relatively low amounts of manual segmentations (few dozen, or less depending on the homogeneity of the dataset). Two use-case scenarios for segmenting the spinal cord white and grey matter are presented: one in marmosets with variable numbers of lesions, and the other in the publicly available human grey matter segmentation challenge [1]. The pipeline is freely available at: https://github.com/neuropoly/multiclass-segmentation.","MRI,Segmentation,Deep learning,U-Net,cnn,Spinal cord,Marmoset",Article,"ELSEVIER SCIENCE INC, STE 800, 230 PARK AVE, NEW YORK, NY 10169 USA","Radiology, Nuclear Medicine & Medical Imaging",,,,MAGNETIC RESONANCE IMAGING,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6800813,
39,Precision diagnostics based on machine learning-derived imaging signatures,64,,49-61,"Davatzikos Christos,Sotiras Aristeidis,Fan Yong,Habes Mohamad,Erus Guray,Rathore Saima,Bakas Spyridon,Chitalia Rhea,Gastounioti Aimilia,Kontos Despina","Davatzikos C,Sotiras A,Fan Y,Habes M,Erus G,Rathore S,Bakas S,Chitalia R,Gastounioti A,Kontos D",Davatzikos C,10.1016/j.mri.2019.04.012,University of Pennsylvania,"The complexity of modern multi-parametric MRI has increasingly challenged conventional interpretations of such images. Machine learning has emerged as a powerful approach to integrating diverse and complex imaging data into signatures of diagnostic and predictive value. It has also allowed us to progress from group comparisons to imaging biomarkers that offer value on an individual basis. We review several directions of research around this topic, emphasizing the use of machine learning in personalized predictions of clinical outcome, in breaking down broad umbrella diagnostic categories into more detailed and precise subtypes, and in non-invasively estimating cancer molecular characteristics. These methods and studies contribute to the field of precision medicine, by introducing more specific diagnostic and predictive biomarkers of clinical outcome, therefore pointing to better matching of treatments to patients.","GROWTH-FACTOR RECEPTOR,RECURRENCE-FREE SURVIVAL,HIGH-GRADE GLIOMAS,BREAST-CANCER,TEXTURE ANALYSIS,PRELIMINARY ASSOCIATIONS,PRIMARY CHEMOTHERAPY,PARKINSONS-DISEASE,ALZHEIMERS-DISEASE,PATIENT SURVIVAL",Article,"ELSEVIER SCIENCE INC, STE 800, 230 PARK AVE, NEW YORK, NY 10169 USA","Radiology, Nuclear Medicine & Medical Imaging",,,"GROWTH-FACTOR,RECEPTOR,RECURRENCE-FREE,SURVIVAL,HIGH-GRADE,GLIOMAS,BREAST-CANCER,TEXTURE,ANALYSIS,PRELIMINARY,ASSOCIATIONS,PRIMARY,CHEMOTHERAPY,PARKINSONS-DISEASE,ALZHEIMERS-DISEASE,PATIENT,SURVIVAL",MAGNETIC RESONANCE IMAGING,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6832825,
40,Applications of a deep learning method for anti-aliasing and super resolution in MRI,64,,132-141,"Zhao Can,Shao Muhan,Carass Aaron,Li Hao,Dewey Blake E.,Ellingsen Lotta M.,Woo Jonghye,Guttman Michael A.,Blitz Ari M.,Stone Maureen","Zhao C,Shao MH,Carass A,Li H,Dewey BE,Ellingsen LM,Woo J,Guttman MA,Blitz AM,Stone M",Zhao C,10.1016/j.mri.2019.05.038,Johns Hopkins University,"Magnetic resonance (MR) images with both high resolutions and high signal-to-noise ratios (SNRs) are desired in many clinical and research applications. However, acquiring such images takes a long time, which is both costly and susceptible to motion artifacts. Acquiring MR images with good in-plane resolution and poor through-plane resolution is a common strategy that saves imaging time, preserves SNR, and provides one viewpoint with good resolution in two directions. Unfortunately, this strategy also creates orthogonal viewpoints that have poor resolution in one direction and, for 2D MR acquisition protocols, also creates aliasing artifacts. A deep learning approach called SMORE that carries out both anti-aliasing and super-resolution on these types of acquisitions using no external atlas or exemplars has been previously reported but not extensively validated. This paper reviews the SMORE algorithm and then demonstrates its performance in four applications with the goal to demonstrate its potential for use in both research and clinical scenarios. It is first shown to improve the visualization of brain white matter lesions in FLAIR images acquired from multiple sclerosis patients. Then it is shown to improve the visualization of scarring in cardiac left ventricular remodeling after myocardial infarction. Third, its performance on multi-view images of the tongue is demonstrated and finally it is shown to improve performance in parcellation of the brain ventricular system. Both visual and selected quantitative metrics of resolution enhancement are demonstrated.","Deep learning,MRI,Super-resolution,Aliasing,Segmentation,Reconstruction,SMORE",Article,"ELSEVIER SCIENCE INC, STE 800, 230 PARK AVE, NEW YORK, NY 10169 USA","Radiology, Nuclear Medicine & Medical Imaging",,2.608,"IMAGE,SUPERRESOLUTION",MAGNETIC RESONANCE IMAGING,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7094770,
41,Denoising of MR images with Rician noise using a wider neural network and noise range division,64,,154-159,"You Xuexiao,Cao Ning,Lu Hao,Mao Minghe,Wang Wei","You XX,Cao N,Lu H,Mao MH,Wang W",Cao N,10.1016/j.mri.2019.05.042,Hohai University,"Magnetic resonance (MR) images denoising is important in medical image analysis. Denoising methods based on deep learning have shown great promise and outperform all of the other conventional methods. However, deep learning methods are limited by the number of training samples. In this article, using a small sample size, we applied a wider denoising neural network to MR images with Rician noise and trained several denoising models. The first model is specific to a certain noise, while the other applies to a wide range of noise levels. We considered the noise range as one interval, two sub-intervals, three sub-intervals, or even more sub-intervals to train the corresponding models. Experimental results demonstrate that for MR images, the proposed deep-learning models are efficient in terms of peak-signal-to-noise ratio, structure-similarity-index metrics and normalized mutual information. In addition, for blind noise, the effect of the three sub-intervals is better than that of the other sub-intervals.","Deep learning,Magnetic resonance (MR),Denoising,Rician noise,Residual learning",Article,"ELSEVIER SCIENCE INC, STE 800, 230 PARK AVE, NEW YORK, NY 10169 USA","Radiology, Nuclear Medicine & Medical Imaging",,2.608,"MAGNETIC-RESONANCE,IMAGES,TRANSFORM-DOMAIN,FILTER,DEEP,NETWORK",MAGNETIC RESONANCE IMAGING,,
42,Role of deep learning in infant brain MRI analysis,64,,171-189,"Mostapha Mahmoud,Styner Martin","Mostapha M,Styner M",Styner M,10.1016/j.mri.2019.06.009,University of North Carolina,"Deep learning algorithms and in particular convolutional networks have shown tremendous success in medical image analysis applications, though relatively few methods have been applied to infant MRI data due numerous inherent challenges such as inhomogenous tissue appearance across the image, considerable image intensity variability across the first year of life, and a low signal to noise setting. This paper presents methods addressing these challenges in two selected applications, specifically infant brain tissue segmentation at the isointense stage and presymptomatic disease prediction in neurodevelopmental disorders. Corresponding methods are reviewed and compared, and open issues are identified, namely low data size restrictions, class imbalance problems, and lack of interpretation of the resulting deep learning solutions. We discuss how existing solutions can be adapted to approach these issues as well as how generative models seem to be a particularly strong contender to address them.","MRI,Infant MRI,Machine learning,Deep learning,Convolutional neural networks,Isointense segmentation,Prediction",Review,"ELSEVIER SCIENCE INC, STE 800, 230 PARK AVE, NEW YORK, NY 10169 USA","Radiology, Nuclear Medicine & Medical Imaging",,2.608,"CONVOLUTIONAL,NEURAL-NETWORKS,AUTISM,SPECTRUM,DISORDER,COMPUTER-AIDED,DETECTION,AUTOMATIC,SEGMENTATION,WHITE-MATTER,TUMOR,SEGMENTATION,HIGH-RISK,IMAGES,REPRESENTATION,PARCELLATION",MAGNETIC RESONANCE IMAGING,https://cdr.lib.unc.edu/downloads/th83m7183,
43,Modeling resilient modulus of fine-grained materials using different statistical techniques,21,,,"Khasawneh Mohammad Ali,Al-jamal Narjes Firas","Khasawneh MA,Al-jamal NF",Khasawneh MA,10.1016/j.trgeo.2019.100263,Jordan University of Science & Technology,"( )For the realistic prediction of pavement performance, it is very important to accurately characterize the mechanical behavior of unbound material layers and subgrade soils. In pavement analysis using the elastic layered theory, material properties in terms of dynamic elastic modulus and Poisson's ratio are the major input parameters. The dynamic elastic modulus of pavement materials or resilient modulus (M-R) is measured by conducting repeated load triaxial compression tests typically not available to highway authorities due to the high costs involved in acquiring such high-performance equipment and/or lack of knowledge on how to operate this specialized equipment. Therefore, pavement engineers are obligated to use pavement design methodologies based on empirical tests. The main objective of this study is to use multiple linear regression (MLR), nonlinear regression and backpropagation artificial neural network algorithms to develop models to predict the resilient modulus of fine-grained materials based on 3709 soil samples collected from the Long-Term Pavement Performance (LTPP) website. The key input parameters selected for this study are: the confining pressure (sigma(3)), nominal maximum axial stress (sigma(1)), percent of silt (S), Liquid Limit (LL), Plasticity Index (PI), percent passing number 200 sieve (P#200), maximum dry density (rho(max-dry)), percent of clay (C), optimum moisture content (w(opt)), and laboratory-determined resilient modulus. Results revealed that the change of w(opt) has a higher effect on M-R compared with the change in density, the percent fines, percent of silt, percent of clay, confining pressure, PI and LL but less than the effect of changing axial stress. In addition, the best modeling technique to predict the resilient modulus of fine-grained soil was found to be the Artificial Neural Network (ANN) followed by the nonlinear regression and finally the MLR.","Long-Term Pavement Performance (LTPP),Resilient modulus (M-R),Fine-grained materials,Multilinear linear regression (MLR),Nonlinear regression,Artificial neural networks (ANN)",Article,"ELSEVIER, RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS",Engineering,,3.642,"ASPHALT,MIXTURES,SUBGRADE,SOILS,PREDICTION,SUCTION,SATURATION,DESIGN",TRANSPORTATION GEOTECHNICS,,
44,Super-resolution recurrent convolutional neural networks for learning with multi-resolution whole slide images,24,12,,"Mukherjee Lopamudra,Bui Huu Dat,Keikhosravi Adib,Loeffler Agnes,Eliceiri Kevin W.","Mukherjee L,Bui HD,Keikhosravi A,Loeffler A,Eliceiri KW",Mukherjee L,10.1117/1.JBO.24.12.126003,University of Wisconsin System,"We study a problem scenario of super-resolution (SR) algorithms in the context of whole slide imaging (WSI), a popular imaging modality in digital pathology. Instead of just one pair of high- and low-resolution images, which is typically the setup in which SR algorithms are designed, we are given multiple intermediate resolutions of the same image as well. The question remains how to best utilize such data to make the transformation learning problem inherent to SR more tractable and address the unique challenges that arises in this biomedical application. We propose a recurrent convolutional neural network model, to generate SR images from such multi-resolution WSI datasets. Specifically, we show that having such intermediate resolutions is highly effective in making the learning problem easily trainable and address large resolution difference in the low and high-resolution images common in WSI, even without the availability of a large size training data. Experimental results show state-of-the-art performance on three WSI histopathology cancer datasets, across a number of metrics. (C) The Authors. Published by SPIE under a Creative Commons Attribution 4.0 Unported License.","image super-resolution,convolutional neural networks,pathology,whole slide imaging,machine learning",Article,"SPIE-SOC PHOTO-OPTICAL INSTRUMENTATION ENGINEERS, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98225 USA","Biochemistry & Molecular Biology,Optics,Radiology, Nuclear Medicine & Medical Imaging",,2.993,"PANCREATIC-CANCER,STROMAL,BIOLOGY,DEEP,GRADE,CLASSIFICATION,CARCINOMA,OBJECT",JOURNAL OF BIOMEDICAL OPTICS,https://europepmc.org/articles/pmc6910074?pdf=render,
45,6-Hydroxythiobinupharidine Inhibits Migration of LM8 Osteosarcoma Cells by Decreasing Expression of LIM Domain Kinase 1,39,12,6507-6513,"Yoshizawa Masato,Nakamura Seikou,Sugiyama Yuki,Tamai Shiori,Ishida Yukiko,Sueyoshi Mari,Toda Yuki,Hosogi Shigekuni,Yano Yoshitaka,Ashihara Eishi","Yoshizawa M,Nakamura S,Sugiyama Y,Tamai S,Ishida Y,Sueyoshi M,Toda Y,Hosogi S,Yano Y,Ashihara E",Ashihara E,10.21873/anticanres.13865,"5 Nakauchi Cho, Kyoto 6078414, Japan.","Background/Aim: Osteosarcoma is the most malignant type of bone tumor. Patients with osteosarcoma metastases have a poorer prognosis than those without metastases. Thus, the prognosis of osteosarcoma patients with metastases must be improved. Materials and Methods: The present study investigated the inhibitory effects of 6-hydroxythiobinupharidine isolated from Nuphar pumilum on migration of LM8 murine osteosarcoma cells by a migration assay and also examined the expression of proteins related to actin dynamics by western blot. The present study also developed an automatic cell counting system using machine learning to count migrated cells by Fiji and Trainable Weka Segmentation. Results: 6-Hydroxythiobinupharidine inhibited migration of LM8 osteosarcoma cells in a dose-dependent manner, and decreased protein expression of Lin11, Isl-1, and Mec-3 domain kinase 1 (LIMK1) and the levels of phosphorylated Cofilin. Conclusion: 6-Hydroxythiobinupharidine suppressed migration of LM8 osteosarcoma cells by decreasing expression of LIMK1. 6-Hydroxythiobinupharidine could be potentially used as an anti-metastatic compound.","Osteosarcoma,6-hydroxythiobinupharidine,metastasis,LIMK,Cofilin,machine learning",Article,"INT INST ANTICANCER RESEARCH, EDITORIAL OFFICE 1ST KM KAPANDRITIOU-KALAMOU RD KAPANDRITI, PO BOX 22, ATHENS 19014, GREECE",Oncology,,2.418,"RHO,GTPASES,ACTIN,MOTILITY",ANTICANCER RESEARCH,,
46,A reinforcement learning decision model for online process parameters optimization from offline data in injection molding,85,,,"Guo Fei,Zhou Xiaowei,Liu Jiahuan,Zhang Yun,Li Dequn,Zhou Huamin","Guo F,Zhou XW,Liu JH,Zhang Y,Li DQ,Zhou HM",Zhou HM,10.1016/j.asoc.2019.105828,Huazhong University of Science & Technology,"Injection molding is widely used owing to its ability to form high precision products. Good dimensional accuracy control depends on appropriate process parameters settings. However, existing optimization methods fail in producing ultra-high precision products due to their narrow process windows. In order to address the problem, an online decision system which consists of a novel reinforcement learning framework and a self-prediction artificial neural network model is developed. This decision system utilizes the knowledge learned from offline data to dynamically optimize the process of ultra-high precision products. Process optimization of an optical lens is dedicated to validating the proposed system. The experimental results show that the proposed system has excellent convergence performance in producing lens with deviation not exceeding +/- 5 mu m. Comparison with the static optimization method prove that the decision model is more robust and effective in online production environment. And it achieves superior results in continuous production with the process capability index of 1.720 compared to 0.315 in fuzzy inference system. There is great potential for utilizing the proposed data-driven decision system in similar manufacturing process. (C) 2019 Elsevier B.V. All rights reserved.","Intelligent manufacturing,Injection molding,Neural network,Reinforcement learning",Article,"ELSEVIER, RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS",Computer Science,,6.607,"ARTIFICIAL,NEURAL-NETWORK,WARPAGE,SYSTEM,MINIMIZATION,ALGORITHM,DESIGN",APPLIED SOFT COMPUTING,,
47,Laser-induced breakdown spectroscopy spectral feature selection to enhance classification capabilities: A t-test filter approach,162,,,"Huffman Curtis,Sobral Hugo,Teran-Hinojosa Estrella","Huffman C,Sobral H,Teran-Hinojosa E",Huffman C,10.1016/j.sab.2019.105721,Universidad Nacional Autonoma de Mexico,"Extracting classification-relevant information from complex spectral data usually takes extensive expert knowledge and a lot of time and effort. The main challenge remains processing, in a practically feasible manner, huge amounts of highly variable data to recognize patterns useful to explain away classifications. In this paper we present a statistical hypothesis testing approach to select, with minimum knowledge regarding the spectral matrix, the LIBS spectral features potentially most useful to feed supervised classification algorithms. In probing the capabilities of our approach, starting from raw spectral data, a bio-LIBS application was used in an attempt to uncover the chemistry behind the severity diagnosis of liver fibrosis. By coupling our approach with Kernel Density Estimation and Gradient Boosting as normalizing strategy and classification algorithm, respectively, we managed to correctly classify 84% of the spectra, 91% of spots and 95% of samples with error rates comparable to computationally costlier learning-methods such as Random Forest. A definite advantage of the proposed approach is that it is highly scalable, parallelizable, and susceptible to carry-out in a piecemeal fashion.","Feature selection,Filter methods,Supervised classification,Laser-induced breakdown spectroscopy",Article,"PERGAMON-ELSEVIER SCIENCE LTD, THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND",Spectroscopy,,3.375,"VARIABLE,SELECTION,LIBS,PARAMETERS,SIGNAL",SPECTROCHIMICA ACTA PART B-ATOMIC SPECTROSCOPY,,
48,Aspect Aware Learning for Aspect Category Sentiment Analysis,13,6,,"Zhu Peisong,Chen Zhuang,Zheng Haojie,Qian Tieyun","Zhu PS,Chen Z,Zheng HJ,Qian TY",Qian TY,10.1145/3350487,Wuhan University,"Aspect category sentiment analysis (ACSA) is an underexploited subtask in aspect level sentiment analysis. It aims to identify the sentiment of predefined aspect categories. The main challenge in ACSA comes from the fact that the aspect category may not occur in the sentence in most of the cases. For example, the review ""they have delicious sandwiches"" positively talks about the aspect category ""food"" in an implicit manner.
In this article, we propose a novel aspect aware learning (AAL) framework for ACSA tasks. Our key idea is to exploit the interaction between the aspect category and the contents under the guidance of both sentiment polarity and predefined categories. To this end, we design a two-way memory network for integrating AAL into the framework of sentiment classification. We further present two algorithms to incorporate the potential impacts of aspect categories. One is to capture the correlations between aspect terms and the aspect category like ""sandwiches"" and ""food."" The other is to recognize the aspect category for sentiment representations like ""food"" for ""delicious."" We conduct extensive experiments on four SemEval datasets. The results reveal the essential role of AAL in ACSA by achieving the state-of-the-art performance.","Aspect category sentiment analysis,aspect aware learning,memory network",Article,"ASSOC COMPUTING MACHINERY, 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA",Computer Science,,3.071,,ACM TRANSACTIONS ON KNOWLEDGE DISCOVERY FROM DATA,,
49,A Solution for Counting Aedes aegypti and Aedes albopictus Eggs in Paddles from Ovitraps Using Deep Learning,17,12,1987-1994,"Santana C.,Firmo A.,Oliveira R.,Buarque P.,Alves G.,Lima R.","Santana C,Firmo A,Oliveira R,Buarque P,Alves G,Lima R",Santana C,10.1109/TLA.2019.9011543,University of Exeter,"In 2018, the Epidemiological Record 36 of the World Health Organization (WHO) indicates that around 390 million people get infected from Arboviroses or mosquito-borne diseases. Among the transmission vectors of these diseases, the Aedes aegypti and Aedes albopictus are responsible for a considerable parcel of the infections since they can transmit a broad range of infections (e.g., dengue, yellow fever, and chikungunya). To reduce the number of infections and deaths caused by these mosquitoes, monitor and control the population of these insects is a key factor. In this sense, ovitraps can be employed to monitor the population of Aedes mosquitoes. Ovitrap is a dark container filled with water where a porous wooden paddle is inserted to serve as an oviposition substrate. These devices are installed in monitored areas and, periodically technicians collect them to count the number of eggs deposited in the paddles manually. Because the manual egg counting task can be time-consuming and susceptible to human errors, in this work we present a solution that uses deep learning algorithms to automate the counting process. Moreover, to further reduce the human effort in the counting process, hardware that automatically acquires the images of the wooden paddles is also presented. Experiments comparing the proposed solution, the manual counting method, and two other solutions, namely ICount and EggCounter, are performed. The results achieved indicate that the proposed method achieved a superior result than the two other methods. Moreover, the application of the Wilcoxon test with a confidence interval of 95% indicates that the solution presented can be as accurate as of the manual counting method which is currently adopted.","Aedes aegypti,Aedes albopictus,ovitraps,deep learning,egg counting",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering",,0.675,"ZIKA,VIRUS,COST",IEEE LATIN AMERICA TRANSACTIONS,,
50,A comparative data mining approach for the prediction of energy recovery potential from various municipal solid waste,116,,,"Bagheri Mehdi,Esfilar Reza,Golchi Mohammad Sina,Kennedy Christopher A.","Bagheri M,Esfilar R,Golchi MS,Kennedy CA",Bagheri M,10.1016/j.rser.2019.109423,University of Victoria,"Knowledge of higher heating value (HHV) is critically important in the techno-economic analysis and optimal performance of thermal waste-to-energy (WTE) systems. In this work, we proposed a rapid and cost-effective method to accurately estimate the high heating value for various municipal solid wastes using a novel machine learning technique and the ultimate analysis. To this end, we relied on a comprehensive experimental HHV dataset from a diverse range of geographical origins and eight various waste classifications (i.e., 30 paper, 12 textile, 12 rubber and leather, 29 MSW mixture, 34 plastic, 61 wood, 20 sewage sludge, and 53 other wastes). In particular, we developed nonlinear HHV equations for fast, accurate, straightforward applications (e.g., especially when limited elemental information is available) using robust gene expression programming (GEP) algorithm. Meanwhile, we analyzed the accuracy gains from the application of more complex and nonlinear models such as a novel support vector machine (RBAS-SVM) and a feed-forward neural network (FFNN). While coefficient of determination (R-2) of the time-saving and straightforward GEP equations is 0.966, the SVM and FFNN models have 0.973 and 0.978 R-2. Besides, root mean square error (RMSE) of GEP, SVM, and FFNN models are 1.57, 1.44, and 1.25, respectively. The results of statistical performances for the entire dataset showed that average absolute error (AAE) of the proposed models in a range of 0.87-1.12%. Finally, the presented models were compared to those developed by other authors regarding the comprehensiveness (dataset size and diversity), accuracy, and validity (statistical performances) for the entire dataset.","Machine learning,higher heating value,Municipal solid wastes,Biomass wastes,Nonlinear modeling",Article,"PERGAMON-ELSEVIER SCIENCE LTD, THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND","Science & Technology - Other Topics,Energy & Fuels",,14.916,"HIGHER,HEATING,VALUE,ARTIFICIAL,NEURAL-NETWORK,TO-ENERGY,FORMATION,ENTHALPIES,ACCURATE,PREDICTION,PROXIMATE,ANALYSIS,REGRESSION-MODELS,FLASH-POINT,BIOMASS,MANAGEMENT",RENEWABLE & SUSTAINABLE ENERGY REVIEWS,,
51,Deep learning network with Euclidean similarity factor for Brain MR Tumor segmentation and volume estimation,10,6,,"Kumar G. Anand,Sridevi P. V","Kumar GA,Sridevi PV",Kumar GA,10.1142/S1793962319500399,Gayatri Vidya Parishad College of Engineering,"The major goal of this paper is to isolate tumor region from nontumor regions and the estimation of tumor volume. Accurate segmentation is not an easy task due to the varying size, shape and location of the tumor. After segmentation, volume estimation is necessary in order to accurately estimate the tumor volume. By exactly estimating the volume of abnormal tissue, physicians can do excellent prognosis, clinical planning and dosage estimation. This paper describes a new Euclidean Similarity factor (ESF) based active contour model with deep learning for segmenting the tumor region into complete, core and enhanced tumor portions. Initially, the ESF considers the spatial distances and intensity differences of the region automatically to detect the tumor region. It preserves the image details but removes the noisy details. Then, the 3D Convolutional Neural Network (3D CNN) segments the tumor by automatically extracting spatiotem-poral features. Finally, the extended shoelace method estimates the volume of the tumor accurately for n-sided polygons. The simulation result achieves a high accuracy of 92% and Jaccard index of 0.912 and computes the tumor volume with effective performance than existing approaches.","Active contour,brain tumor segmentation,convolutional neural network,Euclidean similarity factor,magnetic resonance imaging",Article,"WORLD SCIENTIFIC PUBL CO PTE LTD, 5 TOH TUCK LINK, SINGAPORE 596224, SINGAPORE",Computer Science,,,"IMAGE,SEGMENTATION,LOCALIZATION,MODEL",INTERNATIONAL JOURNAL OF MODELING SIMULATION AND SCIENTIFIC COMPUTING,,
52,Obstructive sleep apnea screening by heart rate variability-based apnea/normal respiration discriminant model,40,12,,"Nakayama Chikao,Fujiwara Koichi,Sumi Yukiyoshi,Matsuo Masahiro,Kano Manabu,Kadotani Hiroshi","Nakayama C,Fujiwara K,Sumi Y,Matsuo M,Kano M,Kadotani H",Fujiwara K,10.1088/1361-6579/ab57be,Nagoya University,"Objective: Obstructive sleep apnea (OSA) is a common sleep disorder; however, most patients are undiagnosed and untreated because it is difficult for patients themselves to notice OSA in daily living. Polysomnography (PSG), which is the gold standard test for sleep disorder diagnosis, cannot be performed in many hospitals. This fact motivates us to develop a simple system for screening OSA at home. Approach: The autonomic nervous system changes during apnea, and such changes affect heart rate variability (HRV). This work develops a new apnea screening method based on HRV analysis and machine learning technologies. An apnea/normal respiration (A/N) discriminant model is built for respiration condition estimation for every heart rate measurement, and an apnea/sleep ratio is introduced for final diagnosis. A random forest is adopted for the A/N discriminant model construction, which is trained with the PhysioNet apnea-ECG database. Main results: The screening performance of the proposed method was evaluated by applying it to clinical PSG data. Sensitivity and specificity achieved 76% and 92%, respectively, which are comparable to existing portable sleep monitoring devices used in sleep laboratories. Significance: Since the proposed OSA screening method can be used more easily than existing devices, it will contribute to OSA treatment.","obstructive sleep apnea,heart rate variability analysis,machine learning,home-use screening",Article,"IOP PUBLISHING LTD, TEMPLE CIRCUS, TEMPLE WAY, BRISTOL BS1 6BE, ENGLAND","Biophysics,Engineering,Physiology",,2.866,"CLINICAL-PRACTICE,GUIDELINE,AMERICAN-COLLEGE,POINCARE,PLOT,ASSOCIATION,POPULATION,ALGORITHM,DIAGNOSIS,HYPOPNEA,PRESSURE,MEDICINE",PHYSIOLOGICAL MEASUREMENT,https://doi.org/10.1088/1361-6579/ab57be,
53,Study on the effect of different electrode channel combinations of motor imagery EEG signals on classification accuracy,2019,23,8641-8645,"Zhu Kai,Wang Shuai,Zheng Dezhi,Dai Mengxi","Zhu K,Wang S,Zheng DZ,Dai MX",Wang S,10.1049/joe.2018.9073,Beihang University,"In order to improve the performance of motor imagery brain-computer interface (BCI) based on deep learning algorithm, here, the authors propose an electrode channel combination method. Although motor imagery electro-encephalography (EEG) signals which contain different electrode channels on the scalp surface have an effect on the classification performance, the effect of different electrode channel combinations has not been systematically explored. With the two deep learning models the authors constructed, the authors list some different electrode channel combinations to classify the left fist and right fist motor imagery EEG signals. The results show that the more the number of channels in these combinations, the higher the classification accuracy. However, when the number of channels exceeds 11, the classification accuracy increases slowly, and the classification effect is rarely improved. Therefore, the authors obtain an optimal electrode channel combination to use the electrode channels efficiently and to improve the performance of motor imagery BCI based on deep learning algorithms.","brain-computer interfaces,signal classification,electroencephalography,learning (artificial intelligence),medical signal processing,different electrode channel combinations,motor imagery EEG,classification accuracy,motor imagery brain-computer interface,deep learning algorithm,electrode channel combination method,motor imagery electro-encephalography signals,different electrode channels,fist motor imagery,classification effect,optimal electrode channel combination,motor imagery BCI",Article,"INST ENGINEERING TECHNOLOGY-IET, MICHAEL FARADAY HOUSE SIX HILLS WAY STEVENAGE, HERTFORD SG1 2AY, ENGLAND",Engineering,,,"EVENT-RELATED,SYNCHRONIZATION,DESYNCHRONIZATION",JOURNAL OF ENGINEERING-JOE,https://ietresearch.onlinelibrary.wiley.com/doi/pdfdirect/10.1049/joe.2018.9073,
54,Comparative study of motor imagery classification based on BP-NN and SVM,2019,23,8646-8649,"Jia Hongru,Wang Shuai,Zheng Dezhi,Qu Xiaolei,Fan Shangchun","Jia HR,Wang S,Zheng DZ,Qu XL,Fan SC",Wang S,10.1049/joe.2018.9075,Beihang University,"Motor imagery (MI) classification based on electroencephalogram (EEG) with high speed and accuracy is a key issue in brain-computer-interface (BCI) technology. This study compared the support vector machine (SVM) and back-propagation neural network (BP-NN) for MI classification. In this study, EEG data of four subjects provided by BCI competition 2008 was employed. For the comparison of classification accuracy (CA), there were three steps. First, EEG feature extraction for MI was implemented by using a common spatial pattern. Second, SVM and BP-NN were used to classify MI by cross-validation. Finally, the CA rate, receiver operating characteristic curve and area under the curve (AUC) were given to evaluate two classifiers. The average CA rates obtained on the four subjects using SVM and BP-NN were 75.20 and 80.73%, respectively. Furthermore, the mean AUCs of SVM and BP-NN were 0.7860 and 0.9462, respectively. Both average CA rate and AUC indicate that BP-NN has better accuracy of classification than SVM.","signal classification,neural nets,electroencephalography,medical signal processing,brain-computer interfaces,backpropagation,support vector machines,feature extraction,SVM,BP-NN,motor imagery classification,brain-computer-interface technology,MI classification,classification accuracy,EEG feature extraction,support vector machine,backpropagation neural network,electroencephalogram,BCI technology",Article,"INST ENGINEERING TECHNOLOGY-IET, MICHAEL FARADAY HOUSE SIX HILLS WAY STEVENAGE, HERTFORD SG1 2AY, ENGLAND",Engineering,,,,JOURNAL OF ENGINEERING-JOE,https://doi.org/10.1049/joe.2018.9075,
55,Deep learning-based research on the influence of training data size for breast cancer pathology detection,2019,23,8729-8732,"Cui Chongyang,Fan Shangchun,Lei Han,Qu Xiaolei,Zheng Dezhi","Cui CY,Fan SC,Lei H,Qu XL,Zheng DZ",Zheng DZ,10.1049/joe.2018.9093,Beihang University,"In pathological diagnosis of breast cancer, there are problems such as shortage of pathologists, difficulties in sample labeling, and huge workload of manual diagnosis. Therefore, deep learning-based computer-assisted pathology analysis systems have been developed to diagnose breast cancer and have achieved impressive results. However, it is difficult to obtain a large number of training sets due to the scarcity of pathological images and the huge labeling costs. Therefore, the size of the training set should be planned before building the pathology computer-assisted breast cancer analysis system. Here, the authors present a study to determine the optimal size of the training data set needed to achieve high classification accuracy when developing a pathology computer-assisted breast cancer analysis system. The authors trained two kind of CNNs using six different sizes of training data set and then tested the resulting system with a total of 10,000 images. All images were acquired from the Camelyon17 challenge. Here, the authors propose a scheme for determining the size of the training set and the size of the model in developing the pathology computer-assisted breast cancer analysis systems, which can be easily applied to develop systems for other different pathological images.","feature extraction,learning (artificial intelligence),medical image processing,patient diagnosis,cancer,neural nets,image classification,training data set,training set,pathology computer-assisted breast cancer analysis system,different pathological images,training data size,breast cancer pathology detection,pathological diagnosis,deep learning-based computer-assisted pathology analysis systems",Article,"INST ENGINEERING TECHNOLOGY-IET, MICHAEL FARADAY HOUSE SIX HILLS WAY STEVENAGE, HERTFORD SG1 2AY, ENGLAND",Engineering,,,,JOURNAL OF ENGINEERING-JOE,https://doi.org/10.1049/joe.2018.9093,
56,Pattern Discovery in White Etching Crack Experimental Data Using Machine Learning Techniques,9,24,,"Azzam Baher,Harzendorf Freia,Schelenz Ralf,Holweger Walter,Jacobs Georg","Azzam B,Harzendorf F,Schelenz R,Holweger W,Jacobs G",Azzam B,10.3390/app9245502,RWTH Aachen University,"White etching crack (WEC) failure is a failure mode that affects bearings in many applications, including wind turbine gearboxes, where it results in high, unplanned maintenance costs. WEC failure is unpredictable as of now, and its root causes are not yet fully understood. While WECs were produced under controlled conditions in several investigations in the past, converging the findings from the different combinations of factors that led to WECs in different experiments remains a challenge. This challenge is tackled in this paper using machine learning (ML) models that are capable of capturing patterns in high-dimensional data belonging to several experiments in order to identify influential variables to the risk of WECs. Three different ML models were designed and applied to a dataset containing roughly 700 high- and low-risk oil compositions to identify the constituting chemical compounds that make a given oil composition high-risk with respect to WECs. This includes the first application of a purpose-built neural network-based feature selection method. Out of 21 compounds, eight were identified as influential by models based on random forest and artificial neural networks. Association rules were also mined from the data to investigate the relationship between compound combinations and WEC risk, leading to results supporting those of previous analyses. In addition, the identified compound with the highest influence was proved in a separate investigation involving physical tests to be of high WEC risk. The presented methods can be applied to other experimental data where a high number of measured variables potentially influence a certain outcome and where there is a need to identify variables with the highest influence.","feature selection,premature bearing failure,wind turbine gearbox failure,random forests,artificial neural networks,association rules",Article,"MDPI, ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND","Chemistry,Engineering,Materials Science,Physics",,2.736,"ROLLING-CONTACT,FATIGUE,MICROSTRUCTURAL,CHANGES,WEC,FORMATION,BEARINGS,AREA,FLAKING",APPLIED SCIENCES-BASEL,https://www.mdpi.com/2076-3417/9/24/5502/pdf,
57,"Identification of Toxic Herbs Using Deep Learning with Focus on the Sinomenium Acutum, Aristolochiae Manshuriensis Caulis, Akebiae Caulis",9,24,,"Cho Jaeseong,Jeon Suyeon,Song Siyoung,Kim Seokyeong,Kim Dohyun,Jeong Jongkil,Choi Goya,Lee Soongin","Cho J,Jeon S,Song S,Kim S,Kim D,Jeong J,Choi G,Lee S",Lee S,10.3390/app9245456,Dongshin University,"Toxic herbs are similar in appearance to those known to be safe, which can lead to medical accidents caused by identification errors. We aimed to study the deep learning models that can be used to distinguish the herb Aristolochiae Manshuriensis Caulis (AMC), which contains carcinogenic and nephrotoxic ingredients from Akebiae Caulis (AC) and Sinomenium acutum (SA). Five hundred images of each herb without backgrounds, captured with smartphones, and 100 images from the Internet were used as learning materials. The study employed the deep-learning models VGGNet16, ResNet50, and MobileNet for the identification. Two additional techniques were tried to enhance the accuracy of the models. One was extracting the edges from the images of the herbs using canny edge detection (CED) and the other was applying transfer learning (TL) to each model. In addition, the sensitivity and specificity of AMC, AC, and SA identification were assessed by experts with a Ph.D. degree in herbology, undergraduates and clinicians of oriental medicine, and the ability was compared with those of MobileNet-TL ' s. The identification accuracies of VGGNet16, ResNet50, and MobileNet were 93.9%, 92.2%, and 95.6%, respectively. After adopting the CED technique, the accuracy was 95.0% for VGGNet16, 63.9% for ResNet50, and 80.0% for MobileNet. After using TL without the CED technique, the accuracy was 97.8% for VGGNet16-TL, 98.9% for ResNet50-TL, and 99.4% for MobileNet-TL. Finally, MobileNet-TL showed the highest accuracy among three models. MobileNet-TL had higher identification accuracy than experts with a Ph.D. degree in herbology in Korea. The result identifying AMC, AC, and SA in MobileNet-TL has demonstrated a great capability to distinguish those three herbs beyond human identification accuracy. This study indicates that the deep-learning model can be used for herb identification.","Herbology,herb identification,deep learning,Aristolochiae Manshuriensis Caulis,MobileNet,transfer learning",Article,"MDPI, ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND","Chemistry,Engineering,Materials Science,Physics",,2.736,,APPLIED SCIENCES-BASEL,https://www.mdpi.com/2076-3417/9/24/5456/pdf,
58,Optimized Resolution-Oriented Many-to-One Intensity Standardization Method for Magnetic Resonance Images,9,24,,"Gao Yuan,Wang Yuanyuan,Yu Jinhua","Gao Y,Wang YY,Yu JH",Wang YY,10.3390/app9245531,Fudan University,"With the development of big data, Radiomics and deep-learning methods based on magnetic resonance (MR) images, it is necessary to conduct large databases containing MR images from multiple centers. Having huge intensity distribution differences among images reduced or even eliminated, robust computer-aided diagnosis models could be established. Therefore, an optimized intensity standardization model is proposed. The network structure, loss function, and data input strategy were optimized to better avoid the image resolution loss during transformation. The experimental dataset was obtained from five MR scanners located in four hospitals and was divided into nine groups based on the imaging parameters, during which 9152 MR images from 499 participants were collected. Experiments show the superiority of the proposed method to the previously proposed unified model in resolution metrics including the peak signal-to-noise ratio, structural similarity, visual information fidelity, universal quality index, and image fidelity criterion. Another experiment further shows the advantage of the proposed method in increasing the effectiveness of following computer-aided diagnosis models by better preservation of MR image details. Moreover, the advantage over conventional standardization methods are also shown. Thus, MR images from different centers can be standardized using the proposed method, which will facilitate numerous data-driven medical imaging studies.","cycle generative adversarial network,intensity standardization,magnetic resonance imaging,resolution oriented,advanced weak-pair strategy",Article,"MDPI, ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND","Chemistry,Engineering,Materials Science,Physics",,,,APPLIED SCIENCES-BASEL,https://www.mdpi.com/2076-3417/9/24/5531/pdf,
59,Detection and Imaging of Underground Objects for Distinguishing Explosives by Using a Fluxgate Sensor Array,9,24,,"Guerkan Serkan,Karapinar Mustafa,Dogan Seydi","Gurkan S,Karapinar M,Dogan S",Gurkan S,10.3390/app9245415,"Natl Def Univ, Dept Elect & Commun, Army NCO Vocat Sch, TR-10100 Balikesir, Turkey.","Active and passive techniques are two different techniques with which to detect buried explosives. In practice, the most preferred active method works by broadcasting a signal underground. This signal may stimulate the buried explosive and cause it to explode. It is important to eliminate or minimize this drawback to ensure the safety of the detector operator. In this respect, it is important to increase the studies on the passive detection technique which is not currently used in practice. The aim of this study was to passively detect improvised explosive devices without stimulating them, and to classify underground objects as explosive or non-explosive. A fluxgate sensor array having 33 components was used for passive magnetic field measurements, and the nearest neighborhood algorithm was preferred for classifying the resulting data. In experimental studies, 33 different samples having different amounts of ferromagnetic properties were used. Successful imaging and classification were achieved for the measurements up to 20 cm below the surface of soil. Data were recorded as 32 x 25 matrices, and then they were reduced to 32 x 2 matrices having the same features. Samples having explosive properties were distinguished from other underground objects with success rates of 86% and 95% for 32 x 25 and 32 x 2 data matrices, respectively. Classification times for 32 x 25 and 32 x 2 data matrices were 42 ms and 3.62 ms, respectively. For data groups where the best results were obtained for the data matrices, frame numbers classified in one second were calculated as 23.80 and 276.2, respectively. False alarm rate achieved was 5.31%. The experimental results proved the successes of the matrices reduction and classification approach. One of the most common problems encountered in passive detecting techniques is that the sensor position affects the measurements negatively. In this paper, a solution has been proposed for this important problem.","underground object detection,magnetic field measurement,nearest neighbor searches,remote sensing",Article,"MDPI, ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND","Chemistry,Engineering,Materials Science,Physics",,2.736,"LANDMINE,DETECTION,MINE,DETECTION,IDENTIFICATION,CLASSIFICATION,SENSITIVITY",APPLIED SCIENCES-BASEL,https://www.mdpi.com/2076-3417/9/24/5415/pdf,
60,Development of Hybrid Machine Learning Models for Predicting the Critical Buckling Load of I-Shaped Cellular Beams,9,24,,"Hai-Bang Ly,Tien-Thinh Le,Lu Minh Le,Van Quan Tran,Vuong Minh Le,Huong-Lan Thi Vu,Quang Hung Nguyen,Binh Thai Pham","Ly HB,Le TT,Le LM,Tran VQ,Le VM,Vu HLT,Nguyen QH,Pham BT",Pham BT,10.3390/app9245458,"Univ Transport Technol, Hanoi 100000, Vietnam.","The principal purpose of this work is to develop three hybrid machine learning (ML) algorithms, namely ANFIS-RCSA, ANFIS-CA, and ANFIS-SFLA which are a combination of adaptive neuro-fuzzy inference system (ANFIS) with metaheuristic optimization techniques such as real-coded simulated annealing (RCSA), cultural algorithm (CA) and shuffled frog leaping algorithm (SFLA), respectively, to predict the critical buckling load of I-shaped cellular steel beams with circular openings. For this purpose, the existing database of buckling tests on I-shaped steel beams were extracted from the available literature and used to generate the datasets for modeling. Eight inputs, considered as independent variables, including the beam length, beam end-opening distance, opening diameter, inter-opening distance, section height, web thickness, flange width, and flange thickness, as well as one output of the critical buckling load of cellular steel beams considered as a dependent variable, were used in the datasets. Three quality assessment criteria, namely correlation coefficient (R), root mean squared error (RMSE) and mean absolute error (MAE) were employed for assessment of three developed hybrid ML models. The obtained results indicate that all three hybrid ML models have a strong ability to predict the buckling load of steel beams with circular openings, but ANFIS-SFLA (R = 0.960, RMSE = 0.040 and MAE = 0.017) exhibits the best effectiveness as compared with other hybrid models. In addition, sensitivity analysis was investigated and compared with linear statistical correlation between inputs and output to validate the importance of input variables in the models. The sensitivity results show that the most influenced variable affecting beam buckling capacity is the beam length, following by the flange width, the flange thickness, and the web thickness, respectively. This study shows that the hybrid ML techniques could help in establishing a robust numerical tool for beam buckling analysis. The proposed methodology is also promising to predict other types of failure, as well as other types of perforated beams.","I-shaped steel beam,machine learning,circular opening,adaptive neuro-fuzzy inference system,optimization techniques,cellular beam",Article,"MDPI, ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND","Chemistry,Engineering,Materials Science,Physics",,2.736,"BIOGEOGRAPHY-BASED,OPTIMIZATION,ARTIFICIAL,NEURAL-NETWORK,FROG-LEAPING,ALGORITHM,STEEL,BEAMS,SPATIAL,PREDICTION,STRENGTH,DESIGN,SYSTEM,INTELLIGENCE,PERFORMANCE",APPLIED SCIENCES-BASEL,https://www.mdpi.com/2076-3417/9/24/5458/pdf,
61,Steel Surface Defect Diagnostics Using Deep Convolutional Neural Network and Class Activation Map,9,24,,"Lee Soo Young,Tama Bayu Adhi,Moon Seok Jun,Lee Seungchul","Lee SY,Tama BA,Moon SJ,Lee S",Lee S,10.3390/app9245449,Pohang University of Science & Technology (POSTECH),"Steel defect diagnostics is considerably important for a steel-manufacturing industry as it is strongly related to the product quality and production efficiency. Product quality control suffers from a real-time diagnostic capability since it is less-automatic and is not reliable in detecting steel surface defects. In this study, we propose a relatively new approach for diagnosing steel defects using a deep structured neural network, e.g., convolutional neural network (CNN) with class activation maps. Rather than using a simple deep learning algorithm for the classification task, we extend the CNN diagnostic model for being used to analyze the localized defect regions within the images to support a real-time visual decision-making process. Based on the experimental results, the proposed approach achieves a near-perfect detection performance at 99.44% and 0.99 concerning the accuracy and F-1 score metric, respectively. The results are better than other shallow machine learning algorithms, i.e., support vector machine and logistic regression under the same validation technique.","steel surface defect,deep learning,convolutional neural network,class activation map",Article,"MDPI, ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND","Chemistry,Engineering,Materials Science,Physics",,2.736,"VISION,SYSTEM,MODEL",APPLIED SCIENCES-BASEL,https://www.mdpi.com/2076-3417/9/24/5449/pdf,
62,Computational Screening of New Perovskite Materials Using Transfer Learning and Deep Learning,9,24,,"Li Xiang,Dan Yabo,Dong Rongzhi,Cao Zhuo,Niu Chengcheng,Song Yuqi,Li Shaobo,Hu Jianjun","Li X,Dan YB,Dong RZ,Cao Z,Niu CC,Song YQ,Li SB,Hu JJ",Li SB; Hu JJ,10.3390/app9245510,Guizhou University,"As one of the most studied materials, perovskites exhibit a wealth of superior properties that lead to diverse applications. Computational prediction of novel stable perovskite structures has big potential in the discovery of new materials for solar panels, superconductors, thermal electric, and catalytic materials, etc. By addressing one of the key obstacles of machine learning based materials discovery, the lack of sufficient training data, this paper proposes a transfer learning based approach that exploits the high accuracy of the machine learning model trained with physics-informed structural and elemental descriptors. This gradient boosting regressor model (the transfer learning model) allows us to predict the formation energy with sufficient precision of a large number of materials of which only the structural information is available. The enlarged training set is then used to train a convolutional neural network model (the screening model) with the generic Magpie elemental features with high prediction power. Extensive experiments demonstrate the superior performance of our transfer learning model and screening model compared to the baseline models. We then applied the screening model to filter out promising new perovskite materials out of 21,316 hypothetical perovskite structures with a large portion of them confirmed by existing literature.","perovskites,transfer learning,deep learning,convolutional neural networks,small dataset,formation energy",Article,"MDPI, ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND","Chemistry,Engineering,Materials Science,Physics",,2.736,"EFFICIENT,MANGANITES",APPLIED SCIENCES-BASEL,https://scholarcommons.sc.edu/cgi/viewcontent.cgi?article=1267&context=csce_facpub,
63,The Relevance of Cataract as a Risk Factor for Age-Related Macular Degeneration: A Machine Learning Approach,9,24,,"Martinez-Velasco Antonieta,Martinez-Villasenor Lourdes,Miralles-Pechuan Luis,Perez-Ortiz Andric C.,Zenteno Juan C.,Estrada-Mena Francisco Javier","Martinez-Velasco A,Martinez-Villasenor L,Miralles-Pechuan L,Perez-Ortiz AC,Zenteno JC,Estrada-Mena FJ",Martinez-Velasco A,10.3390/app9245550,Universidad Panamericana - Ciudad de Mexico,"Age-related macular degeneration (AMD) is the leading cause of visual dysfunction and irreversible blindness in developed countries and a rising cause in underdeveloped countries. There is a current debate on whether or not cataracts are significant risk factors for AMD development. In particular, research regarding this association is so far inconclusive. For this reason, we aimed to employ here a machine-learning approach to analyze the relevance and importance of cataracts as a risk factor for AMD in a large cohort of Hispanics from Mexico. We conducted a nested case control study of 119 cataract cases and 137 healthy unmatched controls focusing on clinical data from electronic medical records. Additionally, we studied two single nucleotide polymorphisms in the CFH gene previously associated with the disease in various populations as positive control for our method. We next determined the most relevant variables and found the bivariate association between cataracts and AMD. Later, we used supervised machine-learning methods to replicate these findings without bias. To improve the interpretability, we detected the five most relevant features and displayed them using a bar graph and a rule-based tree. Our findings suggest that bilateral cataracts are not a significant risk factor for AMD development among Hispanics from Mexico.","age-related macular degeneration,cataract,risk factors,interpretability",Article,"MDPI, ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND","Chemistry,Engineering,Materials Science,Physics",,2.736,"BEAVER,DAM,UNITED-STATES,MACULOPATHY,PREVALENCE,POLYMORPHISMS,ASSOCIATION,SURGERY,CFH",APPLIED SCIENCES-BASEL,https://www.mdpi.com/2076-3417/9/24/5550/pdf,
64,Design on the Winter Jujubes Harvesting and Sorting Device,9,24,,"Ni Hongjun,Zhang Jiaqiao,Zhao Nansheng,Wang Chusen,Lv Shuaishuai,Ren Fuji,Wang Xingxing","Ni HJ,Zhang JQ,Zhao NS,Wang CS,Lv SS,Ren FJ,Wang XX",Lv SS,10.3390/app9245546,Nantong University,"According to the existing problems of winter jujube harvesting, such as the intensive labor of manual picking, damage to the surface of winter jujubes, a winter jujube harvesting and sorting device was developed. This device consisted of vibration mechanism, collection mechanism, and sorting mechanism. The eccentric vibration mechanism made the winter jujubes fall, and the umbrella collecting mechanism can collect winter jujube and avoid the impact of winter jujube on the ground, and the sorting mechanism removed jujube leaves and divided the jujube into two types, and the automatic leveling mechanism made the device run smoothly in the field. Through finite element analysis and BP (Back Propagation) neural network analysis, the results show that: The vibration displacement of jujube tree is related to the trunk diameter and vibration position; the impact force of winter jujubes falling is related to the elastic modulus of umbrella material; the collecting area can be increased four times for each additional step of the collection mechanism; jujube leaves can be effectively removed when blower wind speed reaches 45.64 m/s. According to the evaluation standard grades of the jujubes harvesting and sorting, the device has good effects and the excellent rate up to 90%, which has good practicability and economy.","winter jujube,harvesting and sorting,vibration mechanism,automatic leveling",Article,"MDPI, ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND","Chemistry,Engineering,Materials Science,Physics",,2.736,"STOCHASTIC,RESONANCE,SIMULATION",APPLIED SCIENCES-BASEL,https://www.mdpi.com/2076-3417/9/24/5546/pdf,
65,Deep Learning for Non-Invasive Determination of the Differentiation Status of Human Neuronal Cells by Using Phase-Contrast Photomicrographs,9,24,,"Ooka Maya,Tokuoka Yuta,Nishimoto Shori,Hiroi Noriko F.,Yamada Takahiro G.,Funahashi Akira","Ooka M,Tokuoka Y,Nishimoto S,Hiroi NF,Yamada TG,Funahashi A",Funahashi A,10.3390/app9245503,Keio University,"Regenerative medicine using neural stem cells (NSCs), which self-renew and have pluripotency, has recently attracted a lot of interest. Much research has focused on the transplantation of differentiated NSCs to damaged tissues for the treatment of various neurodegenerative diseases and spinal cord injuries. However, current approaches for distinguishing differentiated from non-differentiated NSCs at the single-cell level have low reproducibility or are invasive to the cells. Here, we developed a fully automated, non-invasive convolutional neural network-based model to determine the differentiation status of human NSCs at the single-cell level from phase-contrast photomicrographs; after training, our model showed an accuracy of identification greater than 94%. To understand how our model distinguished between differentiated and non-differentiated NSCs, we evaluated the informative features it learned for the two cell types and found that it had learned several biologically relevant features related to NSC shape during differentiation. We also used our model to examine the differentiation of NSCs over time; the findings confirmed our model's ability to distinguish between non-differentiated and differentiated NSCs. Thus, our model was able to non-invasively and quantitatively identify differentiated NSCs with high accuracy and reproducibility, and, therefore, could be an ideal means of identifying differentiated NSCs in the clinic.","convolutional neural networks,deep learning,image-wise classification,SH-SY5Y cells",Article,"MDPI, ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND","Chemistry,Engineering,Materials Science,Physics",,,"NEURAL,STEM-CELLS",APPLIED SCIENCES-BASEL,https://www.mdpi.com/2076-3417/9/24/5503/pdf,
66,A Three-Class Classification of Cognitive Workload Based on EEG Spectral Data,9,24,,"Plechawska-Wojcik Malgorzata,Tokovarov Mikhail,Kaczorowska Monika,Zapala Dariusz","Plechawska-Wojcik M,Tokovarov M,Kaczorowska M,Zapala D",Plechawska-Wojcik M,10.3390/app9245340,Lublin University of Technology,"Evaluation of cognitive workload finds its application in many areas, from educational program assessment through professional driver health examination to monitoring the mental state of people carrying out jobs of high responsibility, such as pilots or airline traffic dispatchers. Estimation of multilevel cognitive workload is a task usually realized in a subject-dependent way, while the present research is focused on developing the procedure of subject-independent evaluation of cognitive workload level. The aim of the paper is to estimate cognitive workload level in accordance with subject-independent approach, applying classical machine learning methods combined with feature selection techniques. The procedure of data acquisition was based on registering the EEG signal of the person performing arithmetical tasks divided into six intervals of advancement. The analysis included the stages of preprocessing, feature extraction, and selection, while the final step covered multiclass classification performed with several models. The results discussed show high maximal accuracies achieved: similar to 91% for both the validation dataset and for the cross-validation approach for kNN model.","cognitive workload,classification,EEG,spectra analysis",Article,"MDPI, ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND","Chemistry,Engineering,Materials Science,Physics",,2.736,"SUPPORT,VECTOR,MACHINES,FEATURE-EXTRACTION,PERFORMANCE,SELECTION,FEATURES,LOAD,INTERFACE,STRESS",APPLIED SCIENCES-BASEL,https://www.mdpi.com/2076-3417/9/24/5340/pdf,
67,Application of a Hybrid Artificial Neural Network-Particle Swarm Optimization (ANN-PSO) Model in Behavior Prediction of Channel Shear Connectors Embedded in Normal and High-Strength Concrete,9,24,,"Shariati Mahdi,Mafipour Mohammad Saeed,Mehrabi Peyman,Bahadori Alireza,Zandi Yousef,Salih Musab N. A.,Hoang Nguyen,Dou Jie,Song Xuan,Poi-Ngian Shek","Shariati M,Mafipour MS,Mehrabi P,Bahadori A,Zandi Y,Salih MNA,Nguyen H,Dou J,Song X,Poi-Ngian S",Nguyen H,10.3390/app9245534,Duy Tan University,"Featured Application Behavior prediction of channel shear connectors in normal and high-strength concrete (HSC) without conducting costly experiments.
Abstract Channel shear connectors are known as an appropriate alternative for common shear connectors due to having a lower manufacturing cost and an easier installation process. The behavior of channel connectors is generally determined through conducting experiments. However, these experiments are not only costly but also time-consuming. Moreover, the impact of other parameters cannot be easily seen in the behavior of the connectors. This paper aims to investigate the application of a hybrid artificial neural network-particle swarm optimization (ANN-PSO) model in the behavior prediction of channel connectors embedded in normal and high-strength concrete (HSC). To generate the required data, an experimental project was conducted. Dimensions of the channel connectors and the compressive strength of concrete were adopted as the inputs of the model, and load and slip were predicted as the outputs. To evaluate the ANN-PSO model, an ANN model was also developed and tuned by a backpropagation (BP) learning algorithm. The results of the paper revealed that an ANN model could properly predict the behavior of channel connectors and eliminate the need for conducting costly experiments to some extent. In addition, in this case, the ANN-PSO model showed better performance than the ANN-BP model by resulting in superior performance indices.","artificial neural network (ANN),particle swarm optimization (PSO),hybrid ANN-PSO,channel shear connector,high-strength concrete (HSC),behavior prediction",Article,"MDPI, ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND","Chemistry,Engineering,Materials Science,Physics",,2.736,"BEAMS,PERFORMANCE,DESIGN,FUZZY,SYSTEM,WALLS",APPLIED SCIENCES-BASEL,https://www.mdpi.com/2076-3417/9/24/5534/pdf,
68,Determination of Clamping Force Using Bolt Vibration Responses during the Tightening Process,9,24,,"Toh Gyungmin,Gwon Jaesoo,Park Junhong","Toh G,Gwon J,Park J",Park J,10.3390/app9245379,Hanyang University,"Featured Application
This approach allows detection of transient spectral characteristics using the cepstrum for diagnosis.
Abstract This paper presents a novel method to measure clamping force by using the vibration of bolts. The resonance frequency of the bolt increases in line with the clamping force during the tightening process. These characteristics were measured and utilized in the k-means clustering algorithm. Bolt specimens were fastened to the load cell using a nutrunner for verification of the proposed method. The precisely measured clamping force was labeled. The labeled data was used to predict the clamping force from the vibration responses. To use the proposed method in assembly of actual parts, an accelerometer was attached to the nutrunner for vibration measurements. This enabled continuous monitoring of the clamping force without influence on the parts. The estimated clamping force was compared with those from the torque method. When the vibration of a bolt was transmitted through the nutrunner, loss of high-frequency vibration energy occurred. The resonant frequency band vibrations of the bolt were preserved to determine the fastening force. The components in the low frequency band were excluded using a band-pass filter. The clamping force of the bolt used in the vehicle's lower arm and the link was also evaluated precisely. By using the proposed method, it is possible to continuously monitor variations of the clamping force during the manufacturing process.","bolt joints,clamping force,vibration,machine learning",Article,"MDPI, ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND","Chemistry,Engineering,Materials Science,Physics",,2.736,"NATURAL,FREQUENCIES,JOINTS,TORQUE,BEAMS",APPLIED SCIENCES-BASEL,https://www.mdpi.com/2076-3417/9/24/5379/pdf,
69,Deep learning-based classification using Cumulants and Bispectrum of EMG signals,17,12,1946-1953,"Orosco Eugenio Conrado,Amoros Jeremias,Gimenez Javier Alejandro,Soria Carlos Miguel","Orosco EC,Amoros JG,Gimenez JA,Soria CM",Orosco EC,10.1109/TLA.2019.9011538,"UNSJ CONICET, Inst Automat, Fac Ingn, San Juan, Argentina.","Surface electromyographic signals (EMG) historically have been used to classify tasks in basis of a feature extraction scheme and low complexity classifiers. Deep networks, as Multilayer Perceptron and Convolutional Neural Network (MLP and CNN, respectively), avoid the traditional, complex and heuristic (handcrafted) process of feature extraction. Today, it is possible to face the computational cost that these automatic techniques require due to the technology advancement. This allowed deep learning techniques to be quickly generalized to countless applications. This paper proposes to use the third order cumulants and their 2D Fourier transform (Bispectrum) to directly feed CNN and MLP deep learning networks. The classifier is not user-dependent (same classifier for all users) and obtains better results than the classical scheme according to several metrics.","Electromyography,Media,Two dimensional displays,Deep learning,Irrigation,IEEE transactions,Feature extraction,EMG,Cumulants,Bispectrum,CNN,MLP",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering",,0.675,"MUSCLE,SYNERGIES,DENSITY-FUNCTION",IEEE LATIN AMERICA TRANSACTIONS,,
70,Ensembles of Convolutional Neural Networks on Computer-Aided Pulmonary Tuberculosis Detection,17,12,1954-1963,"Evangelista Lucas Gabriel Coimbra,Guedes Elloa B.","Evangelista LGC,Guedes EB",Evangelista LGC,10.1109/TLA.2019.9011539,Universidade do Estado do Amazonas,"Pulmonary tuberculosis is one of the top 10 causes of death in the world in which Latin America represents 3% of the worldwide incidence with a mortality rate of 7.3%. According to goals of World Health Organization to end the global tuberculosis epidemy by 2030, the development of novel diagnosis strategies is crucial. Taking this context into account, the present work shows results on the use of ensembles of Convolutional Neural Networks to aid Computer Vision diagnosis of tuberculosis from patients' chest X-ray. Our solution comprises an ensemble of three different deep architectures of such networks having accuracy higher than 93% on the proposed task. The solution was obtained and evaluated in real-world data from public datasets, favouring reproducibility, surpasses human experts performance reported by literature in these very same datasets, uses canonical architectures of convolutional neural networks, with and without Transfer Learning, and requires minimal effort on data preparation and no previous feature extraction.","Convolutional neural networks,Deep learning,IEEE transactions,Computer architecture,Image segmentation,X-ray imaging,Hospitals,Pulmonary tuberculosis,Convolutional Neural Networks,Deep Learning",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering",,0.675,"CHEST,RADIOGRAPHS,CLASSIFICATION",IEEE LATIN AMERICA TRANSACTIONS,,
71,Automatic classification of breast lesions usingTransfer Learning,17,12,1964-1969,"da Silva R.,de Carvalho A.","da Silva R,de Carvalho A",da Silva R,10.1109/TLA.2019.9011540,Universidade Federal do Piaui,"Breast lesions are one of the most common types of lesions among women in Brazil and worldwide, accounting forabout 28% of new cases each year. These lesions may have Benignor Malignant behaviors. In this work, a computational method-ology for image classification was developed to differentiate malignant and benign lesions breast, aiming at low computational cost and good efficiency. In our approach, different Convolutional Neural Networks architectures and several classifiers were tested. Transfer Learning was employed to deal with the limitation of the small number of images in the database, reaching an accuracy of 81.73%, a sensitivity of 85.66%, a specificity of 78.40%, Kappa of 0.63 and ROC curve of 0.82. Finally, it is believed that theproposed methodology can integrate a CAD tool acting as patient screening or providing a second opinion to the specialist.","Breast Lesions,Digital Image Processing,Trans-fer Learning,Deep Learning",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering",,0.675,,IEEE LATIN AMERICA TRANSACTIONS,,
72,Reconstructing piecewise planar scenes with multi-view regularization,5,4,337-345,"Xi Weijie,Chen Xuejin","Xi WJ,Chen XJ",Chen XJ,10.1007/s41095-019-0159-7,Chinese Academy of Sciences,"Reconstruction of man-made scenes from multi-view images is an important problem in computer vision and computer graphics. Observing that man-made scenes are usually composed of planar surfaces, we encode plane shape prior in reconstructing man-made scenes. Recent approaches for single-view reconstruction employ multi-branch neural networks to simultaneously segment planes and recover 3D plane parameters. However, the scale of available annotated data heavily limits the generalizability and accuracy of these supervised methods. In this paper, we propose multi-view regularization to enhance the capability of piecewise planar reconstruction during the training phase, without demanding extra annotated data. Our multi-view regularization enables the consistency among multiple views by making the feature embedding more robust against view change and lighting variations. Thus, the neural network trained by multi-view regularization performs better on a wide range of views and lightings in the test phase. Based on more consistent prediction results, we merge the recovered models from multiple views to reconstruct scenes. Our approach achieves state-of-the-art reconstruction performance compared to previous approaches on the public ScanNet dataset.","scene modeling,multi-view,regularization,neural network",Article,"SPRINGERNATURE, CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND",Computer Science,,,,COMPUTATIONAL VISUAL MEDIA,https://link.springer.com/content/pdf/10.1007/s41095-019-0159-7.pdf,
73,A comprehensive study on estimating higher heating value of biomass from proximate and ultimate analysis with machine learning approaches,188,,,"Xing Jiangkuan,Luo Kun,Wang Haiou,Gao Zhengwei,Fan Jianren","Xing JK,Luo K,Wang HO,Gao ZW,Fan JR",Luo K,10.1016/j.energy.2019.116077,Zhejiang University,"Higher heating value (HHV) is an important parameter for design and operation of biomass-fueled energy systems. Experimental approach is always time-consuming and expensive for determinating this property compared with mathematical models. In this paper, three machine learning approaches, including artificial neural network (ANN), support vector machine (SVM) and random forest (RF), are employed for accurately estimating biomass HHV from ultimate or proximate analysis. The linear and nonlinear empirical correlations are also carried out for comparison. The results show machine learning approaches give better predictions (R-2 > 0.90) compared with those of empirical correlations (R-2 < 0.70), especially for the extreme values. The RF model shows the best performances for both the ultimate and proximate analysis, with the determination coefficient >0.94. The SVM and ANN approaches show similar performances with R-2 similar to 0.90. Ultimate-based models show better performances compared with those of the proximate-based models even with much less samples. Relative importance analysis shows for the proximate analysis, the ash, volatile matter and fixed carbon fractions show the maximum, medium and minimum effects, respectively. For the ultimate analysis, carbon and hydrogen fractions hold the first two significant places with carbon fraction having the most significant influence, while the oxygen and nitrogen fractions have limited effects. (C) 2019 Elsevier Ltd. All rights reserved.","Biomass,Higher heat value (HHV),Proximate analysis,Artificial neural network (ANN),Support vector machine (SVM),Random forest (RF)",Article,"PERGAMON-ELSEVIER SCIENCE LTD, THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND","Thermodynamics,Energy & Fuels",,6.845,"ARTIFICIAL,NEURAL-NETWORKS,CALORIFIC,VALUE,EMPIRICAL,CORRELATIONS,COAL,DEVOLATILIZATION,RANDOM,FOREST,PREDICTION,MODEL,HHV,GASIFICATION,ANFIS",ENERGY,,
74,Breast mass detection from the digitized X-ray mammograms based on the combination of deep active learning and self-paced learning,101,,668-679,"Shen Rongbo,Yan Kezhou,Tian Kuan,Jiang Cheng,Zhou Ke","Shen RB,Yan KZ,Tian K,Jiang C,Zhou K",Zhou K,10.1016/j.future.2019.07.013,Huazhong University of Science & Technology,"Breast mass detection is a challenging task in mammogram, since mass is usually embedded and surrounded by various normal tissues with similar density. Recently, deep learning has achieved impressive performance on this task. However, most deep learning methods require large amounts of well-annotated datasets. Generally, the training datasets is generated through manual annotation by experienced radiologists. However, manual annotation is very time-consuming, tedious and subjective. In this paper, for the purpose of minimizing the annotation efforts, we propose a novel learning framework for mass detection that incorporates deep active learning (DAL) and self-paced learning (SPL) paradigm. The DAL can significantly reduce the annotation efforts by radiologists, while improves the efficiency of model training by obtaining better performance with fewer overall annotated samples. The SPL is able to alleviate the data ambiguity and yield a robust model with generalization capability in various scenarios. In detail, we first employ a few of annotated easy samples to initialize the deep learning model using Focal Loss. In order to find out the most informative samples, we propose an informativeness query algorithm to rank the large amounts of unannotated samples. Next, we propose a self-paced sampling algorithm to select a number of the most informative samples. Finally, the selected most informative samples are manually annotated by experienced radiologists, which are added into the annotated samples for the model updating. This process is looped until there are not enough most informative samples in the unannotated samples. We evaluate the proposed learning framework on 2223 digitized mammograms, which are accompanied with diagnostic reports containing weakly supervised information. The experimental results suggest that our proposed learning framework achieves superior performance over the counterparts. Moreover, our proposed learning framework dramatically reduces the requirement of the annotated samples, i.e., about 20% of all training data. (C) 2019 Published by Elsevier B.V.","Breast cancer,Mammography,Mass detection,Deep active learning,Self-paced learning",Article,"ELSEVIER, RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS",Computer Science,,6.644,"COMPUTER-AIDED,DETECTION,SEGMENTATION,CLASSIFICATION",FUTURE GENERATION COMPUTER SYSTEMS-THE INTERNATIONAL JOURNAL OF ESCIENCE,,
75,Auto-detection of epileptic seizure events using deep neural network with different feature scaling techniques,128,,544-550,"Thara D. K.,PremaSudha B. G.,Xiong Fan","Thara DK,PremaSudha BG,Xiong F",Thara DK,10.1016/j.patrec.2019.10.029,"CIT, Dept ISE, Tumakuru, Karnataka, India.","Misdiagnosis of epilepsy is more seen in manual analysis of electroencephalogram (EEG) signals for epileptic seizure event detection. Therefore, automated systems for epilepsy detection are required to help neurologists in diagnosing epilepsy. These automated systems act as supporting systems for the neurologists to diagnose epilepsy with good accuracy in less time. In this paper an attempt is made to develop an automated seizure detection method using deep neural network using the dataset collected from Bonn University, Germany. The results of the experiment are compared with the existing machine learning method. Our model gives better results compared to ML methods without the need of feature extraction. It is important to perform normalization of the dataset using feature scaling techniques to obtain good accuracy in the results. In this experiment we also worked on feature scaling of the dataset. At first we tried using StandardScaler and calculated loss using mean squared error. For this we achieved an accuracy of 97.21%, Sensitivity 98.17%, Specificity 94.93%, F1_score 98.48%, MCC 91.96% and ROC 97.55%. Experiment was continued to compare the performance of four different feature scaling techniques and four different loss functions. From the experimental results it was observed that StandardScaler and RobustScaler are equally good and are the best feature scaling techniques. Loss computed using Mean squared error works better in combination with all feature scaling techniques. (C) 2019 Elsevier B.V. All rights reserved.","Deep neural network,Epilepsy,Seizure,Feature scaling,Loss",Article,"ELSEVIER, RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS",Computer Science,,3.615,,PATTERN RECOGNITION LETTERS,,
76,Machine learning algorithms for structural performance classifications and predictions: Application to reinforced masonry shear walls,22,,252-265,"Siam Ahmad,Ezzeldin Mohamed,El-Dakhakhni Wael","Siam A,Ezzeldin M,El-Dakhakhni W",Ezzeldin M,10.1016/j.istruc.2019.06.017,McMaster University,"Current building codes and design standards classify different structural components according to their expected structural performance. Such classification is usually based on datasets of experimental results typically supplemented by analytical and/or numerical simulations. However, it is usually prohibitive to experimentally evaluate the influence of the typically large number of (and the wide numerical range of each of the) interacting design parameters, on the response of any one class of structural components. Subsequently, the current study builds on the recent advances in the area of machine learning (ML)-a class of artificial intelligence, to introduce a robust ML-based framework for performance prediction and classification of structural components. In order to demonstrate the use of the developed framework, a dataset of 97 reinforced masonry shear walls (RMSWs) is utilized. In this respect, the current study first conducts an exploratory data analysis to recognize the influence of the walls' geometrical and mechanical characteristics on the wall responses. Subsequently, an unsupervised learning algorithm is developed to cluster the walls based on their features. Finally, the training and validation datasets are used to further develop and validate a supervised learning algorithm to classify the walls and predict their lateral drifts according to their failure modes. The study is expected to introduce and demonstrate the capability of ML-based frameworks for future relevant studies within other structural engineering applications.","Classification,Clustering,Exploratory data analysis,Machine learning,Structural performance,Supervised learning,Unsupervised learning,Predictions",Article,"ELSEVIER SCIENCE INC, STE 800, 230 PARK AVE, NEW YORK, NY 10169 USA",Engineering,,3.12,"ARTIFICIAL,NEURAL-NETWORK,STRENGTH,MODEL,DESIGN",STRUCTURES,,
77,A novel Cerenkov luminescence tomography approach using multilayer fully connected neural network,64,24,,"Zhang Zeyu,Cai Meishan,Gao Yuan,Shi Xiaojing,Zhang Xiaojun,Hu Zhenhua,Tian Jie","Zhang ZY,Cai MS,Gao Y,Shi XJ,Zhang XJ,Hu ZH,Tian J",Tian J,10.1088/1361-6560/ab5bb4,Xidian University,"Cerenkov luminescence tomography (CLT) has been proved as an effective tool for various biomedical applications. Because of the severe scattering of Cerenkov luminescence, the performance of CLT remains unsatisfied. This paper proposed a novel CLT reconstruction approach based on a multilayer fully connected neural network (MFCNN). Monte Carlo simulation data was employed to train the MFCNN, and the complex relationship between the surface signals and the true sources was effectively learned by the network. Both simulation and in vivo experiments were performed to validate the performance of MFCNN CLT, and it was further compared with the typical radiative transfer equation (RTE) based method. The experimental data showed the superiority of MFCNN CLT in terms of accuracy and stability. This promising approach for CLT is expected to improve the performance of optical tomography, and to promote the exploration of machine learning in biomedical applications.","Cerenkov luminescence tomography (CLT),optical reconstruction,photon propagation,neural network,inverse problem",Article,"IOP PUBLISHING LTD, TEMPLE CIRCUS, TEMPLE WAY, BRISTOL BS1 6BE, ENGLAND","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,3.554,"RECONSTRUCTION,MOUSE,MODEL",PHYSICS IN MEDICINE AND BIOLOGY,https://doi.org/10.1088/1361-6560/ab5bb4,
78,THE PROJECT OF ANOTHER LOW-COST METAPHASE FINDER (SECOND REPORT-APPLICATION OF ARTIFICIAL INTELLIGENCE),186,1,37-41,Furukawa Akira,Furukawa A,Furukawa A,10.1093/rpd/ncz012,National Institutes for Quantum & Radiological Science & Technology (QST)-Japan,"Biological dosimetry is used to estimate individual absorbed radiation dose by quantifying an appropriate biological marker. The most popular gold-standard marker is the appearance of dicentric chromosomes in metaphase. The metaphase finder is a tool for biological dosimetry that finds metaphase cells on glass slides. The author and a software company have designed a new system and are now preparing to produce the system commercially. The metaphase finder consists of an automated microscope, a camera, and a computer. To enhance the accuracy of the system, an artificial intelligence (AI) with deep learning was tested. A total of 1709 images of the metaphase finder detected as 'metaphases' were read into a nine-layer artificial neural network to detect true metaphases. A total of 456 images were used for training, and the rest of the images were used for validation. The accuracy of AI was 0.89 for metaphases and 0.90 for non-metaphases.",,Article; Proceedings Paper,"OXFORD UNIV PRESS, GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND","Environmental Sciences & Ecology,Public, Environmental & Occupational Health,Nuclear Science & Technology,Radiology, Nuclear Medicine & Medical Imaging",,,,RADIATION PROTECTION DOSIMETRY,,
79,Production Forecast of Fractured Coalbed Methane Wells Based on PCA-NB Method,26,6,108-112,",,,,","Kong Peng,Li Zhongcheng,Yang Chenxi,Zhang Yunpeng,Wang Ruoxuan",,,Kong Peng,"In order to improve the forecast of fractured coalbed methane wellproduction,thePCA (principal component analysis)was used to determine the principal parameters influencing the coalbed methane production. On this basis, the NB (Naive Bayes)method was applied to train the principal components and production in the target block. In addition, it also presents a PCA-NB production forecast model. Research indicates that The production forecast accuracy of this PCA-NB method reaches 84.1%, which is significant higher than that of PCA method (forecast accuracy is 64.8%), NB method (forecast accuracy is 76.4%)and neural network method (forecast accuracy is 73.5%). This research is of certain significance for the well-interval optimization and fracturing optimization of coalbed methane wells.",coalbed methane; production forecast; Naive Bayes; principal component analysis; fracture,Article,,,,,,,,
80,Using EEG for Mental Fatigue Assessment: A Comprehensive Look Into the Current State of the Art,49,6,599-610,"Monteiro Thiago Gabriel,Skourup Charlotte,Zhang Houxiang","Monteiro TG,Skourup C,Zhang HX",Monteiro TG,10.1109/THMS.2019.2938156,Norwegian University of Science & Technology (NTNU),"This paper provides a brief survey of recent developments on the use of electroencephalogram (EEG) sensors for detecting mental fatigue (MF) in human operators during tasks involving human-machine interaction. This research topic has received much attention since there is a consensus among experts on the increasing relation between human failure and accidents in safety-critical tasks. MF is one of the most influential aspects leading to human failure and the most reliable way to assess it is using operators physiological data, especially EEG. In the past few decades, hundreds of publications have explored the use of EEG alone or together with other objective and subjective measures for assessing MF, drowsiness, and tiredness in human operators. With recent improvements in data preprocessing, feature extraction, and classification algorithms, the monitoring and mitigation of MF in real time has become a reality. This trend is mainly due to the increasing use of machine learning techniques. This paper provides a comprehensive look at the current state of the art in the field of MF detection using EEG, identifying the currently used technique, algorithms, and methods and possible trends and promising areas for further research. The paper is concluded by suggesting a kernel partial least squares discrete-output linear regression based model as an all-around good option for an MF assessment system.","Electroencephalography,Feature extraction,Human factors,Sensor fusion,Fatigue,Risk management,Electroencephalogram (EEG),human factors,human-machine systems,mental fatigue (MF) assessment,risk assessment,sensor fusion",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA",Computer Science,,,"EPILEPTIC,SEIZURE,DETECTION,DROWSINESS,DETECTION,AUTOMATIC,DETECTION,DRIVER,DROWSINESS,DETECTION,SYSTEM,WEARABLE,EEG,SIGNALS,ENTROPY,MODEL,SLEEPINESS",IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS,https://ntnuopen.ntnu.no/ntnu-xmlui/bitstream/11250/2618442/1/Using_EEG_for_Mental_Fatigue_Assessment__A_Comprehensive_Look_Into_the_Current__State_of_the_Art.pdf,
81,A Context-Supported Deep Learning Framework for Multimodal Brain Imaging Classification,49,6,611-622,"Jiang Jianmin,Fares Ahmed,Zhong Sheng-Hua","Jiang JM,Fares A,Zhong SH",Zhong SH,10.1109/THMS.2019.2904615,Shenzhen University,"Over the past decade, ""content-based"" multimedia systems have realized success. By comparison, brain imaging and classification systems demand more efforts for improvement with respect to accuracy, generalization, and interpretation. The relationship between electroencephalogram (EEG) signals and corresponding multimedia content needs to be further explored. In this paper, we integrate implicit and explicit learning modalities into a context-supported deep learning framework. We propose an improved solution for the task of brain imaging classification via EEG signals. In our proposed framework, we introduce a consistency test by exploiting the context of brain images and establishing a mapping between visual-level features and cognitive-level features inferred based on EEG signals. In this way, a multimodal approach can be developed to deliver an improved solution for brain imaging and its classification based on explicit learning modalities and research from the image processing community. In addition, a number of fusion techniques are investigated in this work to optimize individual classification results. Extensive experiments have been carried out, and their results demonstrate the effectiveness of our proposed framework. In comparison with the existing state-of-the-art approaches, our proposed framework achieves superior performance in terms of not only the standard visual object classification criteria, but also the exploitation of transfer learning. For the convenience of research dissemination, we make the source code publicly available for downloading at GitHub (https://github.com/aneeg/dual-modal-learning).","Electroencephalography,Brain,Deep learning,Feature extraction,Functional magnetic resonance imaging,Image classification,Deep learning,electroencephalogram (EEG),explicit learning modality,implicit learning modality,object classification",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA",Computer Science,,3.771,"CONVOLUTIONAL,NEURAL-NETWORKS,EMOTION,RECOGNITION,EEG,MUSIC",IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS,,
82,Tangent Space Features-Based Transfer Learning Classification Model for Two-Class Motor Imagery Brain-Computer Interface,29,10,,"Gaur Pramod,McCreadie Karl,Pachori Ram Bilas,Wang Hui,Prasad Girijesh","Gaur P,McCreadie K,Pachori RB,Wang H,Prasad G",Gaur P,10.1142/S0129065719500254,LNM Institute of Information Technology,"The performance of a brain-computer interface (BCI) will generally improve by increasing the volume of training data on which it is trained. However, a classifier's generalization ability is often negatively affected when highly non-stationary data are collected across both sessions and subjects. The aim of this work is to reduce the long calibration time in Bel systems by proposing a transfer learning model which can be used for evaluating unseen single trials for a subject without the need for training session data. A method is proposed which combines a generalization of the previously proposed subject-specific ""multivariate empirical-mode decomposition"" preprocessing technique by taking a fixed band of 8-30 Hz for all four motor imagery tasks and a novel classification model which exploits the structure of tangent space features drawn from the Riemannian geometry framework, that is shared among the training data of multiple sessions and subjects. Results demonstrate comparable performance improvement across multiple subjects without subject-specific calibration, when compared with other state-of-the-art techniques.","Motor imagery,brain-computer interface (BCI),tangent space,covariance matrix,multivariate empirical-mode decomposition (MEMD),subject-specific multivariate empirical-mode decomposition-based filtering (SS-MEMDBF)",Article,"WORLD SCIENTIFIC PUBL CO PTE LTD, 5 TOH TUCK LINK, SINGAPORE 596224, SINGAPORE",Computer Science,,5.439,"COVARIATE,SHIFT,DECOMPOSITION,NOISE",INTERNATIONAL JOURNAL OF NEURAL SYSTEMS,https://pure.ulster.ac.uk/ws/files/77556642/ws_ijns_revision7.pdf,
83,Epileptic Seizure Detection with EEG Textural Features and Imbalanced Classification Based on EasyEnsemble Learning,29,10,,"Sun Chengfa,Cui Hui,Zhou Weidong,Nie Weiwei,Wang Xiuying,Yuan Qi","Sun CF,Cui H,Zhou WD,Nie WW,Wang XY,Yuan Q",Yuan Q,10.1142/S0129065719500217,Shandong Normal University,"Imbalance data classification is a challenging task in automatic seizure detection from electroencephalogram (EEG) recordings when the durations of non-seizure periods are much longer than those of seizure activities. An imbalanced learning model is proposed in this paper to improve the identification of seizure events in long-term EEG signals. To better represent the underlying microstructure distributions of EEG signals while preserving the non-stationary nature, discrete wavelet transform (DWI') and uniform 1D-LBP feature extraction procedure are introduced. A learning framework is then designed by the ensemble of weakly trained support vector machines (SVMs). Under-sampling is employed to split the imbalanced seizure and non-seizure samples into multiple balanced subsets where each of them is utilized to train an individual SVM classifier. The weak SVMs are incorporated to build a strong classifier which emphasizes seizure samples and in the meantime analyzing the unbalanced class distribution of EEG data. Final seizure detection results are obtained in a multi-level decision fusion process by considering temporal and frequency factors. The model was validated over two long-term and one short-term public EEG databases. The model achieved a G-mean of 97.14% with respect to epoch-level assessment, an event-level sensitivity of 96.67%, and a false detection rate of 0.86/h on the long-term intracranial database. An epoch-level G-mean of 95.28% and event-level false detection rate of 0.81/h were yielded over the long-term scalp database. The comparisons with 14 published methods demonstrated the improved detection performance for imbalanced EEG signals and the generalizability of the proposed model.","Seizure detection,imbalanced classification,EasyEnsemble learning,textural feature,local binary pattern",Article,"WORLD SCIENTIFIC PUBL CO PTE LTD, 5 TOH TUCK LINK, SINGAPORE 596224, SINGAPORE",Computer Science,,5.439,"NEURAL-NETWORKS,BINARY,PATTERNS,RECOGNITION,TRANSFORM,ALGORITHM,DISTANCE,MACHINE,ENTROPY,RECORDS,SYSTEM",INTERNATIONAL JOURNAL OF NEURAL SYSTEMS,,
84,Moving in time: Simulating how neural circuits enable rhythmic enactment of planned sequences,120,,86-107,"Zeid Omar,Bullock Daniel","Zeid O,Bullock D",Zeid O,10.1016/j.neunet.2019.08.006,Vanderbilt University,"Many complex actions are mentally pre-composed as plans that specify orderings of simpler actions. To be executed accurately, planned orderings must become active in working memory, and then enacted one-by-one until the sequence is complete. Examples include writing, typing, and speaking. In cases where the planned complex action is musical in nature (e.g. a choreographed dance or a piano melody), it appears to be possible to deploy two learned sequences at the same time, one composed from actions and a second composed from the time intervals between actions. Despite this added complexity, humans readily learn and perform rhythm-based action sequences. Notably, people can learn action sequences and rhythmic sequences separately, and then combine them with little trouble (Ullen & Bengtsson 2003). Related functional MRI data suggest that there are distinct neural regions responsible for the two different sequence types (Bengtsson et al. 2004). Although research on musical rhythm is extensive, few computational models exist to extend and inform our understanding of its neural bases. To that end, this article introduces the TAMSIN (Timing And Motor System Integration Network) model, a systems-level neural network model capable of performing arbitrary item sequences in accord with any rhythmic pattern that can be represented as a sequence of integer multiples of a base interval. In TAMSIN, two Competitive Queuing (CQ) modules operate in parallel. One represents and controls item order (the ORD module) and the second represents and controls the sequence of inter-onset-intervals (IOIs) that define a rhythmic pattern (RHY module). Further circuitry helps these modules coordinate their signal processing to enable performative output consistent with a desired beat and tempo. (C) 2019 Elsevier Ltd. All rights reserved.","Competitive queuing,Computational neuroscience,Music performance,Rhythm,Systems neuroscience",Article,"PERGAMON-ELSEVIER SCIENCE LTD, THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND","Computer Science,Neurosciences & Neurology",,,"ORDINAL,STRUCTURE,WORKING-MEMORY,PREFRONTAL,CORTEX,ARM,MOVEMENTS,SERIAL,ORDER,MODEL,MOTOR,PERCEPTION,SELECTION,SPEECH",NEURAL NETWORKS,,
85,The quantization error in a Self-Organizing Map as a contrast and colour specific indicator of single-pixel change in large random patterns,120,,116-128,"Wandeto John M.,Dresp-Langley Birgitta","Wandeto JM,Dresp-Langley B",Dresp-Langley B,10.1016/j.neunet.2019.09.017,Centre National de la Recherche Scientifique (CNRS),"The quantization error in a fixed-size Self-Organizing Map (SOM) with unsupervised winner-take-all learning has previously been used successfully to detect, in minimal computation time, highly meaningful changes across images in medical time series and in time series of satellite images. Here, the functional properties of the quantization error in SOM are explored further to show that the metric is capable of reliably discriminating between the finest differences in local contrast intensities and contrast signs. While this capability of the QE is akin to functional characteristics of a specific class of retinal ganglion cells (the so-called Y-cells) in the visual systems of the primate and the cat, the sensitivity of the QE surpasses the capacity limits of human visual detection. Here, the quantization error in the SOM is found to reliably signal changes in contrast or colour when contrast information is removed from or added to the image, but not when the amount and relative weight of contrast information is constant and only the local spatial position of contrast elements in the pattern changes. While the RGB Mean reflects coarser changes in colour or contrast well enough, the SOM-QE is shown to outperform the RGB Mean in the detection of single-pixel changes in images with up to five million pixels. This could have important implications in the context of unsupervised image learning and computational building block approaches to large sets of image data (big data), including deep learning blocks, and automatic detection of contrast change at the nanoscale in Transmission or Scanning Electron Micrographs (TEM, SEM), or at the subpixel level in multispectral and hyper-spectral imaging data. (C) 2019 Elsevier Ltd. All rights reserved.","Self-Organizing Maps,Quantization error,Image time series,Medical images,Random-dot images,Change detection",Article,"PERGAMON-ELSEVIER SCIENCE LTD, THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND","Computer Science,Neurosciences & Neurology",,9.171,RECEPTIVE-FIELDS,NEURAL NETWORKS,http://arxiv.org/pdf/2011.03970,
86,The effects of high variability training on voice identity learning,193,,,"Lavan Nadine,Knight Sarah,Hazan Valerie,McGettigan Carolyn","Lavan N,Knight S,Hazan V,McGettigan C",Lavan N; McGettigan C,10.1016/j.cognition.2019.104026,University of London,"High variability training has been shown to benefit the learning of new face identities. In three experiments, we investigated whether this is also the case for voice identity learning. In Experiment 1a, we contrasted high variability training sets - which included stimuli extracted from a number of different recording sessions, speaking environments and speaking styles - with low variability stimulus sets that only included a single speaking style (read speech) extracted from one recording session (see Ritchie & Burton, 2017 for faces). Listeners were tested on an old/new recognition task using read sentences (i.e. test materials fully overlapped with the low variability training stimuli) and we found a high variability disadvantage. In Experiment 1b, listeners were trained in a similar way, however, now there was no overlap in speaking style or recording session between training sets and test stimuli. Here, we found a high variability advantage. In Experiment 2, variability was manipulated in terms of the number of unique items as opposed to number of unique speaking styles. Here, we contrasted the high variability training sets used in Experiment 1a with low variability training sets that included the same breadth of styles, but fewer unique items; instead, individual items were repeated (see Murphy, Ipser, Gaigg, & Cook, 2015 for faces). We found only weak evidence for a high variability advantage, which could be explained by stimulus-specific effects. We propose that high variability advantages may be particularly pronounced when listeners are required to generalise from trained stimuli to different-sounding, previously unheard stimuli. We discuss these findings in the context of mechanisms thought to underpin advantages for high variability training.","Voice identity,Person perception,High variability training,Voice learning",Article,"ELSEVIER, RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS",Psychology,,4.327,"FUNDAMENTAL-FREQUENCY,ACOUSTIC,VARIABILITY,SPEAKING,RATE,FACE,2ND-LANGUAGE,RECOGNITION,HYPOTHESIS,PERCEPTION,FAMILIAR",COGNITION,https://qmro.qmul.ac.uk/xmlui/bitstream/123456789/72569/2/Lavan%20The%20effects%20of%202019%20Accepted.pdf,
87,A comparative study at the flow behavior description of 2A14 alloy using BP-ANN and strain compensated Arrhenius model,6,12,,"Li Shi-Shan,Zhou Jie,Zhang Meng-Meng,Zhou Yu-Ting,Gong-Ye Fan-Jiao,Long Shuai,Dong Xu-Gang,Liang Qiang","Li SS,Zhou J,Zhang MM,Zhou YT,Gong-Ye FJ,Long S,Dong XG,Liang Q",Zhou J,10.1088/2053-1591/ab68e0,Chongqing University,"In this work, a back-propagation artificial neural network (BP-ANN) and Arrhenius constitutive model were used to predict the stress-strain curves of 2A14 aluminum alloy based on the results of isothermal compression tests conducted at the temperature of 648 K-723 K and the strain rate of 0.01 s(-1) - 10 s(-1). A series of statistical analyses were introduced to compare the accuracy of predictions of the two models. The average absolute relative error (AARE), correlation coefficient (R), relative error (mu) and standard deviation (omega) were 0.4338%, 0.9997, 0.2384, 0.0242 by BP-ANN model and 3.06%, 0.9941, 1.7993, 2.6610 by Arrhenius constitutive model, respectively, which indicates that the trained BP-ANN model is more precise than the Arrhenius constitutive model. Then the finite element simulations were conducted under the same deformation conditions on the basis of the pure experimental results and pure BP-ANN predicted results. As a result, the stroke load curve and the distributions of the effective strain are similar, which further proves that the BP-ANN model have a good capability to predict the flow behavior of 2A14 alloy.","hot compression,BP-ANN model,arrhenius model,flow stress prediction,finite element simulation",Article,"IOP PUBLISHING LTD, TEMPLE CIRCUS, TEMPLE WAY, BRISTOL BS1 6BE, ENGLAND",Materials Science,,1.618,"HOT,DEFORMATION-BEHAVIOR,TI-6CR-5MO-5V-4AL,ALLOY,MECHANICAL-PROPERTIES,DENDRITE,GROWTH,NEURAL-NETWORKS,WELDING,POOL,MICROSTRUCTURE,PREDICTION,STRESS,RANGE",MATERIALS RESEARCH EXPRESS,https://doi.org/10.1088/2053-1591/ab68e0,
88,The Use of Machine Learning in the Analytical Control of the Preparations of Medicinal Plants,55,14,1428-1438,"Nazarenko D. V,Rodin I. A.,Shpigun Oleg","Nazarenko DV,Rodin IA,Shpigun O",Nazarenko DV,10.1134/S0020168519140115,Lomonosov Moscow State University,"Despite the fact that the global market for medicinal plants amounts to hundreds of billions of dollars, there is almost no government control over the quality of such pharmaceuticals in most countries of the world. This is partly attributed to the complex composition of plant materials: traditional analytical methodology is based on the use of standard reference samples for each analyte. In this case, preparations based on medicinal plants may contain tens and hundreds of physiologically active components. Isolation of those compounds in a pure form in practice is carried out using preparative chromatography, which leads to their high cost. Moreover, variation of the chemical composition of medicinal plants depending on the geographical origin of the raw materials interferes with prescribing strict ranges of permissible contents for all physiologically active components. The combination of the above factors limits the possibilities of using traditional approaches to analysis requiring strict standardization, the list of compounds for each type of plant, levels of contents, and the availability of reference materials and standards of comparison. This led to the study of the possibility of introducing various mathematical approaches as an auxiliary methodology. Unlike traditional methodologies, machine learning approaches are based on the correct collection of the data samples. Such a sample should contain groups of samples that correspond to the states of the object which the developed algorithm must distinguish: authentic/fake, pure/containing impurities, effective/not containing a certain level of active components, etc. This review is devoted to consideration of the application of machine learning techniques to the problems of chemical analysis and production control of raw materials of medicinal plants and preparations on their basis for the last 15 years.","machine learning,medicinal plants,production and technological control,pharmacology,analytical chemistry",Article,"MAIK NAUKA/INTERPERIODICA/SPRINGER, 233 SPRING ST, NEW YORK, NY 10013-1578 USA",Materials Science,,0.902,"NEAR-INFRARED,SPECTROSCOPY,TANDEM,MASS-SPECTROMETRY,LIQUID-CHROMATOGRAPHY,QUALITY-CONTROL,GEOGRAPHICAL,ORIGIN,HERBAL,MEDICINES,RAPID,AUTHENTICATION,DISCRIMINATION,MS,PERFORMANCE",INORGANIC MATERIALS,,
89,A distance map regularized CNN for cardiac cine MR image segmentation,46,12,5637-5651,"Dangi Shusil,Linte Cristian A.,Yaniv Ziv","Dangi S,Linte CA,Yaniv Z",Dangi S,10.1002/mp.13853,Rochester Institute of Technology,"Purpose: Cardiac image segmentation is a critical process for generating personalized models of the heart and for quantifying cardiac performance parameters. Fully automatic segmentation of the left ventricle (LV), the right ventricle (RV), and the myocardium from cardiac cine MR images is challenging due to variability of the normal and abnormal anatomy, as well as the imaging protocols. This study proposes a multi-task learning (MTL)-based regularization of a convolutional neural network (CNN) to obtain accurate segmenation of the cardiac structures from cine MR images.
Methods: We train a CNN network to perform the main task of semantic segmentation, along with the simultaneous, auxiliary task of pixel-wise distance map regression. The network also predicts uncertainties associated with both tasks, such that their losses are weighted by the inverse of their corresponding uncertainties. As a result, during training, the task featuring a higher uncertainty is weighted less and vice versa. The proposed distance map regularizer is a decoder network added to the bottleneck layer of an existing CNN architecture, facilitating the network to learn robust global features. The regularizer block is removed after training, so that the original number of network parameters does not change. The trained network outputs per-pixel segmentation when a new patient cine MR image is provided as an input.
Results: We show that the proposed regularization method improves both binary and multi-class segmentation performance over the corresponding state-of-the-art CNN architectures. The evaluation was conducted on two publicly available cardiac cine MRI datasets, yielding average Dice coefficients of 0.84 +/- 0.03 and 0.91 +/- 0.04. We also demonstrate improved generalization performance of the distance map regularized network on cross-dataset segmentation, showing as much as 42% improvement in myocardium Dice coefficient from 0.56 +/- 0.28 to 0.80 +/- 0.14.
Conclusions: We have presented a method for accurate segmentation of cardiac structures from cine MR images. Our experiments verify that the proposed method exceeds the segmentation performance of three existing state-of-the-art methods. Furthermore, several cardiac indices that often serve as diagnostic biomarkers, specifically blood pool volume, myocardial mass, and ejection fraction, computed using our method are better correlated with the indices computed from the reference, ground truth segmentation. Hence, the proposed method has the potential to become a non-invasive screening and diagnostic tool for the clinical assessment of various cardiac conditions, as well as a reliable aid for generating patient specific models of the cardiac anatomy for therapy planning, simulation, and guidance. (C) 2019 American Association of Physicists in Medicine","cardiac segmentation,convolutional neural network,magnetic resonance imaging,multi-task learning,regularization,task uncertainty weighting",Article,"WILEY, 111 RIVER ST, HOBOKEN 07030-5774, NJ USA","Radiology, Nuclear Medicine & Medical Imaging",,3.767,"LEFT-VENTRICLE,NEURAL-NETWORKS",MEDICAL PHYSICS,https://rss.onlinelibrary.wiley.com/doi/am-pdf/10.1002/mp.13853,
90,Automated polyp segmentation for colonoscopy images: A method based on convolutional neural networks and ensemble learning,46,12,5666-5676,"Guo Xudong,Zhang Na,Guo Jiefang,Zhang Huihe,Hao Youguo,Hang Jingqing","Guo XD,Zhang N,Guo JF,Zhang HH,Hao YG,Hang JQ",Guo JF,10.1002/mp.13865,PLA Second Military Medical University,"Purpose: To automatically and efficiently segment the lesion area of the colonoscopy polyp image, a polyp segmentation method has been presented.
Methods: An ensemble model of pretrained convolutional neural networks was proposed, using Unet-VGG, SegNet-VGG, and PSPNet. Firstly, the Unet-VGG is obtained by the first 10 layers of VGG16 as the contraction path of the left half of the Unet. Then, the SegNet-VGG is acquired by fine-tuned transfer learning VGG16, using the first 13 layers of VGG16 as the encoder of the SegNet and combined the original decoder of the SegNet. By adjusting the input size of the Unet-VGG, SegNet-VGG, and PSPNet, the preprocessed data can be correctly fed to the three network models. The three models are used as the basic trainer to train and segment the datasets. Based on the ensemble learning algorithm, the weight voting method is used to ensemble the segmentation results corresponding to single basic trainer.
Results: Both IoU and DICE similarity score were used to evaluate the segmentation quality for cvc300 with 300 images, CVC-ClinicDB with 612 images, and ETIS-LaribPolypDB with 196 images. From the experimental results, the IoU and DICE obtained by the proposed method for the cvc300 datasets can reach up to 96.16% and 98.04%, respectively, the IoU and DICE for the CVCC-linicDB datasets can reach up to 96.66% and 98.30%, respectively, whereas the IoU and DICE for the ETIS-LaribPolypDB datasets can reach up to 96.95% and 98.45%, respectively. Evaluation of the IoU and DICE in our methods shows higher accuracy than previous methods.
Conclusions: The experimental results show that the proposed method improved correspondingly in IoU and DICE compared to a single basic trainer. The range of improvement is 1.98%-6.38%. The proposed ensemble learning succeeds in automatic polyp segmentation, which potentially helps to establish more polyp datasets. (C) 2019 American Association of Physicists in Medicine","colonoscopy image,deep learning,ensemble learning,polyp segmentation,transfer learning",Article,"WILEY, 111 RIVER ST, HOBOKEN 07030-5774, NJ USA","Radiology, Nuclear Medicine & Medical Imaging",,3.767,,MEDICAL PHYSICS,,
91,A two-dimensional feasibility study of deep learning-based feature detection and characterization directly from CT sinograms,46,12,E790-E800,"De Man Quinten,Haneda Eri,Claus Bernhard,Fitzgerald Paul,De Man Bruno,Qian Guhan,Shan Hongming,Min James,Sabuncu Mert,Wang Ge","De Man Q,Haneda E,Claus B,Fitzgerald P,De Man B,Qian GH,Shan HM,Min J,Sabuncu M,Wang G",De Man Q; Wang G,10.1002/mp.13640,Rensselaer Polytechnic Institute,"Machine Learning, especially deep learning, has been used in typical x-ray computed tomography (CT) applications, including image reconstruction, image enhancement, image domain feature detection and image domain feature characterization. To our knowledge, this is the first study on machine learning for feature detection and analysis directly based on CT projection data. Specifically, we present neural network methods for blood vessel detection and characterization in the sinogram domain avoiding any partial volume, beam hardening, or motion artifacts introduced during reconstruction. First, we estimate sinogram domain vessel maps using a residual encoder-decoder convolutional neural network (REDCNN). Next, we estimate the vessel centerline and we extract the vessel-only sinogram from the original sinogram, eliminating any background information. Finally, we use a fully connected neural network to estimate the vessel lumen cross-sectional area from the vessel-only sinogram. We trained and tested the proposed methods using CatSim simulations, real CT measurements of vessel phantoms, and clinical data from the NIH CT image database. We achieved encouraging initial results showing the feasibility of CT analysis in the sinogram domain. In principle, sinogram domain analysis should be possible for many other and more complicated clinical CT analysis tasks. Further studies are needed for this sinogram domain analysis approach to become practical for clinical applications. (C) 2019 American Association of Physicists in Medicine.","computed tomography,deep learning,machine learning,sinogram",Article; Proceedings Paper,"WILEY, 111 RIVER ST, HOBOKEN 07030-5774, NJ USA","Radiology, Nuclear Medicine & Medical Imaging",,3.767,,MEDICAL PHYSICS,,
92,Metal artifact reduction for practical dental computed tomography by improving interpolation-based reconstruction with deep learning,46,12,E823-E834,"Liang Kaichao,Zhang Li,Yang Hongkai,Yang Yirong,Chen Zhiqiang,Xing Yuxiang","Liang KC,Zhang L,Yang HK,Yang YR,Chen ZQ,Xing YX",Xing YX,10.1002/mp.13644,Tsinghua University,"Purpose: Metal artifact is a quite common problem in diagnostic dental computed tomography (CT) images. Due to the high attenuation of heavy materials such as metal, severe global artifacts can occur in reconstructions. Typical metal artifact reduction (MAR) techniques segment out the metal regions and estimate the corrupted projection data by various interpolation methods. However, interpolations are not accurate and introduce new artifacts or even deform the teeth in the reconstructed image. This work presents a new strategy to take advantage of the power of deep learning for metal artifact reduction.
Method: The analysis first uses coarse reconstructions from simulated locally interpolated data affected by metal fillings as a starting point. A deep learning network is then trained using the simulated data and applied to practical data. Thus, an easily implemented three-step MAR method is formed: Firstly, use the acquired projection data to create a preliminary image reconstruction with linearly interpolated data for the metal-related projections. Secondly, a deep learning network is used to remove the artifacts from the linear interpolation and recover the nonmetal region information. Thirdly, the method adds the ROI reconstruction of the metal regions. The structures behind the shading artifacts in the direct filtered back-projection (FBP) reconstruction can be partially recovered by interpolation-based MAR (I-MAR) with the network further correcting for interpolation errors. The key to this method is that the linear interpolation reconstruction errors can be easily simulated to train a network and the effectiveness of the network can be easily generalized to I-MAR results in real situations.
Results: We trained a network with a simulation dataset and validated the network against a separate simulation dataset. Then, the network was tested using simulation data that did not overlap with the training/validation datasets and real patient datasets. Both tests gave encouraging results with accurate tooth structure recovery and few artifacts. The relative root mean square error and structure similarity index method indexes were significantly improved in the tests. The method was also evaluated by two experienced dentists who gave positive evaluations.
Conclusions: This work presents a strategy to build a transferable learning from simulations to practical systems for metal artifact reduction using a supervised deep learning method. The system transforms the MAR analyses to an interpolation-artifact reduction problem to recover structural details from the coarse interpolation reconstruction. In this way, training data from simulations with ground truth labels can easily model the similar features in real data with I-MAR as the bridge. The network can seamlessly optimize both simulations and real data. The whole method is easily implemented with little computational cost. Test results demonstrated that this is an effective MAR method applicable to practical dental CT systems. (C) 2019 American Association of Physicists in Medicine.","computed tomography,deep learning,interpolation,Metal artifact reduction,U-Net",Article; Proceedings Paper,"WILEY, 111 RIVER ST, HOBOKEN 07030-5774, NJ USA","Radiology, Nuclear Medicine & Medical Imaging",,3.767,"CT,ALGORITHM",MEDICAL PHYSICS,,
93,Tongue image segmentation algorithm based on deep convolutional neural network and fully conditional random fields,45,12,2364-2374,",,,","Zhang Xinfeng,Guo Yutong,Cai Yiheng,Sun Meng",,,Zhang Xinfeng,"The disadvantage of tongue image segmentation in traditional Chinese medicine are low accuracy, slow segmentation speed and manual calibration of candidate regions. To solve these problems,we propose an end-to-end tongue image segmentation algorithm. Compared with the traditional tongue segmentation algorithm, more accurate segmentation results can be obtained by the proposed method which does not need any manual operation. Firstly,the atrous convolution algorithm is used to increase the feature map of the network without increasing the parameters. Secondly,the atrous spatial pyramid pooling (ASPP) module is used to enable the network to learn the multi-scale feature of the tongue image through different receptive fields. Finally, the deep convolutional neural networks (DCNN) are combined with fully connected conditional random fields (CRF) to refine the edge of the segmented tongue image. The experimental results show that the proposed method outperforms traditional tongue image segmentation algorithm and popular DCNN with higher segmentation accuracy,and the mean intersection over union reaches 95.41%.",deep learning; convolutional neural network (CNN); semantic segmentation; tongue images; conditional random fields (CRF),Article,,,,,,,,
94,Predicting emergency department orders with multilabel machine learning techniques and simulating effects on length of stay,26,12,1427-1436,"Hunter-Zinck Haley S.,Peck Jordan S.,Strout Tania D.,Gaehde Stephan A.","Hunter-Zinck HS,Peck JS,Strout TD,Gaehde SA",Hunter-Zinck HS,10.1093/jamia/ocz171,US Department of Veterans Affairs,"Objective: Emergency departments (EDs) continue to pursue optimal patient flow without sacrificing quality of care. The speed with which a healthcare provider receives pertinent information, such as results from clinical orders, can impact flow. We seek to determine if clinical ordering behavior can be predicted at triage during an ED visit.
Materials and Methods: Using data available during triage, we trained multilabel machine learning classifiers to predict clinical orders placed during an ED visit. We benchmarked 4 classifiers with 2 multilabel learning frameworks that predict orders independently (binary relevance) or simultaneously (random k-labelsets). We evaluated algorithm performance, calculated variable importance, and conducted a simple simulation study to examine the effects of algorithm implementation on length of stay and cost.
Results: Aggregate performance across orders was highest when predicting orders independently with a multilayer perceptron (median F-1 score = 0.56), but prediction frameworks that simultaneously predict orders for a visit enhanced predictive performance for correlated orders. Visit acuity was the most important predictor for most orders. Simulation results indicated that direct implementation of the model would increase ordering costs (from $21 to $45 per visit) but reduce length of stay (from 158 minutes to 151 minutes) over all visits.
Discussion: Simulated implementations of the predictive algorithm decreased length of stay but increased ordering costs. Optimal implementation of these predictions to reduce patient length of stay without incurring additional costs requires more exploration.
Conclusions: It is possible to predict common clinical orders placed during an ED visit with data available at triage.","machine learning,emergency medicine,clinical decision support systems",Article,"OXFORD UNIV PRESS, GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND","Computer Science,Health Care Sciences & Services,Information Science & Library Science,Medical Informatics",,5.178,"DECISION-SUPPORT,IMPROVE,TRIAGE,SAFETY,CARE",JOURNAL OF THE AMERICAN MEDICAL INFORMATICS ASSOCIATION,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7647143,
95,A nonparametric updating method to correct clinical prediction model drift,26,12,1448-1457,"Davis Sharon E.,Greevy Robert A. Jr.,Fonnesbeck Christopher,Lasko Thomas A.,Walsh Colin G.,Matheny Michael E.","Davis SE,Greevy RA,Fonnesbeck C,Lasko TA,Walsh CG,Matheny ME",Davis SE,10.1093/jamia/ocz127,Vanderbilt University,"Objective: Clinical prediction models require updating as performance deteriorates over time. We developed a testing procedure to select updating methods that minimizes overfitting, incorporates uncertainty associated with updating sample sizes, and is applicable to both parametric and nonparametric models.
Materials and Methods: We describe a procedure to select an updating method for dichotomous outcome models by balancing simplicity against accuracy. We illustrate the test's properties on simulated scenarios of population shift and 2 models based on Department of Veterans Affairs inpatient admissions.
Results: In simulations, the test generally recommended no update under no population shift, no update or modest recalibration under case mix shifts, intercept correction under changing outcome rates, and refitting under shifted predictor-outcome associations. The recommended updates provided superior or similar calibration to that achieved with more complex updating. In the case study, however, small update sets lead the test to recommend simpler updates than may have been ideal based on subsequent performance.
Discussion: Our test's recommendations highlighted the benefits of simple updating as opposed to systematic refitting in response to performance drift. The complexity of recommended updating methods reflected sample size and magnitude of performance drift, as anticipated. The case study highlights the conservative nature of our test.
Conclusions: This new test supports data-driven updating of models developed with both biostatistical and machine learning approaches, promoting the transportability and maintenance of a wide array of clinical prediction models and, in turn, a variety of applications relying on modern prediction tools.","predictive analytics,calibration,model updating",Article,"OXFORD UNIV PRESS, GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND","Computer Science,Health Care Sciences & Services,Information Science & Library Science,Medical Informatics",,5.178,"RISK,PREDICTION,PERFORMANCE,CARE,VALIDATION,REGRESSION,PROGNOSIS,ANALYTICS,IMPACT,TIME",JOURNAL OF THE AMERICAN MEDICAL INFORMATICS ASSOCIATION,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6857513,
96,What health records data are required for accurate prediction of suicidal behavior?,26,12,1458-1465,"Simon Gregory E.,Shortreed Susan M.,Johnson Eric,Rossom Rebecca C.,Lynch Frances L.,Ziebell Rebecca,Penfold Robert B.","Simon GE,Shortreed SM,Johnson E,Rossom RC,Lynch FL,Ziebell R,Penfold RB",Simon GE,10.1093/jamia/ocz136,Kaiser Permanente,"Objective: The study sought to evaluate how availability of different types of health records data affect the accuracy of machine learning models predicting suicidal behavior.
Materials and Methods: Records from 7 large health systems identified 19 061 056 outpatient visits to mental health specialty or general medical providers between 2009 and 2015. Machine learning models (logistic regression with penalized LASSO [least absolute shrinkage and selection operator] variable selection) were developed to predict suicide death (n = 1240) or probable suicide attempt (n = 24 133) in the following 90 days. Base models were used only historical insurance claims data and were then augmented with data regarding sociodemographic characteristics (race, ethnicity, and neighborhood characteristics), past patient-reported outcome questionnaires from electronic health records, and data (diagnoses and questionnaires) recorded during the visit.
Results: For prediction of any attempt following mental health specialty visits, a model limited to historical insurance claims data performed approximately as well (C-statistic 0.843) as a model using all available data (C-statistic 0.850). For prediction of suicide attempt following a general medical visit, addition of data recorded during the visit yielded a meaningful improvement over a model using all data up to the prior day (C-statistic 0.853 vs 0.838).
Discussion: Results may not generalize to setting with less comprehensive data or different patterns of care. Even the poorest-performing models were superior to brief self-report questionnaires or traditional clinical assessment.
Conclusions: Implementation of suicide risk prediction models in mental health specialty settings may be less technically demanding than expected. In general medical settings, however, delivery of optimal risk predictions at the point of care may require more sophisticated informatics capability.","suicide,machine learning,risk prediction,insurance claims,electronic health records,patient-reported outcomes",Article,"OXFORD UNIV PRESS, GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND","Computer Science,Health Care Sciences & Services,Information Science & Library Science,Medical Informatics",,5.178,"ASSESS,RISK,RESILIENCE,AREA,ARMY",JOURNAL OF THE AMERICAN MEDICAL INFORMATICS ASSOCIATION,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6857508,
97,Assessing clinical heterogeneity in sepsis through treatment patterns and machine learning,26,12,1466-1477,"Fohner Alison E.,Greene John D.,Lawson Brian L.,Chen Jonathan H.,Kipnis Patricia,Escobar Gabriel J.,Liu Vincent X.","Fohner AE,Greene JD,Lawson BL,Chen JH,Kipnis P,Escobar GJ,Liu VX",Fohner AE,10.1093/jamia/ocz106,University of Washington,"Objective: To use unsupervised topic modeling to evaluate heterogeneity in sepsis treatment patterns contained within granular data of electronic health records.
Materials and Methods: A multicenter, retrospective cohort study of 29 253 hospitalized adult sepsis patients between 2010 and 2013 in Northern California. We applied an unsupervised machine learning method, Latent Dirichlet Allocation, to the orders, medications, and procedures recorded in the electronic health record within the first 24 hours of each patient's hospitalization to uncover empiric treatment topics across the cohort and to develop computable clinical signatures for each patient based on proportions of these topics. We evaluated how these topics correlated with common sepsis treatment and outcome metrics including inpatient mortality, time to first antibiotic, and fluids given within 24 hours.
Results: Mean age was 70 +/- 17 years with hospital mortality of 9.6%. We empirically identified 42 clinically recognizable treatment topics (eg, pneumonia, cellulitis, wound care, shock). Only 43.1% of hospitalizations had a single dominant topic, and a small minority (7.3%) had a single topic comprising at least 80% of their overall clinical signature. Across the entire sepsis cohort, clinical signatures were highly variable.
Discussion: Heterogeneity in sepsis is a major barrier to improving targeted treatments, yet existing approaches to characterizing clinical heterogeneity are narrowly defined. A machine learning approach captured substantial patient- and population-level heterogeneity in treatment during early sepsis hospitalization.
Conclusion: Using topic modeling based on treatment patterns may enable more precise clinical characterization in sepsis and better understanding of variability in sepsis presentation and outcomes.","infection,machine learning,latent Dirichlet allocation,treatment heterogeneity,topic modeling",Article,"OXFORD UNIV PRESS, GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND","Computer Science,Health Care Sciences & Services,Information Science & Library Science,Medical Informatics",,,"INTERNATIONAL,CONSENSUS,DEFINITIONS,SEPTIC,SHOCK,HOSPITAL,MORTALITY,CARE,CRITERIA,ASSOCIATION,INPATIENT,OUTCOMES,MODELS,TRENDS",JOURNAL OF THE AMERICAN MEDICAL INFORMATICS ASSOCIATION,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7647146,
98,Detecting conversation topics in primary care office visits from transcripts of patient-provider interactions,26,12,1493-1504,"Park Jihyun,Kotzias Dimitrios,Kuo Patty,Logan Robert L.,Merced Kritzia,Singh Sameer,Tanana Michael,Taniskidou Efi Karra,Lafata Jennifer Elston,Atkins David C.","Park J,Kotzias D,Kuo P,Logan RL,Merced K,Singh S,Tanana M,Taniskidou EK,Lafata JE,Atkins DC",Park J,10.1093/jamia/ocz140,University of California System,"Objective: Amid electronic health records, laboratory tests, and other technology, office-based patient and provider communication is still the heart of primary medical care. Patients typically present multiple complaints, requiring physicians to decide how to balance competing demands. How this time is allocated has implications for patient satisfaction, payments, and quality of care. We investigate the effectiveness of machine learning methods for automated annotation of medical topics in patient-provider dialog transcripts.
Materials and Methods: We used dialog transcripts from 279 primary care visits to predict talk-turn topic labels. Different machine learning models were trained to operate on single or multiple local talk-turns (logistic classifiers, support vector machines, gated recurrent units) as well as sequential models that integrate information across talk-turn sequences (conditional random fields, hidden Markov models, and hierarchical gated recurrent units).
Results: Evaluation was performed using cross-validation to measure 1) classification accuracy for talk-turns and 2) precision, recall, and F1 scores at the visit level. Experimental results showed that sequential models had higher classification accuracy at the talk-turn level and higher precision at the visit level. Independent models had higher recall scores at the visit level compared with sequential models.
Conclusions: Incorporating sequential information across talk-turns improves the accuracy of topic prediction in patient-provider dialog by smoothing out noisy information from talk-turns. Although the results are promising, more advanced prediction techniques and larger labeled datasets will likely be required to achieve prediction performance appropriate for real-world clinical applications.","classification,patient care,communication",Article,"OXFORD UNIV PRESS, GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND","Computer Science,Health Care Sciences & Services,Information Science & Library Science,Medical Informatics",,5.178,"ELECTRONIC,HEALTH,RECORD,MEDICAL-RECORDS,DE-IDENTIFICATION,SELF-MANAGEMENT,COMMUNICATION,PHYSICIANS,INFORMATION,TIME,OPPORTUNITIES,ALLOCATION",JOURNAL OF THE AMERICAN MEDICAL INFORMATICS ASSOCIATION,https://doi.org/10.1093/jamia/ocz140,
99,The machine giveth and the machine taketh away: a parrot attack on clinical text deidentified with hiding in plain sight,26,12,1536-1544,"Carrell David S.,Cronkite David J.,Li Muqun (Rachel),Nyemba Steve,Malin Bradley A.,Aberdeen John S.,Hirschman Lynette","Carrell DS,Cronkite DJ,Li MQ,Nyemba S,Malin BA,Aberdeen JS,Hirschman L",Carrell DS,10.1093/jamia/ocz114,Kaiser Permanente,"Objective: Clinical corpora can be deidentified using a combination of machine-learned automated taggers and hiding in plain sight (HIPS) resynthesis. The latter replaces detected personally identifiable information (PII) with random surrogates, allowing leaked PII to blend in or ""hide in plain sight."" We evaluated the extent to which a malicious attacker could expose leaked PII in such a corpus.
Materials and Methods: We modeled a scenario where an institution (the defender) externally shared an 800-note corpus of actual outpatient clinical encounter notes from a large, integrated health care delivery system in Washington State. These notes were deidentified by a machine-learned PII tagger and HIPS resynthesis. A malicious attacker obtained and performed a parrot attack intending to expose leaked PII in this corpus. Specifically, the attacker mimicked the defender's process by manually annotating all PII-like content in half of the released corpus, training a PII tagger on these data, and using the trained model to tag the remaining encounter notes. The attacker hypothesized that untagged identifiers would be leaked PII, discoverable by manual review. We evaluated the attacker's success using measures of leak-detection rate and accuracy.
Results: The attacker correctly hypothesized that 211 (68%) of 310 actual PII leaks in the corpus were leaks, and wrongly hypothesized that 191 resynthesized PII instances were also leaks. One-third of actual leaks remained undetected.
Discussion and Conclusion: A malicious parrot attack to reveal leaked PII in clinical text deidentified by machine-learned HIPS resynthesis can attenuate but not eliminate the protective effect of HIPS deidentification.","deidentification,patient privacy,machine learning,natural language processing,patient data privacy",Article,"OXFORD UNIV PRESS, GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND","Computer Science,Health Care Sciences & Services,Information Science & Library Science,Medical Informatics",,5.178,"OF-THE-ART,DE-IDENTIFICATION,INFORMATION,PRIVACY,SYSTEM,POLICY",JOURNAL OF THE AMERICAN MEDICAL INFORMATICS ASSOCIATION,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6857511,
100,"Reducing drug prescription errors and adverse drug events by application of a probabilistic, machine-learning based clinical decision support system in an inpatient setting",26,12,1560-1565,"Segal G.,Segev A.,Brom A.,Lifshitz Y.,Wasserstrum Y.,Zimlichman E.","Segal G,Segev A,Brom A,Lifshitz Y,Wasserstrum Y,Zimlichman E",Segal G,10.1093/jamia/ocz135,"Internal Med T, 2 Sheba Rd, IL-5265601 Ramat Gan, Israel.","Background: Drug prescription errors are made, worldwide, on a daily basis, resulting in a high burden of morbidity and mortality. Existing rule-based systems for prevention of such errors are unsuccessful and associated with substantial burden of false alerts.
Objective: In this prospective study, we evaluated the accuracy, validity, and clinical usefulness of medication error alerts generated by a novel system using outlier detection screening algorithms, used on top of a legacy standard system, in a real-life inpatient setting.
Materials and Methods: We integrated a novel outlier system into an existing electronic medical record system, in a single medical ward in a tertiary medical center. The system monitored all drug prescriptions written during 16 months. The department's staff assessed all alerts for accuracy, clinical validity, and usefulness. We recorded all physician's real-time responses to alerts generated.
Results: The alert burden generated by the system was low, with alerts generated for 0.4% of all medication orders. Sixty percent of the alerts were flagged after the medication was already dispensed following changes in patients' status which necessitated medication changes (eg, changes in vital signs). Eighty-five percent of the alerts were confirmed clinically valid, and 80% were considered clinically useful. Forty-three percent of the alerts caused changes in subsequent medical orders.
Conclusion: A clinical decision support system that used a probabilistic, machine-learning approach based on statistically derived outliers to detect medication errors generated clinically useful alerts. The system had high accuracy, low alert burden and low false-positive rate, and led to changes in subsequent orders.","drug prescription errors,adverse drug events,outlier system,drug safety,patient safety,in-patient setting",Article,"OXFORD UNIV PRESS, GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND","Computer Science,Health Care Sciences & Services,Information Science & Library Science,Medical Informatics",,,,JOURNAL OF THE AMERICAN MEDICAL INFORMATICS ASSOCIATION,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7647149,
