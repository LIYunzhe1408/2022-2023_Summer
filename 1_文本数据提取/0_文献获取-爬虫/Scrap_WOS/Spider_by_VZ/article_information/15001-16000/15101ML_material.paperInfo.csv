,title,Volume,Issue,Pages,whole__author_name,simply_author_name,reprint author,DOI,reprint address,Abstract,Keywords,Document Type,Publisher,Research Domain,Published Date,impact_factor,Keywords_plus,joural,pdf_link,Download_SuccessOrDefeat
1,Hazards of data leakage in machine learning: A study on classification of breast cancer using deep neural networks,11314,,,"Samala Ravi K.,Chan Heang-Ping,Hadjiiski Lubomir,Koneru Sathvik","Samala RK,Chan HP,Hadjiiski L,Koneru S",Samala RK,10.1117/12.2549313,University of Michigan System,"With the renewed interest in developing machine learning methods for medical imaging using deep-learning approaches, it is essential to reexamine data leakage. In this study, we simulated data leakage in the form of feature leakage, where a classifier was trained on the training set, but the feature selection was influenced by the performance on the validation set. A pre-trained deep-learning convolutional neural network (DCNN) without fine-tuning was used as a feature extractor for malignant and benign mass classification in mammography. A feature selection algorithm was trained in the wrapper mode with a cost function tuned to follow the performance metric on the validation set. Linear discriminant analysis (LDA) classifier was trained to classify masses on mammographic patches Mammograms from 1,882 patient cases with 4,577 unique patches were partitioned by patient into 3,222 for training and 508 for validation, while 847 were sequestered as unseen independent test set to evaluate the generalization error. The effects of the finite sample size on data leakage were studied by varying the training and validation set sizes from 10% to 100% of the available sets. The area under the receiver operating characteristic curve (AUC) was used as the performance metric. The results show that the performance on the validation set could be overestimated, having AUCs of 0.75 to 0.99 for various sample sizes, whereas the independent test performance could realistically only reach an AUC of 0.72. The analysis indicates that deep learning can risk a high inflation in performance and proper housekeeping rules should be followed when designing and developing deep learning methods in medical imaging.","data leakage,feature leakage,deep-learning,convolutional neural network,transfer learning,mammography,sample size,breast cancer",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,"TISSUE,MASS",,,
2,Radiomics and artificial intelligence analysis of CT data for the identification of prognostic features in multiple myeloma,11314,,,"Schenone Daniela,Lai Rita,Cea Michele,Rossi Federica,Torri Lorenzo,Bignotti Bianca,Succio Giulia,Gualco Stefano,Conte Alessio,Dominietto Alida","Schenone D,Lai R,Cea M,Rossi F,Torri L,Bignotti B,Succio G,Gualco S,Conte A,Dominietto A",Schenone D,10.1117/12.2548983,University of Genoa,"Multiple Myeloma (MM) is a blood cancer implying bone marrow involvement, renal damages and osteolytic lesions. The skeleton involvement of MM is at the core of the present paper, exploiting radiomics and artificial intelligence to identify image-based biomarkers for MM. Preliminary results show that MM is associated to an extension of the intrabone volume for the whole body and that machine learning can identify CT image features mostly correlating with the disease evolution. This computational approach allows an automatic stratification of MM patients relying of these biomarkers and the formulation of a prognostic procedure for determining the disease follow-up.","X-ray CT,image segmentation,image features,clustering",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,http://arxiv.org/pdf/2001.08924,
3,Segmentation of uterus and placenta in MR images using a fully convolutional neural network,11314,,,"Shahedi Maysam,Dormer James D.,Devi Anusha T. T.,Do Quyen N.,Xi Yin,Lewis Matthew A.,Madhuranthakam Ananth J.,Twickler Diane M.,Fei Baowei","Shahedi M,Dormer JD,Devi TTA,Do QN,Xi Y,Lewis MA,Madhuranthakam AJ,Twickler DM,Fei BW",Fei BW,10.1117/12.2549873,University of Texas System,"Segmentation of the uterine cavity and placenta in fetal magnetic resonance (MR) imaging is useful for the detection of abnormalities that affect maternal and fetal health. In this study, we used a fully convolutional neural network for 3D segmentation of the uterine cavity and placenta while a minimal operator interaction was incorporated for training and testing the network. The user interaction guided the network to localize the placenta more accurately. We trained the network with 70 training and 10 validation MRI cases and evaluated the algorithm segmentation performance using 20 cases. The average Dice similarity coefficient was 92% and 82% for the uterine cavity and placenta, respectively. The algorithm could estimate the volume of the uterine cavity and placenta with average errors of 2% and 9%, respectively. The results demonstrate that the deep learning-based segmentation and volume estimation is possible and can potentially be useful for clinical applications of human placental imaging.","Convolutional neural network,image segmentation,placenta,uterus,fetal magnetic resonance imaging",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,https://europepmc.org/articles/pmc7261604?pdf=render,
4,Radiomic features derived from periprostatic fat on pre-surgical T2w MRI predict extraprostatic extension of prostate cancer identified on post-surgical pathology: Preliminary Results,11314,,,"Shiradkar Rakesh,Zuo Ruyuan,Mahran Amr,Ponsky Lee,Tirumani Sree Harsha,Madabhushi Anant","Shiradkar R,Zuo RY,Mahran A,Ponsky L,Tirumani SH,Madabhushi A",Shiradkar R,10.1117/12.2551248,Case Western Reserve University,"Periprostatic fat composition on T2-weighted (T2w) MRI has been shown to be associated with aggressive prostate cancer and may influence extraprostatic extension (EPE). In this study, we interrogate the periprostatic fat (PPF) region adjacent to cancer lesion on prostate T2w MRI. Patients with pathologic stage >= pT3a are considered to experience EPE (EPE+) and those with stage <= T2c are without EPE (EPE-) post radical prostatectomy (RP). We use a cohort of N = 45 prostate cancer patients retrospectively acquired from a single institution who underwent 3T multi-parametric MRI prior to RP. Radiomic features including 1st and 2nd order statistics, Haralick, Gabor, CoLlAGe features are extracted from a region of interest (ROI) in the PPF on pre-surgical T2w MRI delineated by an experienced radiologist. Haralick, gradient and CoLlAGe features were observed to be significantly different (p<0.05) in PPF ROIs between EPE+ and EPE- and were significantly over expressed in EPE+ patients compared to EPE- patients, suggesting a higher heterogeneity within the PPF region for EPE+ patients. These features were used to train machine learning classifiers using a 3-fold cross validation approach in conjunction with feature selection methods to predict EPE. The best classification performance was obtained with Support Vector Machine (SVM) classifiers resulting in an AUC = 0.88 (+/- 0.04). On univariable and multivariable analysis, we observed that radiomic classifier predictions resulted in significant separation between EPE+ and EPE- while none of the routinely used clinical parameters including prostate specific antigen (PSA), Gleason Grade Groups (GGG), age, race and prostate imaging reporting and data system (PI-RADS v2) scores showed significant differences. Our results suggest that radiomic features may quantify the underlying heterogeneity in periprostatic fat and predict patients who are likely to experience extraprostatic extension of disease post RP.","Prostate cancer,Magnetic Resonance Imaging,periprostatic fat,extraprostatic extension",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,"EXTRACAPSULAR,EXTENSION,RADICAL,PROSTATECTOMY,NOMOGRAM,RECURRENCE,ACCURACY",,,
5,Adaptation of a deep learning malignancy model from full-field digital mammography to digital breast tomosynthesis,11314,,,"Singh Sadanand,Matthews Thomas Paul,Shah Meet,Mombourquette Brent,Tsue Trevor,Long Aaron,Almohsen Ranya,Pedemonte Stefano,Su Jason","Singh S,Matthews TP,Shah M,Mombourquette B,Tsue T,Long A,Almohsen R,Pedemonte S,Su J",Singh S,10.1117/12.2549923,"Whiterabbit AI Inc, Santa Clara, CA 95054 USA.","Mammography-based screening has helped reduce the breast cancer mortality rate, but has also been associated with potential harms due to low specificity, leading to unnecessary exams or procedures, and low sensitivity. Digital breast tomosynthesis (DBT) improves on conventional mammography by increasing both sensitivity and specificity and is becoming common in clinical settings. However, deep learning (DL) models have been developed mainly on conventional 2D full-field digital mammography (FFDM) or scanned film images. Due to a lack of large annotated DBT datasets, it is difficult to train a model on DBT from scratch. In this work, we present methods to generalize a model trained on FFDM images to DBT images. In particular, we use average histogram matching (HM) and DL fine-tuning methods to generalize a FFDM model to the 2D maximum intensity projection (MIP) of DBT images. In the proposed approach, the differences between the FFDM and DBT domains are reduced via HM and then the base model, which was trained on abundant FFDM images, is fine-tuned. When evaluating on image patches extracted around identified findings, we are able to achieve similar areas under the receiver operating characteristic curve (ROC AUC) of similar to 0.9 for FFDM and similar to 0.85 for MIP images, as compared to a ROC AUC of similar to 0.75 when tested directly on MIP images.","Mammography,Tomosynthesis,Deep Learning,Domain Adaptation,Transfer Learning",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,http://arxiv.org/pdf/2001.08381,
6,Evaluating several ways to combine handcrafted feature-based systems with a deep learning system for the LUNA16 Challenge framework,11314,,,"Sonora-Mengana Alexander,Gonidakis Panagiotis,Jansen Bart,Garcia-Naranjo Juan,Vandemeulebroucke Jef","Sonora-Mengana A,Gonidakis P,Jansen B,Garcia-Naranjo J,Vandemeulebroucke J",Sonora-Mengana A,10.1117/12.2549778,Universidad de Oriente Santiago de Cuba,"Computer aided diagnosis systems are used to assist radiologists in their decision making. The sensitivity of these systems is hindered by the complexity of the structures inside the lungs. Several systems and methods have been proposed to detect and classify lung nodules, but all of them have their strengths and weaknesses. One way to overcome the weaknesses is to combine multiple systems. Systems based on handcrafted features capture a limited set of characteristics from the image, while deep learning based classifiers can deal with a wider range of structures. In this work, several ways to combine a handcrafted feature based classifier with four convolutional neural network are explored. The systems were combined merging the probabilities assigned to the detections in several ways. Support-vector machine, multilayer perceptron and random forest classifiers were used to combine the selected classifiers. The LUNA16 Challenge was used to evaluate the performance of the resulting hybrid systems. In all cases, the hybrid systems outperformed the individual systems. Although the average of sensitivities are similar for most of the combinations, the best hybrid system achieves a gain of 35 extra nodules at 4 FP per scan.","convolutional neural netwoks,support vector machine,fusion,random forest,multilayer perceptron,computer aided diagnosis,LUNA16",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,"FALSE-POSITIVE,REDUCTION,NODULE,DETECTION,LUNG,SURVIVAL,IMAGES",,,
7,Multilevel UNet for Pancreas Segmentation from Non-Contrast CT Scans through Domain Adaptation,11314,,,"Sriram Sai Aditya,Paul Angshuman,Zhu Yingying,Sandfort Veit,Pickhardt Perry J.,Summers Ronald M.","Sriram SA,Paul A,Zhu YY,Sandfort V,Pickhardt PJ,Summers RM",Sriram SA,10.1117/12.2551093,National Institutes of Health (NIH) - USA,"A persistent issue in deep learning (DL) is the inability of models to function in a domain in which they were not trained. For example, a model trained to segment an organ in MRI scans often dramatically fails when tested in the domain of computed tomography (CT) scans. Since manual segmentation is extremely time-consuming, it is often not feasible to acquire an annotated dataset in the target domain. Domain adaptation allows transfer of knowledge about a labelled source domain into a target domain. In this work, we attempt to address the differences in model performance when segmenting from intravenous contrast (IVC) enhanced or from non-contrast (NC) CT scans. Most of the publicly available, large-scale, annotated CT datasets are IVC-enhanced. However, physicians frequently use NC scans in clinical practice. This necessitates methods capable of reliably functioning across both domains. We propose a novel DL framework that can segment the pancreas from non-contrast CT scans through training with the help of IVC-enhanced CT scans. Our method first utilizes a CycleGAN to create synthetic NC (s-NC) variants from IVC scans. Subsequently, we introduce a multilevel 3D UNet architecture to perform pancreas segmentation. The proposed method significantly outperforms the baseline. Experimental results show 6.2% percent improvement compared to the baseline model in terms of the Dice coefficient. To our knowledge, this method is the first of its kind in pancreas segmentation from NC CTs.","Deep Learning,Domain Adaptation,Pancreas,Segmentation,CT Scans,Convolutional Neural Networks,Generative Adversarial Networks",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
8,CNN-based detection of distal tibial fractures in radiographic images in the setting of open growth plates,11314,,,"Starosolski Zbigniew A.,Kan J. Herman,Annapragada Ananth","Starosolski ZA,Kan JH,Annapragada A",Starosolski ZA,10.1117/12.2549297,Baylor College of Medicine,"The goal of this study was to assess the performance of a deep convolutional neural network trained with a limited number of cases for the detection of distal tibial fractures in children. We identified 516 ankle and leg radiographic exams in children (6.4 +/- 4.4 (mean +/- s.d) years) containing 2118 individual images. Radiographs with implants, casts, advanced healing, and including other pathology such as infra-osseous bone lesions were excluded. After these exclusions, 490 positive distal tibial fracture radiographs were identified and a matching number of normal radiographs ware selected, creating a dataset of 980 radiographs. These were sequentially partitioned, in 10 permutations, into a training set (784 radiographs), validation set (98 radiographs) and a test set (98 radiographs), with equal numbers of fracture and normal radiographs in each set. A modified transfer learning network based on the Xception-V3 architecture with additional fully convoluted reasoning layers was trained against each of the subsets. The best performing trained network successfully recognized 47 of 49 fractures and 47 of 49 normal exams (95.9% accuracy). The best three performing networks were all very similar, with accuracy of 95.6 +/- 0.6%. In no instances were normal physes or normal developmental epiphyseal or medial malleolus fragmentation misclassified as a fracture. Using a pre-trained deep convolutional neural network adapted to identifying or excluding distal tibial fractures in children using only a small number of radiographs is feasible and highly accurate without the need for the large training sets typically needed for network training.","Distal tibia,Pediatric fractures,Radiographs,Convolutional neural network",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,NEURAL-NETWORKS,,,
9,Deep learning methods for segmentation of lines in pediatric chest radiographs,11314,,,"Sullivan Ryan,Holste Greg,Burkow Jonathan,Alessio Adam","Sullivan R,Holste G,Burkow J,Alessio A",Alessio A,10.1117/12.2550686,Michigan State University,"Surgical procedures often require the use of catheters, tubes, and lines, collectively called lines. Misplaced lines can cause serious complications, such as pneumothorax. cardiac perforation. or thrombosis. To prevent these problems. radiologists examine chest radiographs after insertion and throughout intensive care to evaluate their placement. This process is time consuming. and incorrect interpretations occur with notable frequency(1). Fast and reliable automatic interpretations could potentially reduce the cost of these surgical operations, decrease the workload of radiologists, and improve the quality of care for patients. We develop a segmentation model which can highlight the medically relevant lines in pediatric chest radiographs using deep learning. We propose a two-stage segmentation network which first classifies whether images have medically relevant lines and then segments images with lines. For the segmentation stage, we use the popular U-Net architecture(2) substituting the encoder path with multiple state-of-the-art CNN encoders. Our study compares the performance of different permutations of model architectures for the task of highlighting lines in pediatric chest radiographs and demonstrates the effectiveness of the two-stage architecture.",,Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
10,Weak Supervision in Convolutional Neural Network for Semantic Segmentation of Diffuse Lung Diseases Using Partially Annotated Dataset,11314,,,"Suzuki Yuki,Yamagata Kazuki,Masahiro Yanagawa,Kido Shoji,Tomiyama Noriyuki","Suzuki Y,Yamagata K,Masahiro Y,Kido S,Tomiyama N",Suzuki Y,10.1117/12.2548930,Osaka University,"Computer-aided diagnosis system for diffuse lung diseases (DLDs) is necessary for the objective assessment of the lung diseases. In this paper, we develop semantic segmentation model for 5 kinds of DLDs. DLDs considered in this work are consolidation, ground glass opacity, honeycombing, emphysema, and normal. Convolutional neural network (CNN) is one of the most promising technique for semantic segmentation among machine learning algorithms. While creating annotated dataset for semantic segmentation is laborious and time consuming, creating partially annotated dataset, in which only one chosen class is annotated for each image, is easier since annotators only need to focus on one class at a time during the annotation task. In this paper, we propose a new weak supervision technique that effectively utilizes partially annotated dataset. The experiments using partially annotated dataset composed 372 CT images demonstrated that our proposed technique significantly improved segmentation accuracy.","deep learning,weak supervision,convolutional neural network,diffuse lung diseases,semantic segmentation",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,http://arxiv.org/pdf/2002.11936,
11,Automatic estimation of knee joint space narrowing by deep learning segmentation algorithms,11314,,,"Swiecicki Albert,Said Nicholas,O'Donnell Jonathan,Buda Mateusz,Li Nianyi,Jiranek William A.,Mazurowski Maciej A.","Swiecicki A,Said N,O'Donnell J,Buda M,Li NY,Jiranek WA,Mazurowski MA",Swiecicki A,10.1117/12.2551377,Duke University,"Evaluating the severity of knee osteoarthritis (OA) accounts for significant plain film workload and is a crucial component of knee radiograph interpretation, which informs surgical decision-making for costly and invasive procedures such as knee replacement. The Kellgren-Lawrence (KL) grading scale systematically and quantitatively assesses the severity of knee OA but is associated with notable inter-reader variability. In this study, we propose a deep learning method for the assessment of joint space narrowing (JSN) in the knee, which is an essential part of determining the KL grade. To determine the extent of JSN, we analyzed 99 knee radiographs to calculate the distance between the femur and tibia. Our algorithm's measurements of JSN and KL grade correlated well other radiologists' assessments. The average distance (in pixels) between the femur and tibia bones as measured by our algorithm was 9.60 for KL=0, 7.60 for KL=1, 6.89 for KL=2, 3.75 for KL=3, 1.25 for KL=4. Additionally, we used 100 manually annotated knee radiographs to train the algorithm to segment the femur and tibia bones. When evaluated on an independent set of 20 knee radiographs, the algorithm demonstrated a Dice coefficient of 96.59%. An algorithm for measurement of JSN and KL grades may play a significant role in automatically, reliably, and passively evaluating knee OA severity and influence and surgical decision-making and treatment pathways.","Joint space narrowing,segmentation,deep learning,Kellgren-Lawrence,osteoarthritis,Multicenter Osteoarthritis Study",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
12,Generative adversarial network-based image completion to identify abnormal locations in digital breast tomosynthesis images,11314,,,"Swiecicki Albert,Buda Mateusz,Saha Ashirbani,Li Nianyi,Ghate Sujata V.,Walsh Ruth,Mazurowski Maciej A.","Swiecicki A,Buda M,Saha A,Li N,Ghate SV,Walsh R,Mazurowski MA",Swiecicki A,10.1117/12.2551379,Duke University,"Deep learning has achieved great success in image analysis and decision making in radiology. However, a large amount of annotated imaging data is needed to construct well-performing deep learning models. A particular challenge in the context of breast cancer is the number of available cases that contain cancer, given the very low prevalence of the disease in the screening population. The question arises whether normal cases, which in the context of breast cancer screening are available in abundance, can be used to train a deep learning model that identifies locations that are abnormal. In this study, we propose to achieve this goal through the generative adversarial network (GAN)-based image completion. Our hypothesis is that if a generative network has a difficulty to correctly complete a part of an image at a certain location, then such a location is likely to represent an abnormality. We test this hypothesis using a dataset of 4348 patients with digital breast tomosynthesis (DBT) imaging from our institution. We trained our model on normal only images, to be able to fill in parts of images that were artificially removed. Then, using an independent test set, at different locations in the images, we measured how difficult it was for the network to reconstruct an artificially removed patch of the image. The difficulty was measured by mean squared error (MSE) between the original removed patch and the reconstructed patch. On average, the MSE was 2.11 times higher (with standard deviation equal to 1.01) at the locations containing expert-annotated cancerous lesions than that at the locations outside those abnormal locations. Our generative approach demonstrates a great potential for using this model to aid breast cancer detection.","digital breast tomosynthesis,image completion,abnormality detection,generative adversarial network,deep learning",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
13,A Multi-stage Fusion Strategy for Multi-scale GLCM-CNN Model in Differentiating Malignant from Benign Polyps,11314,,,"Tan Jiaxing,Zhang Shu,Cao Weiguo,Gao Yongfeng,Li Lihong,Huo Yumei,Liang Zhengrong","Tan JX,Zhang S,Cao WG,Gao YF,Li LH,Huo YM,Liang ZR",Liang ZR,10.1117/12.2549831,State University of New York (SUNY) System,"Computer aided diagnosis (CADx) of polyps has shown great potential to advance the computed tomography colonography (CTC) technique with diagnostic capability. Facing the problem of numerous uncertainties such as polyp size, shape, and orientation in CTC, GLCM-CNN has been proved to be an effective deep learning based tumor classification method, where convolution neural network (CNN) makes decision based on the texture pattern encoded in gray level co-occurrence matrix (GLCM) containing 13 directions. The 13 directional GLCM, by sampling displacement, can be classified into 3 subgroups. Based on our evaluation on the information encoded in the three subgroups, we propose a multi-stage fusion CNN model, which makes the final decision based on two types of features, i.e. (1) a gate module selected group-specific features and (2) fused features learnt from all the features from three groups. On our polyp dataset, which contains 87 polyp masses, our proposed method outperforms both single sub-group based and 13 directional GLCM based CNN model by at least 1.3% in AUC by the average of 20 times 2 fold cross validation experiment results.","CT colonography,colonic polyps,gray level co-occurrence matrix,Deep Learning,Data Fusion",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,FEATURES,,,
14,"Comparative performance of 3D-DenseNet, 3D-ResNet, and 3D-VGG models in polyp detection for CT colonography",11314,,,"Uemura Tomoki,Nappi Janne J.,Hironaka Toru,Kim Hyougseop,Yoshida Hiroyuki","Uemura T,Nappi JJ,Hironaka T,Kim H,Yoshida H",Uemura T; Nappi JJ; Yoshida H,10.1117/12.2549103,Harvard University,"Three-dimensional (3D) convolutional neural networks (CNNs) can process volumetric medical imaging data in their native volumetric input form. However, there is little information about the comparative performance of such models in medical imaging in general and in CT colonography (CTC) in particular. We compared the performance of a 3D densely connected CNN (3D-DenseNet) with those of the popular 3D residual CNN (3D-ResNet) and 3D Visual Geometry Group CNN (3D-VGG) in the reduction of false-positive detections (FPs) in computer-aided detection (CADe) of polyps in CTC. VGG is the earliest CNN design of these three models. ResNet has been used widely as a de-facto standard model for constructing deep CNNs for image classification in medical imaging. DenseNet is the most recent of these models and improves the flow of information and reduces the number of network parameters as compared to those of ResNet and VGG. For the evaluation, we used 403 CTC datasets from 203 patients. The classification performance of the CNNs was evaluated by use of 5-fold cross-validation, where the area under the receiver operating characteristic curve (AUC) was used as the figure of merit. Each training fold was balanced by use of data augmentation of the samples of real polyps. Our preliminary results showed that the AUC value of the 3D-DenseNet (0.951) was statistically significantly higher than those of the reference models (P < 0.005), indicating that the 3D-DenseNet has the potential of substantially outperforming the other models in reducing FPs in CADe for CTC. This improvement was highest for the smallest polyps.","MULTICENTER,CAD",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,"MULTICENTER,CAD",,,
15,Interpretable deep learning regression for breast density estimation on MRI,11314,,,"van der Velden Bas H. M.,Ragusi Max A. A.,Janse Markus H. A.,Loo Claudette E.,Gilhuijs Kenneth G. A.","van der Velden BHM,Ragusi MAA,Janse MHA,Loo CE,Gilhuijs KGA",van der Velden BHM,10.1117/12.2549003,Utrecht University,"Breast density, which is the ratio between fibroglandular tissue (FGT) and total breast volume, can be assessed qualitatively by radiologists and quantitatively by computer algorithms. These algorithms often rely on segmentation of breast and FGT volume. In this study, we propose a method to directly assess breast density on MRI, and provide interpretations of these assessments.
We assessed breast density in 506 patients with breast cancer using a regression convolutional neural network (CNN). The input for the CNN were slices of breast MRI of 128 x 128 voxels, and the output was a continuous density value between 0 (fatty breast) and 1 (dense breast). We used 350 patients to train the CNN, 75 for validation, and 81 for independent testing. We investigated why the CNN came to its predicted density using Deep SHapley Additive exPlanations (SHAP).
The density predicted by the CNN on the testing set was significantly correlated with the ground truth densities (N = 81 patients, Spearman's rho = 0.86, P < 0.001). When inspecting what the CNN based its predictions on, we found that voxels in FGT commonly had positive SHAP-values, voxels in fatty tissue commonly had negative SHAP-values, and voxels in non-breast tissue commonly had SHAP-values near zero. This means that the prediction of density is based on the structures we expect it to be based on, namely FGT and fatty tissue.
To conclude, we presented an interpretable deep learning regression method for breast density estimation on MRI with promising results.","Breast density,interpretable deep learning,Shapley additive explanations,Deep SHAP,MRI,CNN regression",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,"PARENCHYMAL,ENHANCEMENT,FIBROGLANDULAR,TISSUE,MAMMOGRAPHIC,DENSITY,RISK,PATTERNS",,http://arxiv.org/pdf/2012.04336,
16,The field effect in Barrett's Esophagus: a macroscopic view using white light endoscopy and deep learning,11314,,,"Verhage Levi,van der Putten Joost,van der Sommen Fons,de Groof Jeroen,Struyvenberg Maarten,de With Peter H. N.","Verhage L,van der Putten J,van der Sommen F,de Groof J,Struyvenberg M,de With PHN",Verhage L,10.1117/12.2549391,Eindhoven University of Technology,"Over the past few decades, primarily developed countries witnessed an increased incidence of esophageal adenocarcinoma (EAC). Screening and surveillance of Barrett's esophagus (BE), which is known to augment the probability of developing EAC, can significantly improve survival rates. This is because early-stage dysplasia in BE can be treated effectively, while each subsequent stage complicates successful treatment and seriously reduces survival rates. This study proposes a convolutional neural network-based algorithm, which classifies images of BE visualized with White Light Endoscopy (WLE) as either dysplastic or non-dysplastic. To this end, we use merely pixels surrounding the dysplastic region, while excluding the pixels covering the dysplastic region itself. The phenomenon where the diagnosis of a patient can be determined from tissue other than the clearly observable diseased area, is termed the field effect. With its potential to identify missed lesions, it may prove to be a helpful innovation in the screening and surveillance process of BE. A statistical significant difference test indicates the presence of the field effect in WLE, when comparing the distribution of the algorithm classifications of unseen data and the distribution obtained by a random classification.","Convolutional neural networks,Deep learning,Field effect,Barrett's Esophagus",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
17,Case-based repeatability of machine learning classification performance on breast MRI,11314,,,"Vieceli Michael,Van Dusen Amy,Drukker Karen,Abe Hiroyuki,Giger Maryellen L.,Whitney Heather M.","Vieceli M,Van Dusen A,Drukker K,Abe H,Giger ML,Whitney HM",Vieceli M,10.1117/12.2548144,Wheaton College,"Computer-aided diagnosis and radiomics have shown potential in diagnosis and prognosis of breast cancer. The purpose of this study was to investigate repeatability of classifier output and its relationship to classification performance of breast lesions imaged with dynamic contrast-enhanced MRI. Images of 1,169 breast lesions (267 benign, 902 cancers) were retrospectively collected under HIPAA/IRB compliance. The lesions were segmented automatically using a fuzzy c-means method and thirty-eight radiomic features were extracted. Three classification tasks were investigated, with different proportions of cases in each class: (i) benign (23%) vs. malignant (77%), (ii) ""pure"" ductal carcinoma in situ (DCIS) (25%) vs. DCIS with invasive ductal carcinoma (IDC) (75%), and (iii) invasive cancers of molecular subtype luminal A or luminal B (66%) vs. other molecular subtypes (34%). For each task, support vector machine classifiers were trained and tested within 0.632+ bootstrap analyses (1000 iterations) and the 0.632+ bias-corrected area under the ROC curve (AUC) served as the classification performance metric. Repeatability of classifier output was evaluated at three levels: a) repeatability by case (performance metric: width of the 95% confidence interval of classifier-estimated posterior probabilities for each case), b) repeatability within the dataset (performance metric: median and 95% confidence interval of the by-case 95% confidence interval widths), and c) potential relationship between classification performance and repeatability. In classification performance assessment, median AUCs [95% confidence interval] for the three tasks were 0.85 [0.83, 0.87], 0.84 [0.80, 0.87], and 0.65 [0.60, 0.69], respectively. In repeatability assessment within the dataset, the median confidence interval widths [95% confidence interval] for the posterior probabilities were 0.25 [0.m, 0.72], 0.34 [0.14, 0.84], and 0.23 [0.14, 0.68]. In conclusion, the classifiers in the first two tasks demonstrated strong classification performance while in all three they showed similar repeatability in posterior probabilities.","computer-aided diagnosis,breast cancer,radiomics,machine learning,repeatability",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,"COMPUTER-AIDED,DIAGNOSIS,RADIOMIC,FEATURES,LESIONS,CANCER",,,
18,Organ Segmentation From Full-size CT Images Using Memory-Efficient FCN,11314,,,"Wang Chenglong,Oda Masahiro,Mori Kensaku","Wang CL,Oda M,Mori K",Wang CL,10.1117/12.2551024,Nagoya University,"In this work, we present a memory-efficient fully convolutional network (FCN) incorporated with several memory-optimized techniques to reduce the run-time GPU memory demand during training phase. In medical image segmentation tasks, subvolume cropping has become a common preprocessing. Subvolumes (or small patch volumes) were cropped to reduce GPU memory demand. However, small patch volumes capture less spatial context that leads to lower accuracy. As a pilot study, the purpose of this work is to propose a memory-efficient FCN which enables us to train the model on full size CT image directly without subvolume cropping, while maintaining the segmentation accuracy. We optimize our network from both architecture and implementation. With the development of computing hardware, such as graphics processing unit (GPU) and tensor processing unit (TPU), now deep learning applications is able to train networks with large datasets within acceptable time. Among these applications, semantic segmentation using fully convolutional network (FCN) also has gained a significant improvement against traditional image processing approaches in both computer vision and medical image processing fields. However, unlike general color images used in computer vision tasks, medical images have larger scales than color images such as 3D computed tomography (CT) images, micro CT images, and histopathological images. For training these medical images, the large demand of computing resource become a severe problem. In this paper, we present a memory-efficient FCN to tackle the high GPU memory demand challenge in organ segmentation problem from clinical CT images. The experimental results demonstrated that our GPU memory demand is about 40% of baseline architecture, parameter amount is about 30% of the baseline.","Organ segmentation,memory efficient,FCN",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,http://arxiv.org/pdf/2003.10690,
19,Computer-aided staging of gastric cancer using Radiomics signature on computed tomography imaging,11314,,,"Wang Lili,Wu Jie,Yang Guang,Zheng Bin","Wang LL,Wu J,Yang G,Zheng B",Wang LL,10.1117/12.2549667,Fujian Medical University,"Gastric cancer is one of the most common malignant tumors with high mortality rate worldwide. In order to optimally treating gastric cacner patients and reduce cancer mortality, it requires to accurately predict Tumor, Node, Metastasis (TNM) staging of the tumor, which will determine whether the patients need neoadjuvant chemotherapy before surgery. However, subjectively reading CT images is difficult to predict TNM with large infra- and inter-reader variability. To address this challenge, we developed and tested a new CAD approach that uses radiomics features computed from the segmented tumor regions depicting on CT images to build a machine learning classifier to predict TNM and divide patients into two groups of whether need neoadjuvant chemotherapy. A CT image dataset involving 219 gastric cancer patients was retrospectively assembled and used in this study. In addition, 3 clinicopathological markers were also acquired and used. From an initial pool of 367 radiomics features, an optimal set of 11 features was selected. Then, 3 machine learning classifiers using (1) 11 CT image features, (2) 3 clinicopathological markers, and (3) fusion of both 11 CT image features and 3 clinicopathological markers, were trained and tested using a leave-one-case-out validation methods. Areas under ROC curves of three classifies are 0.74, 0.71, and 0.79, respectively. The results indicated that (1) radiomics image features computed from CT images carry higher discriminatory power to predict TNM than using clinicopathological markers acquired from surgically resected specimen and (2) fusion of CT image features and clinicopathological markers can further increase performance to predict TNM of gastric cancer patients.","Computer-aided detection (CAD),Radiomics,CT images of gastric cancer,predicting TNM staging using radiomics features",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
20,Hybrid deep-learning model for volume segmentation of lung nodules in CT images,11314,,,"Wang Yifan,Zhou Chuan,Chan Heang-Ping,Hadjiiski Lubomir M.,Wei Jun,Chughtai Aamer,Kazerooni Ella A.","Wang YF,Zhou C,Chan HP,Hadjiiski LM,Wei J,Chughtai A,Kazerooni EA",Wang YF,10.1117/12.2548994,University of Michigan System,"We are developing quantitative image analysis methods for early diagnosis of lung cancer. We designed a hybrid deep learning (H-DL) method for volume segmentation of lung nodules with large variations in size, shape, margin and opacity. In our H-DL method, two UNet++ based DL models, one used a 19-layer VGG network and the other used a 201-layer DenseNet network as backbone, were trained separately and then combined to segment nodules with wide ranges of size, shape, margin, and opacity. A data set collected from LIDC-IDRI containing 430 cases with lung nodules manually segmented by at least two radiologists was split into 352 training and 78 independent test cases. The 50% consensus consolidation of radiologists' annotation was used as the reference standard for each nodule. For the 78 test cases with 167 nodules, our H-DL model achieved an average 3D DICE coefficient of 0.732 0.158 for all nodules. For the nodules with size larger than 9.5 mm, nodules with margin described by LIDC-IDRI as sharp or spiculated, or nodules with structure described as lobulated or having solid opacity, the segmentation accuracy achieved by our H-DL model were not significantly different from the average of radiologists' manual annotation in terms of the DICE coefficient. The results demonstrated that our hybrid deep learning scheme could achieve high segmentation accuracy comparable to radiologists' average segmentations for the wide variety of nodules.","Computer-aided diagnosis,lung nodule,Deep learning",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
21,Deep Convolutional Neural Networks for Molecular Subtyping of Gliomas Using Magnetic Resonance Imaging,11314,,,"Wei Dong,Li Yiming,Wang Yinyan,Qian Tianyi,Zheng Yefeng","Wei D,Li YM,Wang YY,Qian TY,Zheng YF",Wei D,10.1117/12.2544074,Tencent,"Purpose: Knowledge of molecular subtypes of gliomas can provide valuable information for tailored therapies. This study aimed to investigate the use of deep convolutional neural networks (DCNNs) for noninvasive glioma subtyping with radiological imaging data according to the new taxonomy announced by the World Health Organization in 2016. Methods: A DCNN model was developed for the prediction of the five glioma subtypes based on a hierarchical classification paradigm. This model used three parallel, weight-sharing, deep residual-learning networks to process 2.5-dimensional input of trimodal MRI data, including T1-weighted, T1-weighted with contrast enhancement, and T2-weighted images. A data set comprising 1,016 real patients was collected for evaluation of the developed DCNN model. The predictive performance was evaluated via the area under the curve (AUC) from the receiver operating characteristic analysis. For comparison, the performance of a radiomics-based approach was also evaluated. Results: The AUCs of the DCNN model for the four classification tasks in the hierarchical classification paradigm were 0.89, 0.89, 0.85, and 0.66, respectively, as compared to 0.85, 0.75, 0.67, and 0.59 of the radiomics approach. Conclusion: The results showed that the developed DCNN model can predict glioma subtypes with promising performance, given sufficient, non-ill-balanced training data.","Glioma,molecular subtyping,magnetic resonance imaging,radiomics,deep learning",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
22,Large-Scale Extended Granger Causality (lsXGC) for Classification of Autism Spectrum Disorder from Resting-State Functional MRI,11314,,,"Wismueller Axel,Foxe John J.,Geha Paul,Saboksayr Seyed Saman","Wismuller A,Foxe JJ,Geha P,Saboksayr SS",Saboksayr SS,10.1117/12.2550027,University of Rochester,"It has been shown in the literature that Autism Spectrum Disorder (ASD) is associated with changes in brain network connectivity. Therefore, we investigate, if it is possible to capture any significant difference between brain connections of healthy subjects and ASD patients using resting-state fMRI time-series. To this end, we have developed large-scale Extended Granger Causality (lsXGC), which combines dimension reduction with source time-series augmentation and uses predictive time-series modeling for estimating directed causal relationships among resting-state fMRI time-series. This method is a multivariate approach, since it is capable of identifying the influence of each time-series on any other time-series in the presence of all other time-series of the underlying dynamic system. Here, we investigate whether this model can serve as a biomarker for classifying ASD patients from typical controls using a subset of 59 subjects of the Autism Brain Imaging Data Exchange II (ABIDE II) data repository. In this study, we use brain connections as features for classification and estimate them by lsXGC. As a reference method, we compare our results with cross-correlation, which is typically used in the literature as a standard measure of functional connectivity. After feature extraction, we perform feature selection by Kendall's Tau rank correlation coefficient followed by classification using a Support Vector Machine (SVM). In order to evaluate the diagnostic accuracy of lsXGC, we compare its classification performance with cross-correlation. Within a cross-validation scheme of 100 different training/test data splits, we obtain a mean accuracy range of [0.7,0.81] and a mean Area Under the Receiver Operator Characteristic Curve (AUC) range of [0.78,0.85] across all tested numbers of features for lsXGC, which is significantly better than results obtained with cross-correlation namely mean accuracy of [0.57,0.61] and mean AUC of [0.54,0.59], which clearly demonstrates the applicablity of 1sXGC as a potential biomarker for ASD.","Resting-state fMRI,Large-Scale Extended Granger Causality,functional connectivity,machine learning,support vector machine,autism spectrum disorder",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,"COMPUTER-AIDED,DIAGNOSIS,INDEPENDENT,COMPONENT,ANALYSIS,DYNAMIC,BREAST,MRI,CLUSTER-ANALYSIS,QUANTITATIVE,CHARACTERIZATION,TEXTURE,FEATURES,SMALL,LESIONS,CONNECTIVITY,SEGMENTATION,TOMOGRAPHY",,,
23,Deciphering tissue relaxation parameters from a single MR image using deep learning,11314,,,"Wu Yan,Ma Yajun,Du Jiang,Xing Lei","Wu Y,Ma YJ,Du J,Xing L",Xing L,10.1117/12.2546025,Stanford University,"Despite its superior soft tissue contrast, conventional MRI is qualitative in nature and this presents a bottleneck problem in quantitative image analysis and data-driven medicine. Various investigations have been devoted to overcoming this limitation, but practical solutions remain elusive. Leveraging from the unique ability of emerging deep learning in feature extraction, we investigate a data-driven strategy of separating contributions of various contributing factors intertwined in a single T-1 weighted image to derive quantitative T-1 and rho maps without any additional image acquisition. Furthermore, in the proposed deep learning framework, compensation for radiofrequency field inhomogeneities is automatically achieved without extra measurement of B-1 map. The tasks are accomplished using self-attention deep convolutional neural networks, which make efficient use of both local and non-local information. The premise of the approach is that qualitative and quantitative MRI, named Q(2)MRI, can be attained simultaneously without changing the existing imaging protocol. Q(2)MRI lays foundation for next generation of digital medicine and provides a promising quantitative imaging tool for a wide spectrum of biomedical applications, ranging from disease diagnosis, treatment planning, prognosis to assessment of therapeutic response.",,Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
24,"A Deep Learning Based Integration of Multiple Texture Patterns from Intensity, Gradient and Curvature GLCMs in Differentiating the Malignant from Benign Polyps",11314,,,"Zhang Shu,Cao Weiguo,Pomeroy Marc,Gao Yongfeng,Tan Jiaxing,Liang Zhengrong","Zhang S,Cao WG,Pomeroy M,Gao YF,Tan JX,Liang ZR",Liang ZR,10.1117/12.2550014,State University of New York (SUNY) System,"Deep learning such as Convolutional Neural Network (CNN) has demonstrated its superior in the field of image analysis. However, in the medical imaging field, deep learning faces more challenges for tumor classification in computer-aided diagnosis due to uncertainties of lesions including their size, scaling factor, rotation, shapes, etc. Thus, instead of feeding raw images, texture-based CNN model has been designed to classify the objects with their good attributes. For example, gray level co-occurrence matrix (GLCM) can be chosen as the descriptor of the texture pattern for many good properties such as uniform size, shape invariance, scaling invariance. However, there are many different texture metrics to measure the different texture patterns. Thus, an effective and efficient integration model is essential to further improve the classification performance from different texture patterns. In this paper, we proposed a multi-channel texture-based CNN model to effectively integrate intensity, gradient and curvature texture patterns together for differentiating the malignant from benign polyps. Performance was evaluated by the merit of area under the curve of receiver operating characteristics (AUC). Around 0.3 similar to 4.8% improvement has been observed by combining different texture patterns together. Finally, classification performance of AUC=86.7% has been achieved for a polyp mass dataset of 87 samples, which obtains 1.8% improvement compared with a state-of-the-art method. The results indicate that texture information from different metrics could be fused and classified with a better classification performance. It also sheds lights that data integration is important and indispensable to pursuit improvement in classification task.","Polyp Classification,Deep Learning,Texture GLCM,Data Integration",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,"CONVOLUTIONAL,NEURAL-NETWORKS,COMPUTER-AIDED,DETECTION,CLASSIFICATION,DIAGNOSIS,FEATURES",,,
25,Generating High Resolution Digital Mammogram from Digitized Film Mammogram with Conditional Generative Adversarial Network,11314,,,"Zhou Yuanpin,Wei Jun,Helvie Mark A.,Chan Heang-ping,Zhou Chuan,Lubomir Hadjiiski,Lu Yao","Zhou YP,Wei J,Helvie MA,Chan HP,Zhou C,Lubomir H,Lu Y",Zhou YP,10.1117/12.2551278,University of Michigan System,"Deep-learning based application for digital mammography screening is limited due to lack of labeled data. Generating digital mammogram (DM) from existing labeled digitized screen-film mammogram (DFM) dataset is one approach that may alleviate the problem. Generating high resolution DMs from DFMs is a challenge due to the limitations of network capacity and lack of GPU memory. In this study, we developed a deep learning framework, Cycle-HDDM, with which high resolution DMs were generated from DFMs. Our Cycle-HDDM model first used a sliding window to crop DFMs and DMs into patches of 256 by 256 in size. Then, we divided the patches into three categories (breast, background and boundary) using breast masks. We paired patches from the DFM and DM datasets for training with the constraint that these paired patches should be sampled from the same category of the two different image sets. We used U-Net as the generators and modified the discriminators so that the outputs of the discriminators were a two-channel image, one channel for distinguishing real and synthesized DMs, and the other for representing a probability map for breast mask. We designed a study to evaluate the usefulness of Cycle-HDDM in a segmentation task, the objective of which was to estimate the percentage of breast density (PD) on DMs using deep neural network (DNN). With IRB approval, 1651 DFMs and 813 DMs were collected. Both DFMs and DMs were normalized to a pixel size of 100 mu m x 100 mu m for the experiments. The results show that the synthesized DMs by Cycle-HDDM could significantly improve (p < 0.001) the DNN-based mammographic density segmentation.","breast,mammogram,high resolution,conditional generative adversarial network",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
26,Performance Assessment of Texture Reproduction in High-Resolution CT,11316,,,"Shi Hui,Gang Grace J.,Li Junyuan,Liapi Eleni,Abbey Craig,Stayman J. Webster","Shi H,Gang GJ,Li JY,Liapi E,Abbey C,Stayman JW",Shi H,10.1117/12.2550579,Johns Hopkins University,"Assessment of computed tomography (CT) images can be complex due to a number of dependencies that affect system performance. In particular, it is well-known that noise in CT is object-dependent. Such object-dependence can be more pronounced and extend to resolution and image textures with the increasing adoption of model-based reconstruction and processing with machine learning methods. Moreover, such processing is often inherently nonlinear complicating assessments with simple measures of spatial resolution, etc. Similarly, recent advances in CT system design have attempted to improve fine resolution details - e.g., with newer detectors, smaller focal spots, etc. Recognizing these trends, there is a greater need for imaging assessment that are considering specific features of interest that can be placed within an anthropomorphic phantom for realistic emulation and evaluation. In this work, we devise a methodology for 3D-printing phantom inserts using procedural texture generation for evaluation of performance of high-resolution CT systems. Accurate representations of texture have previously been a hindrance to adoption of processing methods like model-based reconstruction, and texture serves as an important diagnostic feature (e.g. heterogeneity of lesions is a marker for malignancy). We consider the ability of different systems to reproduce various textures (as a function of the intrinsic feature sizes of the texture), comparing microCT, cone-beam CT, and diagnostic CT using normal- and high-resolution modes. We expect that this general methodology will provide a pathway for repeatable and robust assessments of different imaging systems and processing methods.",,Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7643885,
27,Multi-Label Visual Feature Learning with Attentional Aggregation,,,2190-2198,"Guan Ziqiao,Yager Kevin G.,Yu Dantong,Qin Hong","Guan ZQ,Yager KG,Yu DT,Qin H",Guan ZQ,,State University of New York (SUNY) System,"Today convolutional neural networks (CNNs) have reached out to specialized applications in science communities that otherwise would not be adequately tackled. In this paper, we systematically study a multi-label annotation problem of x-ray scattering images in material science. For this application, we tackle an open challenge with training CNNs - identifying weak scattered patterns with diffuse background interference, which is common in scientific imaging. We articulate an Attentional Aggregation Module (AAM) to enhance feature representations. First, we reweight and highlight important features in the images using data-driven attention maps. We decompose the attention maps into channel and spatial attention components. In the spatial attention component, we design a mechanism to generate multiple spatial attention maps tailored for diversified multi-label learning. Then, we condense the enhanced local features into non-local representations by performing feature aggregation. Both attention and aggregation are designed as network layers with learnable parameters so that CNN training remains fluidly end-to-end, and we apply it in-network a few times so that the feature enhancement is multi-scale. We conduct extensive experiments on CNN training and testing, as well as transfer learning, and empirical studies confirm that our method enhances the discriminative power of visual features of scientific imaging.",CLASSIFICATION,Proceedings Paper,"IEEE COMPUTER SOC, 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA","Computer Science,Imaging Science & Photographic Technology",,,CLASSIFICATION,,https://www.osti.gov/biblio/1717900,
28,NeurReg: Neural Registration and Its Application to Image Segmentation,,,3606-3615,"Zhu Wentao,Myronenko Andriy,Xu Ziyue,Li Wenqi,Roth Holger,Huang Yufang,Milletari Fausto,Xu Daguang","Zhu WT,Myronenko A,Xu ZY,Li WQ,Roth H,Huang YF,Milletari F,Xu DG",Zhu WT,,Nvidia Corporation,"Registration is a fundamental task in medical image analysis which can be applied to several tasks including image segmentation, intra-operative tracking, multi-modal image alignment, and motion analysis. Popular registration tools such as ANTs and NiftyReg optimize an objective function for each pair of images from scratch which is time-consuming for large images with complicated deformation. Facilitated by the rapid progress of deep learning, learning-based approaches such as VoxelMorph have been emerging for image registration. These approaches can achieve competitive performance in a fraction of a second on advanced GPUs. In this work, we construct a neural registration framework, called NeurReg, with a hybrid loss of displacement fields and data similarity, which substantially improves the current state-of-the-art of registrations. Within the framework, we simulate various transformations by a registration simulator which generates fixed image and displacement field ground truth for training. Furthermore, we design three segmentation frameworks based on the proposed registration framework: 1) atlas-based segmentation, 2) joint learning of both segmentation and registration tasks, and 3) multi-task learning with atlas-based segmentation as an intermediate feature. Extensive experimental results validate the effectiveness of the proposed NeurReg framework based on various metrics: the endpoint error (EPE) of the predicted displacement field, mean square error (MSE), normalized local cross-correlation (NLCC), mutual information (MI), Dice coefficient, uncertainty estimation, and the interpretability of the segmentation. The proposed NeurReg improves registration accuracy with fast inference speed, which can greatly accelerate related medical image analysis tasks.",,Proceedings Paper,"IEEE COMPUTER SOC, 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA","Computer Science,Imaging Science & Photographic Technology",,,,,http://arxiv.org/pdf/1910.01763,
29,3D Semi-Supervised Learning with Uncertainty-Aware Multi-View Co-Training,,,3635-3644,"Xia Yingda,Liu Fengze,Yang Dong,Cai Jinzheng,Yu Lequan,Zhu Zhuotun,Xu Daguang,Yuille Alan,Roth Holger","Xia YD,Liu FZ,Yang D,Cai JZ,Yu LQ,Zhu ZT,Xu DG,Yuille A,Roth H",Xia YD,,Johns Hopkins University,"While making a tremendous impact in various fields, deep neural networks usually require large amounts of labeled data for training which are expensive to collect in many applications, especially in the medical domain. Unlabeled data, on the other hand, is much more abundant. Semi-supervised learning techniques, such as co-training, could provide a powerful tool to leverage unlabeled data. In this paper, we propose a novel framework, uncertainty-aware multi-view co-training (UMCT), to address semi-supervised learning on 3D data, such as volumetric data from medical imaging. In our work, co-training is achieved by exploiting multi-viewpoint consistency of 3D data. We generate different views by rotating or permuting the 3D data and utilize asymmetrical 3D kernels to encourage diversified features in different sub-networks. In addition, we propose an uncertainty-weighted label fusion mechanism to estimate the reliability of each view's prediction with Bayesian deep learning. As one view requires the supervision from other views in co-training, our self-adaptive approach computes a confidence score for the prediction of each unlabeled sample in order to assign a reliable pseudo label. Thus, our approach can take advantage of unlabeled data during training. We show the effectiveness of our proposed semi-supervised method on several public datasets from medical image segmentation tasks (NIH pancreas & LiTS liver tumor dataset). Meanwhile, a fully-supervised method based on our approach achieved state-of-the-art performances on both the LiTS liver tumor segmentation and the Medical Segmentation Decathlon (MSD) challenge, demonstrating the robustness and value of our framework, even when fully supervised training is feasible.",FRAMEWORK,Proceedings Paper,"IEEE COMPUTER SOC, 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA","Computer Science,Imaging Science & Photographic Technology",,,FRAMEWORK,,http://arxiv.org/pdf/1811.12506,
30,Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy,,,3655-3662,"Weigert Martin,Schmidt Uwe,Haase Robert,Sugawara Ko,Myers Gene","Weigert M,Schmidt U,Haase R,Sugawara K,Myers G",Weigert M,,Ecole Polytechnique Federale de Lausanne,"Accurate detection and segmentation of cell nuclei in volumetric (3D) fluorescence microscopy datasets is an important step in many biomedical research projects. Although many automated methods for these tasks exist, they often struggle for images with low signal-to-noise ratios and/or dense packing of nuclei. It was recently shown for 2D microscopy images that these issues can be alleviated by training a neural network to directly predict a suitable shape representation (star-convex polygon) for cell nuclei. In this paper, we adopt and extend this approach to 3D volumes by using star-convex polyhedra to represent cell nuclei and similar shapes. To that end, we overcome the challenges of 1) finding parameter-efficient star-convex polyhedra representations that can faithfully describe cell nuclei shapes, 2) adapting to anisotropic voxel sizes often found in fluorescence microscopy datasets, and 3) efficiently computing intersections between pairs of star-convex polyhedra (required for non-maximum suppression). Although our approach is quite general, since star-convex polyhedra include common shapes like bounding boxes and spheres as special cases, our focus is on accurate detection and segmentation of cell nuclei. Finally, we demonstrate on two challenging datasets that our approach (STARDIST-3D) leads to superior results when compared to classical and deep learning based methods.",,Proceedings Paper,"IEEE COMPUTER SOC, 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA","Computer Science,Imaging Science & Photographic Technology",,,,,http://arxiv.org/pdf/1908.03636,
31,Facial Anthropometric Measurements and Photographs - An Interdisciplinary Study,8,,181998-182013,"Dhaliwal Jasbir,Wagner John,Leong Shu Ling,Lim Chern Hong","Dhaliwal J,Wagner J,Leong SL,Lim CH",Dhaliwal J,10.1109/ACCESS.2020.3028694,Monash University,"In recent years, automatic facial analysis has attracted much interest among computer science researchers in the healthcare and computer vision fields studying facial anthropometric measurements using photographs. However, to date, there have been no healthcare or computer vision publications that use standardized photographs to differentiate features between sub-ethnic groups by leveraging the power of machine learning on two-dimensional computer vision benchmark data sets (2D CVBDs). Thus, the present work is an interdisciplinary study at the interface of healthcare and computer vision fields that attempts to fill this literature gap where we explore the use of machine learning on 2,789 photographs from eleven 2D CVBDs to identify k top discriminative features in major and sub-ethnic groups. These features are ranked based on information gain values and p-values. We also provide a comprehensive analysis of using information-gain-based and p-value-based features. Our machine learning model achieves an accuracy of 96-99%, and our findings reveal that information-gain-based features have the upper hand over p-value-based features. The top three information-gain-based features in sub-ethnic groups are: dn (distance from the tip of the nose to the center of the mouth), hf (face height) and wn (nose width), while the top three information-gain-based features in major ethnic groups are: de (distance between the inner corners of the eyelids), hf and dn. These results are then compared to the results obtained using standard deep learning techniques such as OxfordNet (VGG16), Residual Networks (ResNet50), and Inception-V3, where accuracy of 90-94% was seen. We hope that these findings will lead to future collaboration between computer vision and healthcare researchers studying facial anthropometric measurement studies.","Facial anthropometric measurements,computer vision benchmark data sets,machine learning,healthcare",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,"FACE,DATABASE,ETHNICITY,PROFILE,WHITE,RACE,AGE",IEEE ACCESS,https://doi.org/10.1109/access.2020.3028694,
32,OREBA: A Dataset for Objectively Recognizing Eating Behavior and Associated Intake,8,,181955-181963,"Rouast Philipp V,Heydarian Hamid,Adam Marc T. P.,Rollo Megan E.","Rouast PV,Heydarian H,Adam MTP,Rollo ME",Adam MTP,10.1109/ACCESS.2020.3026965,University of Newcastle,"Automatic detection of intake gestures is a key element of automatic dietary monitoring. Several types of sensors, including inertial measurement units (IMU) and video cameras, have been used for this purpose. The common machine learning approaches make use of labeled sensor data to automatically learn how to make detections. One characteristic, especially for deep learning models, is the need for large datasets. To meet this need, we collected the Objectively Recognizing Eating Behavior and Associated Intake (OREBA) dataset. The OREBA dataset aims to provide comprehensive multi-sensor data recorded during the course of communal meals for researchers interested in intake gesture detection. Two scenarios are included, with 100 participants for a discrete dish and 102 participants for a shared dish, totalling 9069 intake gestures. Available sensor data consist of synchronized frontal video and IMU with accelerometer and gyroscope for both hands. We report the details of data collection and annotation, as well as details of sensor processing. The results of studies on IMU and video data involving deep learning models are reported to provide a baseline for future research. Specifically, the best baseline models achieve performances of F-1 = 0.853 for the discrete dish using video and F-1 = 0.852 for the shared dish using inertial data.","Sensors,Monitoring,Deep learning,Accelerometers,Annotations,Cameras,Synchronization,Dietary monitoring,eating behavior assessment,accelerometer,communal eating,gyroscope,360-degree video camera",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,,IEEE ACCESS,https://doi.org/10.1109/access.2020.3026965,
33,A Prototype System for Real-Time Monitoring of Arctic Char in Indoor Aquaculture Operations: Possibilities & Challenges,8,,180815-180824,"Soltanzadeh Ramin,Hardy Bruce,Mcleod Robert D.,Friesen Marcia R.","Soltanzadeh R,Hardy B,Mcleod RD,Friesen MR",Friesen MR,10.1109/ACCESS.2020.3028544,University of Manitoba,"In this exploratory study, we studied and qualitatively evaluated a prototype video data collection system to capture and analyze fish behavior in a small-scale indoor aquaculture operation. The research objective was to design and develop a hardware / software system that would have the potential to capture meaningful data from which to extract fish size, swim trajectory, and swim velocity, ultimately as information toward an assessment of fish health. The initial work presented in this paper discusses the development choices of the prototype system, including various combinations of lighting and camera positions both inside and outside of the aquaculture tanks, and several post-processing techniques to isolate fish in video, calibrate the distance from camera to fish through water, and infer fish trajectories and swim velocities. Preliminary results provided a qualitative assessment of such a system. Specific results on the system's ability to detect fishes' positions, trajectories, and velocities are presently limited to observational outcomes and descriptive statistics rather than large-scale quantitative analysis. The present work lays a foundation for a future commercially hardened system that would be required for the collection of larger datasets, which would in turn facilitate the future development of machine learning (ML) algorithms to begin to statistically correlate data to fish conditions and behaviors in near-real time.","Fish,Lighting,Cameras,Aquaculture,Image color analysis,Prototypes,Arctic,Aquaculture,arctic char,fish monitoring,video monitoring,image processing",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,"FISH,TRACKING,WELFARE,VIDEO",IEEE ACCESS,https://doi.org/10.1109/access.2020.3028544,
34,Blockchain for Privacy Preserving and Trustworthy Distributed Machine Learning in Multicentric Medical Imaging (C-DistriM),8,,183939-183951,"Zerka Fadila,Urovi Visara,Vaidyanathan Akshayaa,Barakat Samir,Leijenaar Ralph T. H.,Walsh Sean,Gabrani-Juma Hanif,Miraglio Benjamin,Woodruff Henry C.,Dumontier Michel","Zerka F,Urovi V,Vaidyanathan A,Barakat S,Leijenaar RTH,Walsh S,Gabrani-Juma H,Miraglio B,Woodruff HC,Dumontier M",Zerka F,10.1109/ACCESS.2020.3029445,Maastricht University,"The utility of Artificial Intelligence (AI) in healthcare strongly depends upon the quality of the data used to build models, and the confidence in the predictions they generate. Access to sufficient amounts of high-quality data to build accurate and reliable models remains problematic owing to substantive legal and ethical constraints in making clinically relevant research data available offsite. New technologies such as distributed learning offer a pathway forward, but unfortunately tend to suffer from a lack of transparency, which undermines trust in what data are used for the analysis. To address such issues, we hypothesized that, a novel distributed learning that combines sequential distributed learning with a blockchain-based platform, namely Chained Distributed Machine learning C-DistriM, would be feasible and would give a similar result as a standard centralized approach. C-DistriM enables health centers to dynamically participate in training distributed learning models. We demonstrate C-DistriM using the NSCLC-Radiomics open data to predict two-year lung-cancer survival. A comparison of the performance of this distributed solution, evaluated in six different scenarios, and the centralized approach, showed no statistically significant difference (AUCs between central and distributed models), all DeLong tests yielded p-val > 0.05. This methodology removes the need to blindly trust the computation in one specific server on a distributed learning network. This fusion of blockchain and distributed learning serves as a proof-of-concept to increase transparency, trust, and ultimately accelerate the adoption of AI in multicentric studies. We conclude that our blockchain-based model for sequential training on distributed datasets is a feasible approach, provides equivalent performance to the centralized approach.","Data models,Training,Machine learning,Servers,Biomedical imaging,Blockchain,data privacy,decentralized learning,distributed learning",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,"HEALTH-CARE,MODEL",IEEE ACCESS,https://ieeexplore.ieee.org/ielx7/6287639/8948470/09216036.pdf,
35,Tractable Inference and Observation Likelihood Evaluation in Latent Structure Influence Models,68,,5736-5745,"Karimi Sajjad,Shamsollahi Mohammad Bagher","Karimi S,Shamsollahi MB",Karimi S,10.1109/TSP.2020.3025522,Sharif University of Technology,"Latent Structure Influence Models (LSIMs) are a particular kind of Coupled Hidden Markov Models (CHMMs). Against CHMMs, LSIMs overcome the exponential growth of state-space parameters by considering the influence model for coupled Markov chains. Nevertheless, the exact inference in LSIMs requires exponential complexity. We propose a new recursive formulation to compute marginal forward and backward parameters by O(T (NC)(2)) instead of O((TNC)-C-2) for C channels of N states apiece observing T data points. This formulation is derived systematically and carefully to increase the inference accuracy. Furthermore, a solution is presented for the evaluation problem of LSIMs based on the proposed marginal forward parameter. This solution is essential in statistical multi-channel time-series classification. The results show that the proposed algorithm is generally more accurate and reliable than other existing algorithms. Novelties in deriving the marginal backward parameter plays an important role in this superiority. The Hellinger distance is computed between the proposed and exact forward and one-slice parameters for various simulation scenarios. Distances are small enough, indicating that the proposed inference algorithm is sufficiently close to exact inference for various channels, hidden state numbers, and other parameters. Statistical multi-channel time-series classification is also considered for both proposed and exact algorithms. Classification results are almost similar, indicating that the proposed approximate inference is proper and acceptable in the classification task. Finally, the iEEG dataset's parameter learning indicates that the proposed inference algorithm leads to a higher log-likelihood than the existing algorithms.","Hidden Markov models,Inference algorithms,Approximation algorithms,Brain modeling,Computational modeling,Signal processing algorithms,Heuristic algorithms,Approximate inference,Boyen-Koller algorithm,convex combination,coupled hidden Markov model,forward and backward parameters,influence model,latent structure influence model,squared euclidean distance",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA",Engineering,,5.239,"HIDDEN,MARKOV,MODEL,KULLBACK-LEIBLER,DIVERGENCE,COUPLED-HMM",IEEE TRANSACTIONS ON SIGNAL PROCESSING,,
36,Automated surgical margin assessment in breast conserving surgery using SFDI with ensembles of self-confident deep convolutional networks,11362,,,"Pardo Arturo,Gutierrez-Gutierrez Jose Alberto,Streeter Samuel S.,Maloney Benjamin W.,Lopez-Higuera Jose M.,Pogue Brian W.,Conde Olga M.","Pardo A,Gutierrez-Gutierrez JA,Streeter SS,Maloney BW,Lopez-Higuera JM,Pogue BW,Conde OM",Conde OM,10.1117/12.2554965,Universidad de Cantabria,"With an adequate tissue dataset, supervised classification of tissue optical properties can be achieved in SFDI images of breast cancer lumpectomies with deep convolutional networks. Nevertheless, the use of a black-box classifier in current ex vivo setups provides output diagnostic images that are inevitably bound to show misclassified areas due to interand intra-patient variability that could potentially be misinterpreted in a real clinical setting. This work proposes the use of a novel architecture, the self-introspective classifier, where part of the model is dedicated to estimating its own expected classification error. The model can be used to generate metrics of self-confidence for a given classification problem, which can then be employed to show how much the network is familiar with the new incoming data. A heterogenous ensemble of four deep convolutional models with self-confidence, each sensitive to a different spatial scale of features, is tested on a cohort of 70 specimens, achieving a global leave-one-out cross-validation accuracy of up to 81%, while being able to explain where in the output classification image the system is most confident.",,Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Medical Laboratory Technology,Optics",,,,,https://repositorio.unican.es/xmlui/bitstream/10902/20859/1/AutomatedSurgicalMargin.pdf,
37,Machine Learning for Rapid Magnetic Resonance Fingerprinting Tissue Property Quantification,108,1,69-85,"Hamilton Jesse I.,Seiberlich Nicole","Hamilton JI,Seiberlich N",Hamilton JI,10.1109/JPROC.2019.2936998,Case Western Reserve University,"Magnetic resonance fingerprinting (MRF) is a magnetic resonance imaging (MRI)-based method that can provide quantitative maps of multiple tissue properties simultaneously from a single rapid acquisition. Tissue property maps are generated by matching the complex signal evolutions collected at the scanner to a dictionary of signals derived using the Bloch equation simulations. However, in some circumstances, the process of dictionary generation and signal matching can be time-consuming, reducing the utility of this technique. Recently, several groups have proposed using machine learning to accelerate the extraction of quantitative maps from the MRF data. This article will provide an overview of current research that combines MRF and machine learning, as well as present original research demonstrating how machine learning can speed up dictionary generation for cardiac MRF (cMRF).","Dictionaries,Machine learning,Magnetic resonance imaging,Mathematical model,Trajectory,Magnetic properties,Machine learning,MR fingerprinting (MRF),neural networks,non-Cartesian,relaxometry,tissue characterization",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA",Engineering,,13.102,"STATE,FREE,PRECESSION,SLIDING-WINDOW,MR,RECONSTRUCTION,T-1,REDUCTION,ARTIFACTS,FRAMEWORK,T1",PROCEEDINGS OF THE IEEE,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7595247,
38,Information Technology for Medical Data Stream Mining,,,93-97,"Perova Iryna,Brazhnykova Yelizaveta,Miroshnychenko Nelia,Bodyanskiy Yevgeniy","Perova I,Brazhnykova Y,Miroshnychenko N,Bodyanskiy Y",Perova I,10.1109/TCSET49122.2020.235399,Ministry of Education & Science of Ukraine,"In this paper information technology for medical data stream analysis under conditions of uncertainty is proposed. In this case, the uncertainty is defined like unknown total number of patients, of the initial number of medical features and diagnoses that can be changed during diagnostic process, the data need to be processed sequentially in online-mode (Data Stream processing). Information technology consists of three modules: controlled learning module (Data Stream consists of sufficient number of marked samples - representative dataset), active learning module (Data Stream consists of a few number of marked samples - unrepresentative dataset) and self-learning module (Data Stream consists of only unmarked samples). As a result of the operation of information technology, we obtain a diagnosis of each patient in sequential online mode.","information technology,medical data mining,controlled learning,self-learning,active learning",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Computer Science,Engineering,Telecommunications",,,,"15TH INTERNATIONAL CONFERENCE ON ADVANCED TRENDS IN RADIOELECTRONICS, TELECOMMUNICATIONS AND COMPUTER ENGINEERING (TCSET - 2020)",,
39,Non-invasive Cuff-less Measurement of Blood Pressure Based on Machine Learning,,,203-206,"Viunytskyi Oleh,Shulgin Vyacheslav,Sharonov Valery,Totsky Alexander","Viunytskyi O,Shulgin V,Sharonov V,Totsky A",Viunytskyi O,10.1109/TCSET49122.2020.235423,Ministry of Education & Science of Ukraine,"This article attempts to consider a new approach to continuous measurement of blood pressure (BP), based on the pulse transit time (PTT) between two points of a blood vessel and the shape of pulse wave (PW). The measuring of PW/PTT based on the signal processing and analysis of the patient's electrocardiogram (ECG) and photoplethysmogram (PPG). The PTT-based blood pressure estimation algorithms, used by most authors, suggests their individual calibration for each patient. More flexible is a different approach - the use of machine learning. It is especially noted that the use of machine learning had reduced the error of blood pressure measurement.","blood pressure,electrocardiogram,photoplethysmogram,machine learning,pulse wave,pulse transit time",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Computer Science,Engineering,Telecommunications",,,"PULSE-WAVE,VELOCITY","15TH INTERNATIONAL CONFERENCE ON ADVANCED TRENDS IN RADIOELECTRONICS, TELECOMMUNICATIONS AND COMPUTER ENGINEERING (TCSET - 2020)",,
40,Prediction of Fabric Drape Based on BP Neural Network Paper,,,206-210,"Xia Shuhui,Fan Xiujuan","Xia SH,Fan XJ",Xia SH,,Beijing Institute of Fashion Technology,"yThis paper studies the design of fabric drape prediction model based on BP neural network, and predicts the static and dynamic drape coefficients according to the number of warp and weft yarns, density of warp and weft yarns, weight per square meter and thickness of fabric. The effect of the number of intermediate layer neurons on the accuracy of neural network was studied. The influence of activation function on neural network prediction and initial value on local optimization. The simulation results show that the prediction results of fabric drape by BP neural network are good and meet the expected requirements.","lightweight fabrics,drape coefficient prediction,BP neural network",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,2020 IEEE 7TH INTERNATIONAL CONFERENCE ON INDUSTRIAL ENGINEERING AND APPLICATIONS (ICIEA 2020),,
41,Raw Material Characteristic Prediction for Packing Media Preparation in Canned Pineapple Production Line,,,350-354,"Thiwa-Anont Kwanluck,Buddhakulsomsiri Jirachai,Pannakkong Warut","Thiwa-Anont K,Buddhakulsomsiri J,Pannakkong W",Thiwa-Anont K,,Thammasat University,"The quality standard is an important, especially in the food industry. In canned pineapple production under the uncertain quality of raw materials (i.e., pineapples), preparing packing medium (PM) with a suitable degree of Brix is important because the sweetness of the canned pineapple must match with a standard degree of Brix. This paper develops a support vector machine (SVM), a machine learning technique, to predict the degree of Brix of the pineapples. Response surface method (RSM) is applied to investigate the best hyperparameters setting of SVM. For estimating the model performance, mean absolute error (MAE) is used as the measurement. Finally, the experimental results show that the SVM hyperparameters should be set that the value of C equal to -1, convergence equal to 0.001 and kernel function is anova. This setting can provide the minimum MAE which equals to 1.10.","support vector machine (SVM),quality standard,machine learning,response surface method (RSM)",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,"SELECTION,MODELS",2020 IEEE 7TH INTERNATIONAL CONFERENCE ON INDUSTRIAL ENGINEERING AND APPLICATIONS (ICIEA 2020),,
42,Developing immersive VR experience for visualizing cross-cultural relationships in music,,,401-406,"Ganguli Kaustuv Kanti,Gomez Oscar,Kuzmenko Leonid,Guedes Carlos","Ganguli KK,Gomez O,Kuzmenko L,Guedes C",Ganguli KK,10.1109/VRW50115.2020.0-190,"New York Univ Abu Dhabi, Mus & Sound Cultures Res Grp, Abu Dhabi 129188, U Arab Emirates.","With the advent in advanced computing methodologies and forward-thinking data storage needs over the last decade, there has been a major drive for reformatting archival collections to enable advanced computational analysis. In this paper, we present our digital compendium of music from the Arab Mashriq andWestern Indian Ocean comprising two music collections drawn from Library materials and field recordings at the New York University Abu Dhabi. This is at once the product and object of our ongoing research at the intersection of cultural heritage preservation and computational analysis. Through computational-ethnomusicological research, we explore the cross-cultural similarities, interactions, and patterns from the music excerpts in order to understand their similarity space by employing audio analysis, machine learning, and visualization techniques. Besides the digital artifactual value, pedagogical/educational and scholarly outcomes, we focus on attracting user-friendly and community engagement into appreciating the music from this region. This is done by providing interactive visualizations of the musical features on a dashboard application and 3-D rendering of the mappings in a VR environment. The VR experience is not only immersive but also provides a scope for appreciation, learning, and dissemination of the music from the region.","H.5.2 [User Interfaces]: User Interfaces,Graphical user interfaces (GUI),H.5.m [Information Interfaces and Presentation]: Miscellaneous",Proceedings Paper,"IEEE COMPUTER SOC, 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA","Computer Science,Imaging Science & Photographic Technology",,,,2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES WORKSHOPS (VRW 2020),,
43,Automatic Detection of Cybersickness from Physiological Signal in a Virtual Roller Coaster Simulation,,,649-650,"Islam Rifatul,Lee Yonggun,Jaloli Mehrad,Muhammad Imtiaz,Zhu Dakai,Quarles John","Islam R,Lee Y,Jaloli M,Muhammad I,Zhu DK,Quarles J",Islam R,10.1109/VRW50115.2020.0-100,University of Texas System,"Virtual reality (VR) systems often induce motion sickness like dis-comfort known as cybersickness. The standard approach for detecting cybersickness includes collecting both subjective and objective measurements, while participants are exposed to VR. With the recent advancement of machine learning, we can train deep neural networks to detect cybersickness severity from subjective (e.g., self-reported sickness periodically) and objective measurements. In this study, we collected physiological data from 31 participants while they were immersed in VR. Self-reported verbal sickness was collected at each minute interval for labeling the physiological data. Finally, a simple neural network was proposed to detect cybersickness severity.","Human-centered computing,User studies,Cyber-sickness,Neural networks",Proceedings Paper,"IEEE COMPUTER SOC, 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA","Computer Science,Imaging Science & Photographic Technology",,,,2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES WORKSHOPS (VRW 2020),,
44,Detection of Tactile Feedback on Touch-screen Devices using EEG Data,,,775-780,"Alsuradi Haneen,Pawari Chaitali,Park Wanjoo,Eid Mohamad","Alsuradi H,Pawari C,Park W,Eid M",Alsuradi H,10.1109/HAPTICS45997.2020.ras.HAP20.16.8d90d0bd,New York University,"Neurohaptics strive to study brain activation associated with haptic interaction (tactile and/or kinesthetic). Understanding the haptic perception and cognition has become an exciting area in the technological, medical and psychophysical research. Neurohaptics has the potential to provide quantitative (objective) evaluation of the user haptic experience by directly measuring brain activities via EEG devices. In this study, we employed a Machine Learning (ML) based classifier model, namely the Radial Based Function Support Vector Machine (RBF-SVM) to select a few relevant Electroencephalography (EEG) channels and to detect the presence of tactile feedback during interaction with touch-screen devices using EEG data. To overcome the problem of limited training data, time-shifting is proposed as a method for data augmentation in time-series neural data which increased the classification accuracy. An experimental setup comprising an active touch task on the Tanvas touch-screen device is designed to evaluate the developed model. Results demonstrated that the middle frontal cortex, namely channels AF3, AF4, and F1 produced the best recognition rate of 85 +/- 3.3% in detecting the presence of the tactile feedback. This work is a step forward towards building a quantitative evaluation of tactile experience during haptic interaction.",CLASSIFICATION,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Computer Science,Engineering",,,CLASSIFICATION,,,
45,Transfer Learning Convolutional Neural Network for Sleep Stage Classification Using Two-Stage Data Fusion Framework,8,,180618-180632,"Abdollahpour Mehdi,Rezaii Tohid Yousefi,Farzamnia Ali,Saad Ismail","Abdollahpour M,Rezaii TY,Farzamnia A,Saad I",Rezaii TY,10.1109/ACCESS.2020.3027289,University of Tabriz,"The most important part of sleep quality assessment is the classification of sleep stages, which helps to diagnose sleep-related disease. In the traditional sleep staging method, subjects have to spend a night in the sleep clinic for recording polysomnogram. Sleep expert classifies the sleep stages by monitoring the signals, which is time consuming and frustrating task and can be affected by human error. New studies propose fully automated techniques for classifying sleep stages that makes sleep scoring possible at home. Despite comprehensive studies have been presented in this field the classification results have not yet reached the gold standard due to the concentration on the use of a limited source of information such as single channel EEG. Therefore, this article introduces a new method for fusing two sources of information, including electroencephalogram (EEG) and electrooculogram (EOG), to achieve promising results in the classification of sleep stages. In the proposed method, extracted features from the EEG and EOG signals, are divided into two feature sets consisting of the EEG features and fused features of EEG and EOG. Then, each feature set transformed into a horizontal visibility graph (HVG). The images of the HVG are produced in a novel framework and classified by proposed transfer learning convolutional neural network for data fusion (TLCNN-DF). Employing transfer learning at the training stage of the model has accelerated the training process of the CNN and improved the performance of the model. The proposed algorithm is used to classify the Sleep-EDF and Sleep-EDFx benchmark datasets. The algorithm can classify the Sleep-EDF dataset with an accuracy of 93.58% and Cohen's kappa coefficient of 0.899. The results show proposed method can achieve superior performance compared to state-of-the-art studies on classification of sleep stages. Furthermore, it can attain reliable results as an alternative to conventional sleep staging.","Sleep,Feature extraction,Electroencephalography,Electrooculography,Brain modeling,Training,Data integration,Convolutional neural network,data fusion,horizontal visibility graph,sleep stage classification,transfer learning",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,"FEATURES,AGREEMENT,SYSTEM",IEEE ACCESS,https://doi.org/10.1109/access.2020.3027289,
46,An Overview of Deep Learning Approaches in Chest Radiograph,8,,182347-182354,"Anis Shazia,Lai Khin Wee,Chuah Joon Huang,Ali Shoaib Mohammad,Mohafez Hamidreza,Hadizadeh Maryam,Yan Ding,Ong Zhi-Chao","Anis S,Lai KW,Chuah JH,Ali SM,Mohafez H,Hadizadeh M,Yan D,Ong ZC",Lai KW,10.1109/ACCESS.2020.3028390,Universiti Malaya,"Chest X-ray (CXR) interpretations are conducted in hospitals and medical facilities on daily basis. If the interpretation tasks were performed correctly, various vital medical conditions of patients can be revealed such as pneumonia, pneumothorax, interstitial lung disease, heart failure and bone fracture. The current practices often involve tedious manual processes dependent on the expertise of radiologist or consultant, thus, the execution is easily prone to human errors of being misdiagnosed. With the recent advances of deep learning and increased hardware computational power, researchers are working on various networks and algorithms to develop machines learning that can assists radiologists in their diagnosis and reduce the probability of misdiagnosis. This paper presents a review of deep learning advancements made in the field of chest radiography. It discusses single and multi-level localization and segmentation techniques adopted by researchers for higher accuracy and precision.","Artificial neural networks,deep learning,transfer learning,multi-task learning,object detection,localization,segmentation",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,"NEURAL-NETWORK,CLASSIFICATION,CT,ECHOCARDIOGRAPHY,RADIOLOGISTS,ERROR",IEEE ACCESS,https://doi.org/10.1109/access.2020.3028390,
47,Automatic Recognition of Fundamental Heart Sound Segments From PCG Corrupted With Lung Sounds and Speech,8,,179983-179994,"Babu K. Ajay,Ramkumar Barathram","Babu KA,Ramkumar B",Babu KA,10.1109/ACCESS.2020.3023044,Indian Institute of Technology System (IIT System),"Automated recognition of fundamental heart sound segments (FHSS) from Phonocardiogram (PCG) is the preliminary step before clinical parameters extraction to detect the presence of abnormality if any. PCG acquisition systems are usually based on microphones. These microphones apart from cardiac sounds will also pick up non-cardiac sounds like lung sounds and speech. The recognition of FHSS is challenging in the presence of non-cardiac events. Deep learning techniques like convolutional neural network (CNN) and recurrent neural network (RNN) are suitable for automated FHSS. However, it will be shown that their performance is degraded in the presence of interference like lung sounds, and speech. Hence in this work, a combination of conventional signal processing technique with deep neural network (DNN) is proposed to enhance the accuracy of automated FHSS. The conventional signal processing technique is based on EWT which can adaptively design the filter banks based on the type of interference. For DNN, U-Net is considered. The method involves the segmentation of PCG using EWT and recognition of FHSS using U-Net based DNN. Envelope features are extracted from the EWT based reconstructed signal and used for training the U-Net based DNN to recognize FHSS. To further improve the recognition accuracy of FHSS, delineation parameters obtained from EWT are incorporated for temporal modeling with the outcomes of U-Net based DNN. The performance of the proposed method is analyzed using both real-time signals and signals taken from standard databases like the Physionet database, and Littmann's lung sound library. Realtime PCG is acquired using an in-house developed PCG acquisition system. The proposed U-Net based DNN with the EWT method achieves FHSS recognition accuracy of 91.17% for PCG with lung sound interference and 90.78% for PCG with speech interference. The proposed method significantly improves the accuracy of FHSS recognition compared to long short term memory (LSTM), and gated recurrent unit (GRU).","Phonocardiography,Heart,Lung,Speech recognition,Interference,Feature extraction,Transforms,Recognition,segmentation,fundamental heart sound,lung sound,speech,empirical wavelet transform (EWT),deep neural network (DNN)",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,"IDENTIFICATION,DECOMPOSITION,S-1",IEEE ACCESS,https://doi.org/10.1109/access.2020.3023044,
48,Chromosome Extraction Based on U-Net and YOLOv3,8,,178563-178569,"Bai Hua,Zhang Tianhang,Lu Changhao,Chen Wei,Xu Fangyun,Han Zhi-Bo","Bai H,Zhang TH,Lu CH,Chen W,Xu FY,Han ZB",Han ZB,10.1109/ACCESS.2020.3026483,"Tianjin Key Lab Engn Technol Cell Phamaceut, Tianjin 300457, Peoples R China.","Karyotype analysis based on chromosome banding and microscopic imaging is an important means for the diagnosis of genetic symptoms. Chromosome extraction is one of the key steps in karyotype analysis, but it faces some complex situations i.e. chromosome overlaps and adhesions, which are still a challenge for traditional algorithms. Here, we proposed a method for chromosome extraction based on deep learning. In this method, U-Net was used to segment the original micrographs to remove background noise such as nuclei and other interferences. Then YOLOv3 was used to detect and extract each chromosome. Further, U-Net was used again to extract the single chromosomes precisely. The results show that this method can remove effectively the interferences outside the chromosomes, and accurately extract the overlapping and adhesive chromosomes. The accuracy of extracting chromosomes from the raw G-band chromosome images reaches 99.3%. This method is of great significance for the development of automatic karyotype analysis technology.","Biological cells,Feature extraction,Training,Noise reduction,Convolution,Machine learning,Adhesives,Chromosome extraction,U-Net,YOLOv3",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,SEGMENTATION,IEEE ACCESS,https://doi.org/10.1109/access.2020.3026483,
49,Research on the Prediction Method of Ultimate Bearing Capacity of PBL Based on IAGA-BPNN Algorithm,8,,179141-179155,"Chen Yixin,Zhang Jianye,Liu Yongsheng,Zhao Siwei,Zhou Shunli,Chen Jing","Chen YX,Zhang JY,Liu YS,Zhao SW,Zhou SL,Chen J",Chen YX,10.1109/ACCESS.2020.3026091,Chang'an University,"In order to better predict ultimate bearing capacity of perfobond leiste shear connection (PBL), the six specimens were designed for push-out test, and the prediction models were built based on an Improved Adaptive Genetic Algorithm (IAGA) and Back Propagation neural network (BPNN) algorithm. With the finite element model established, it was found that the effects of different parameters on the ultimate bearing capacity of PBL vary greatly if using a single parameter method. The calculation results showed that transverse reinforcement diameter, hole diameter of steel plate, the thickness of steel plate and the strength grade of concrete were the four key factors affecting the ultimate bearing capacity of PBL. In order to overcome the disadvantages of BPNN, such as slow convergence speed and easy to fall into local optimization, an improved adaptive genetic algorithm is used to optimize the initial weights and thresholds of BPNN. The comparison shows that the algorithm is superior to the standard genetic algorithm and other heuristic algorithms, in terms of convergence speed, global search ability and robustness. The IAGA-BPNN prediction model was established. Using the experimental data obtained from both the fatigue test and the references as samples to train the prediction model, the results show that the IAGA-BPNN algorithm proposed in this article can accurately predict the ultimate bearing capacity of PBL, with an average error of 1.69%. The comprehensive sensitivity analysis (CSA) method adopts to explore the relative contribution of each key factors and the interaction between the key factors, and the analysis results show that the ultimate bearing capacity of PBL increases significantly with the increase of the thickness of steel plate and hole diameter of steel plate. The accuracy and stability were better than formulas for calculating the ultimate bearing capacity and the BP neural network prediction algorithm.","Concrete,Steel,Prediction algorithms,Heuristic algorithms,Finite element analysis,Neural networks,Predictive models,Perfobond leiste,ultimate bearing capacity,IAGA-BPNN algorithm,comprehensive sensitivity analysis,prediction",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,"SHEAR,CONNECTORS,STRENGTH,STEEL,PERFORMANCE,RESISTANCE",IEEE ACCESS,https://doi.org/10.1109/access.2020.3026091,
50,Unsupervised Domain Adversarial Self-Calibration for Electromyography-Based Gesture Recognition,8,,177941-177955,"Cote-Allard Ulysse,Gagnon-Turcotte Gabriel,Phinyomark Angkoon,Glette Kyrre,Scheme Erik J.,Laviolette Francois,Gosselin Benoit","Cote-Allard U,Gagnon-Turcotte G,Phinyomark A,Glette K,Scheme EJ,Laviolette F,Gosselin B",Cote-Allard U,10.1109/ACCESS.2020.3027497,Laval University,"Surface electromyography (sEMG) provides an intuitive and non-invasive interface from which to control machines. However, preserving the myoelectric control system's performance over multiple days is challenging, due to the transient nature of the signals obtained with this recording technique. In practice, if the system is to remain usable, a time-consuming and periodic recalibration is necessary. In the case where the sEMG interface is employed every few days, the user might need to do this recalibration before every use. Thus, severely limiting the practicality of such a control method. Consequently, this paper proposes tackling the especially challenging task of unsupervised adaptation of sEMG signals, when multiple days have elapsed between each recording, by introducing Self-Calibrating Asynchronous Domain Adversarial Neural Network (SCADANN). SCADANN is compared with two state-of-the-art self-calibrating algorithms developed specifically for deep learning within the context of EMG-based gesture recognition and three state-of-the-art domain adversarial algorithms. The comparison is made both on an offline and a dynamic dataset (20 participants per dataset), using two different deep network architectures with two different input modalities (temporal-spatial descriptors and spectrograms). Overall, SCADANN is shown to substantially and systematically improves classification performances over no recalibration and obtains the highest average accuracy for all tested cases across all methods.","Gesture recognition,Training,Real-time systems,Heuristic algorithms,Electrodes,Clustering algorithms,Electromyography,EMG,myoelectric control,domain adaptation,self-calibration,domain adversarial,gesture recognition",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,,"UPPER-LIMB,PROSTHESES,PATTERN-RECOGNITION,ORIENTATION,CLASSIFIERS,SCHEME,ROBUST,STATE",IEEE ACCESS,http://arxiv.org/pdf/1912.11037,
51,Novel Feature Selection and Voting Classifier Algorithms for COVID-19 Classification in CT Images,8,,179317-179335,"El-kenawy El-Sayed M.,Ibrahim Abdelhameed,Mirjalili Seyedali,Eid Marwa Metwally,Hussein Sherif E.","El-kenawy ESM,Ibrahim A,Mirjalili S,Eid MM,Hussein SE",El-kenawy ESM,10.1109/ACCESS.2020.3028012,"Delta Higher Inst Engn & Technol DHIET, Dept Commun & Elect, Mansoura 35111, Egypt.","Diagnosis is a critical preventive step in Coronavirus research which has similar manifestations with other types of pneumonia. CT scans and X-rays play an important role in that direction. However, processing chest CT images and using them to accurately diagnose COVID-19 is a computationally expensive task. Machine Learning techniques have the potential to overcome this challenge. This article proposes two optimization algorithms for feature selection and classification of COVID-19. The proposed framework has three cascaded phases. Firstly, the features are extracted from the CT scans using a Convolutional Neural Network (CNN) named AlexNet. Secondly, a proposed features selection algorithm, Guided Whale Optimization Algorithm (Guided WOA) based on Stochastic Fractal Search (SFS), is then applied followed by balancing the selected features. Finally, a proposed voting classifier, Guided WOA based on Particle Swarm Optimization (PSO), aggregates different classifiers' predictions to choose the most voted class. This increases the chance that individual classifiers, e.g. Support Vector Machine (SVM), Neural Networks (NN), k-Nearest Neighbor (KNN), and Decision Trees (DT), to show significant discrepancies. Two datasets are used to test the proposed model: CT images containing clinical findings of positive COVID-19 and CT images negative COVID-19. The proposed feature selection algorithm (SFS-Guided WOA) is compared with other optimization algorithms widely used in recent literature to validate its efficiency. The proposed voting classifier (PSO-Guided-WOA) achieved AUC (area under the curve) of 0.995 that is superior to other voting classifiers in terms of performance metrics. Wilcoxon rank-sum, ANOVA, and T-test statistical tests are applied to statistically assess the quality of the proposed algorithms as well.","Computed tomography,Feature extraction,Optimization,Support vector machines,Sensitivity,Lung,Machine learning,COVID-19,CT scans,convolutional neural network,guided whale optimization algorithm,features selection,voting ensemble",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,"NEURAL-NETWORKS,OPTIMIZATION,SEARCH,WOLF",IEEE ACCESS,https://research-repository.griffith.edu.au/bitstream/10072/399593/2/Mirjalili449018-Published.pdf,
52,Arterial Spin Labeling Image Synthesis From Structural MRI Using Improved Capsule-Based Networks,8,,181137-181153,"Huang Wei,Luo Mingyuan,Liu Xi,Zhang Peng,Ding Huijun","Huang W,Luo MY,Liu X,Zhang P,Ding HJ",Huang W,10.1109/ACCESS.2020.3028113,Nanchang University,"Medical image synthesis receives much popularity in recent years, and ample medical images can be synthesized by diverse deep learning models to alleviate the problem of lack of data in many medical imaging utilizations. However, most medical image synthesis methods still incorporate the well-known pooling operation in their convolutional neural networks-based / generative adversarial networks-based models, from which image details will be inevitably lost due to the pooling operation. In order to tackle the above problem, improved capsule-based networks, in which no pooling operation is executed and spatial details of images can be effectively preserved thanks to the equivariance characteristics of capsule models, are proposed in this paper to synthesize arterial spin labeling images, for the first time. Technically, three important issues in constructing improved capsule-based networks, including the depth of basic convolutions, the layer of capsules, and the capacity of capsules, are thoroughly investigated. Comprehensive experiments made up of region-based / voxel-based partial volume corrections and dementia diseases diagnosis based on two different datasets are conducted. The superiority of improved capsule-based networks introduced in this paper is substantiated from the statistical point of view.","Image generation,Magnetic resonance imaging,Medical diagnostic imaging,Machine learning,Dementia,Medical diagnosis,Image analysis,image generation,computer aided diagnosis,capsule,arterial spin labeling,deep learning",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,"POSITRON-EMISSION-TOMOGRAPHY,TIME",IEEE ACCESS,https://doi.org/10.1109/access.2020.3028113,
53,Melanoma Detection Using an Objective System Based on Multiple Connected Neural Networks,8,,179189-179202,"Ichim Loretta,Popescu Dan","Ichim L,Popescu D",Popescu D,10.1109/ACCESS.2020.3028248,Polytechnic University of Bucharest,"Melanoma is a common form of skin cancer that dangerously affects many people around the world. Detection of melanoma with the naked eye by dermatologists may be subject to errors. Therefore, the implementation of image processing devices equipped with artificial intelligence can act as a support for the dermatologist in examination and decision making. However, due to the various characteristics of this type of lesions and the presence of noises and artifacts in the images, it is difficult to distinguish melanomas from benign lesions. In this article, we propose a new type of intelligent system which is based on several neural networks connected on two levels of classification. The first level contains five classifiers (subjective classifiers): the perceptron coupled with color local binary patterns, the perceptron coupled with color histograms of oriented gradients, the generative adversarial network (for segmentation) coupled with ABCD rule, the ResNet, and the AlexNet. They are chosen experimentally and consider the following features of melanomas: texture, shape, color, size, and convolutional pixel connections. At the second level (objective level), one classifier (perceptron-type) decides whether the lesion is a melanoma, based on learning-adjusted weight and the decisions at the first level. The second level is based on back-propagation perceptron that provides the final decision (melanoma or non-melanoma). The subjective and objective levels undergo two separate training phases. This approach allows an easier transition of the system from one database to another. This study shows that the use of the objective classifier brings an accuracy of 97.5% and an $F1$ score of 97.47%. These results are better than those of the individual classifier and those of the previous literature mentioned in References.","Melanoma,Lesions,Skin,Image color analysis,Shape,Feature extraction,Image segmentation,Artificial neural networks,decision fusion,dermoscopic images,feature extraction,image classification,image decomposition,image segmentation,melanoma detection",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,"CLASSIFICATION,SEGMENTATION",IEEE ACCESS,https://doi.org/10.1109/access.2020.3028248,
54,Tissue-Type Classification With Uncertainty Quantification of Microwave and Ultrasound Breast Imaging: A Deep Learning Approach,8,,182092-182104,"Mojabi Pedram,Khoshdel Vahab,Lovetri Joe","Mojabi P,Khoshdel V,Lovetri J",Mojabi P,10.1109/ACCESS.2020.3027805,University of Manitoba,"A deep learning approach is proposed for performing tissue-type classification of tomographic microwave and ultrasound property images of the breast. The approach is based on a convolutional neural network (CNN) utilizing the U-net architecture that also quantifies the uncertainty in the classification of each pixel. Quantitative tomographic reconstructions of dielectric properties (complex-valued permittivity), ultrasonic properties (compressibility and attenuation), as well as their combination, with the corresponding actual tissue-type classification constitute the training set. The CNN learns to map the quantitative property reconstructions to a single tissue-type image. The level of confidence in predicting a tissue-type at each pixel is determined. This uncertainty quantification is diagnostically critical for biomedical applications, especially when attempting to distinguish between cancerous and healthy tissues. The Gauss-Newton Inversion algorithm is used for the quantitative reconstruction of both dielectric and ultrasonic properties. Electromagnetic and ultrasound scattered-field data is obtained from MRI-derived numerical breast phantoms. Several numerical breast phantoms types, from fatty to dense, are considered. The proposed classification and uncertainty quantification approach is shown to outperform a previously studied tissue-type classification method based on a Bayesian approach.","Tumors,Breast,Image reconstruction,Dielectrics,Uncertainty,Phantoms,Acoustics,Tissue classification,uncertainty quantification,deep learning method,convolutional neural network,microwave tomography,ultrasound tomography,multi-physics imaging,inverse scattering,breast imaging",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,,"INVERSE-SCATTERING,ITERATIVE,METHOD,NEURAL-NETWORK,RECONSTRUCTION,TOMOGRAPHY,CANCER,DENSITY",IEEE ACCESS,https://doi.org/10.1109/access.2020.3027805,
55,Rotation Equivariant Convolutional Neural Networks for Hyperspectral Image Classification,8,,179575-179591,"Paoletti Mercedes E.,Haut Juan M.,Roy Swalpa Kumar,Hendrix Eligius M. T.","Paoletti ME,Haut JM,Roy SK,Hendrix EMT",Paoletti ME,10.1109/ACCESS.2020.3027776,Universidad de Extremadura,"Detection of surface material based on hyperspectral imaging (HSI) analysis is an important and challenging task in remote sensing. It is widely known that spectral-spatial data exploitation performs better than traditional spectral pixel-wise procedures. Nowadays, convolutional neural networks (CNNs) have shown to be a powerful deep learning (DL) technique due their strong feature extraction ability. CNNs not only combine spectral-spatial information in a natural way, but have also shown to be able to learn translation-equivariant representations, i.e. a translation of input features into an equivalent internal CNN feature map. This provides great robustness to spatial feature locations. However, as far as we know, CNNs do not exhibit a natural way to exploit rotation equivariance, i.e. make use of the fact that data patches in a HSI data cube are observed in different orientations due to their orientation or on the varying paths/orbits of the airborne/spaceborne spectrometers. This article presents a rotation-equivariant CNN2D model for HSI analysis, where traditional convolution kernels have been replaced by circular harmonic filters (CHFs). The obtained results over three well-known HSI datasets showcase the potential of the approach.","Feature extraction,Data models,Data mining,Support vector machines,Convolutional neural networks,Hyperspectral imaging,Computer architecture,Hyperspectral imaging (HSI),convolutional neural network (CNN),harmonic network (H-net),rotation invariance",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,"SPECTRAL-SPATIAL,CLASSIFICATION,INVARIANT,TEXTURE,CLASSIFICATION,REMOTE-SENSING,DATA,DIMENSIONALITY,REDUCTION,MORPHOLOGICAL,PROFILES,CHLOROPHYLL,CONTENT,VEGETATION,INDEXES,NEAREST-NEIGHBOR,RANDOM,FOREST,SPECTROMETER",IEEE ACCESS,https://doi.org/10.1109/access.2020.3027776,
56,Empirical Evaluation on the Impact of Class Overlap for EEG-Based Early Epileptic Seizure Detection,8,,180328-180340,"Qu Yubin,Chen Xiang,Li Fang,Yang Fan,Ji Junxia,Li Long","Qu YB,Chen X,Li F,Yang F,Ji JX,Li L",Ji JX,10.1109/ACCESS.2020.3028139,Nantong University,"Important physiological information is hidden in electroencephalography (EEG), which can reflect the human brain's activity. EEG, which is a kind of complicated signal, can be used for epileptic seizure detection and epilepsy diagnosis via machine learning. A large amount of effort, including raw signal preprocessing and data preprocessing for machine learning, is required for constructing high-quality training datasets because the classification performance highly depends on high-quality data. Feature extraction has been widely used in EEG-based early epileptic seizure detection. Due to the complexity of data collection and labeling, some of the training instances are inevitably mislabeled. That means some similar instances have different labels. This is called the issue of class overlap, which leads to a poor class boundary for classification models and makes constructing a high-quality classification model more difficult. However, the previous studies investigating the impact of the class overlap for EEG data is quite limited. Our goal is to investigate the impact of the class overlap on EEG-based early epileptic seizure detection. We propose a special neighborhood cleaning rule (SNCR) to solve the class overlap issue. To alleviate the class overlap issue, we conduct large-scale experiments on two widely-used EEG datasets and compare our proposed SNCR strategy with a state-of-the-art data clean strategy, i.e., the improved k-means clustering cleaning approach (IKMCCA). The experimental results show that the classification model can achieve significantly better performance in terms of AUC, recall, and F1 metrics when using our proposed SNCR strategy. Therefore, for EEG-based early epileptic seizure detection, we recommend researchers to apply the SNCR strategy to mitigate the class overlap issue and use the SNCR strategy to perform data preprocessing in a future related study.","EEG,early epileptic seizure detection,class overlap,class imbalance,empirical evaluation",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,,,IEEE ACCESS,https://doi.org/10.1109/access.2020.3028139,
57,Rapid Prediction of Brain Injury Pattern in mTBI by Combining FE Analysis With a Machine-Learning Based Approach,8,,179457-179465,"Shim Vickie B.,Holdsworth Samantha,Champagne Allen A.,Coverdale Nicole S.,Cook Douglas J.,Lee Tae-Rin,Wang Alan D.,Li Shaofan,Fernandez Justin W.","Shim VB,Holdsworth S,Champagne AA,Coverdale NS,Cook DJ,Lee TR,Wang AD,Li SF,Fernandez JW",Shim VB,10.1109/ACCESS.2020.3026350,University of Auckland,"Mild traumatic brain injury (mTBI) is a significant issue worldwide. Public awareness of the dangers of mTBI has increased sharply in recent years, yet there is no easy-to-use tool available for early detection and post injury management. Computational models of the head impact, usually in the form of finite element analysis, are a method of choice for characterizing how mechanical impacts lead to brain damage by causing high strains in certain regions of the brain. However, those models require a prohibitively large amount of computational power as well as pre and post processing expertise, making them unrealistic to be used in clinical settings. In this study, we propose a framework that combines finite element analysis with a machine learning based approach where a large number of pre-computed FE results are used to train a statistical model. We analyzed a number of different head impact scenarios in which a football player would sustain a minor brain injury and computed brain internal strain patterns. These pre-computed strain patterns were then used to train a partial least squares regression model to be able to predict the general strain pattern and the location and magnitude of peak strains. Our models were able to predict the overall distribution pattern, including the location of the peak strain, with an average error of 3%. The peak strain magnitudes were also predicted accurately with the average error of 9% at almost real time speed (less than 10 seconds). This model may play an important role in developing a diagnostic tool for mTBI that can predict the severity of head impacts.","Brain modeling,Strain,Computational modeling,Head,Magnetic heads,Finite element analysis,Magnetic resonance imaging,Diffusion tensor imaging,finite element analysis,magnetic resonance imaging,mild traumatic brain injury,partial least squares regression",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,"MAXIMUM,PRINCIPAL,STRAIN,TISSUE,MODEL,IMPLEMENTATION,VALIDATION,IMPACTS",IEEE ACCESS,https://doi.org/10.1109/access.2020.3026350,
58,Surgical Tools Detection Based on Training Sample Adaptation in Laparoscopic Videos,8,,181723-181732,"Wang Guangyao,Wang Shengsheng","Wang GY,Wang SS",Wang SS,10.1109/ACCESS.2020.3028910,Jilin University,"The performance of object detection methods plays an important role in the recognition of surgical tools, and is a key link in the automated evaluation of surgical skills. In this paper, we propose a novel framework for one-stage object detection based on a sample adaptive process controlled by reinforcement learning, which can maintain the speed advantage while maintaining higher accuracy than two-stage object detection methods. We use m2cai16-tool-locations and AJU-Set, two datasets covering seven surgical tools with spatial information collected from hospital gallbladder surgery videos to evaluate and verify the effectiveness of our proposed framework. The experiments show that our proposed framework can make the one-stage object detection method achieve 70.1% and 77.3% accuracy on m2cai16-tool-locations and AJU-Set, respectively. We further validated the effectiveness of our proposed framework by analyzing the usage patterns, motion trajectories, and mobile values of surgical tools.","Object detection,Surgery,Training,Tools,Learning (artificial intelligence),Videos,Hidden Markov models,Laparoscopic surgery,reinforcement learning,object detection",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,"DEEP,LEVEL",IEEE ACCESS,https://doi.org/10.1109/access.2020.3028910,
59,Prediction of Survival Time of Patients With Esophageal Squamous Cell Carcinoma Based on Univariate Analysis and ASSA-BP Neural Network,8,,181127-181136,"Wang Yanfeng,Liang Enhao,Zhao Xueke,Song Xin,Wang Lidong,Sun Junwei","Wang YF,Liang EH,Zhao XK,Song X,Wang LD,Sun JW",Sun JW,10.1109/ACCESS.2020.3028147,Zhengzhou University of Light Industry,"Esophageal squamous cell carcinoma (ESCC) is one of the most common malignant tumors in the world. In order to find out the influencing factors, univariate Cox regression analysis is used to analyze the blood indexes to screen out the factors affecting the survival or death of patients. Spearman and Pearson correlation analysis can verify whether screening factors are related to survival. The survival curves and progression-free survival curves of 5 factors are given after the threshold is obtained by receiver operating characteristics (ROC). In order to optimize the survival accuracy of patients with ESCC, in view of the low convergence accuracy and easy to fall into local optimization of back propagation (BP) prediction network, the improved adaptive salp swarm algorithm (ASSA), genetic algorithm (GA) and back propagation (BP) neural network are combined. The initial weight and threshold of BP neural network are determined, and the ASSA-BP prediction model and GA-BP prediction model are established. In order to show the reliability and accuracy of the new model in a large range, the ASSA-BP model, GA-BP model and BP model are evaluated respectively. The ASSA-BP model is more effective in predicting the survival time of patients with ESCC, and shortens the training time and improves the prediction accuracy.","Neural networks,Cancer,Predictive models,Correlation,Prognostics and health management,MONOS devices,Blood,Esophageal squamous cell carcinoma,univariate analysis,correlation analysis,back propagation,adaptive salp swarm algorithm",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,"SALP,SWARM,ALGORITHM,SHORT-TERM,OUTCOMES,OPEN,SURGERY,CANCER,PROGNOSIS,COLECTOMY,MODEL",IEEE ACCESS,https://doi.org/10.1109/access.2020.3028147,
60,Stages-Based ECG Signal Analysis From Traditional Signal Processing to Machine Learning Approaches: A Survey,8,,177782-177803,"Wasimuddin Muhammad,Elleithy Khaled,Abuzneid Abdel-Shakour,Faezipour Miad,Abuzaghleh Omar","Wasimuddin M,Elleithy K,Abuzneid AS,Faezipour M,Abuzaghleh O",Faezipour M,10.1109/ACCESS.2020.3026968,University of Bridgeport,"Electrocardiogram (ECG) gives essential information about different cardiac conditions of the human heart. Its analysis has been the main objective among the research community to detect and prevent life threatening cardiac circumstances. Traditional signal processing methods, machine learning and its subbranches, such as deep learning, are popular techniques for analyzing and classifying the ECG signal and mainly to develop applications for early detection and treatment of cardiac conditions and arrhythmias. A detailed literature survey regarding ECG signal analysis is presented in this article. We first introduce a stages-based model for ECG signal analysis where a survey of ECG analysis related work is then presented in the form of this stage-based process model. The model describes both traditional time/frequency-domain and advanced machine learning techniques reported in the published literature at every stage of analysis, starting from ECG data acquisition to its classification for both simulations and real-time monitoring systems. We present a comprehensive literature review of real-time ECG signal acquisition, prerecorded clinical ECG data, ECG signal processing and denoising, detection of ECG fiducial points based on feature engineering and ECG signal classification along with comparative discussions among the reviewed studies. This study also presents a detailed literature review of ECG signal analysis and feature engineering for ECG-based body sensor networks in portable and wearable ECG devices for real-time cardiac status monitoring. Additionally, challenges and limitations are discussed and tools for research in this field as well as suggestions for future work are outlined.","Electrocardiography,Heart,Real-time systems,Signal analysis,Machine learning,Analytical models,Biomedical monitoring,ECG analysis,cardiac arrhythmias,QRS and ST detection,ECG classification,deep learning",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,"CONVOLUTIONAL,NEURAL-NETWORK,ST-SEGMENT,ARRHYTHMIA,DETECTION,DETECTION,ALGORITHM,AUTOMATED,DETECTION,WAVELET,TRANSFORM,CLASSIFICATION,SYSTEMS,DATABASE,DELINEATION",IEEE ACCESS,https://doi.org/10.1109/access.2020.3026968,
61,CM-210910-4986247 Multi-Task Learning for Lung Nodule Classification on Chest CT,8,,180317-180327,"Zhai Penghua,Tao Yaling,Chen Hao,Cai Ting,Li Jinpeng","Zhai PH,Tao YL,Chen H,Cai T,Li JP",Li JP,10.1109/ACCESS.2020.3027812,Chinese Academy of Sciences,"Lung cancer is one of the leading causes of death over the world. Detecting and identifying malignant nodules on chest computed tomography (CT) plays an important role in the diagnosis and treatment of lung cancer. Computer-aided diagnosis (CAD) systems have been developed to identify lung nodules. However, the problem of a high false positive rate is still not well solved. In this paper, we propose a novel multi-task convolutional neural network (MT-CNN) framework to identify malignant nodules from benign nodules on chest CT scans. MT-CNN learns three-dimensional (3-D) lung nodule characteristics from nine two-dimensional (2-D) views, which are decomposed from different angles of each nodule. Each of 2-D MT-CNN model consists of two branches, one is the nodule classification branch (main task) and the other is the image reconstruction branch (auxiliary task). The motivation of the auxiliary task is to preserve more microscopic information in the hierarchical structure of CNN, which is beneficial to malignant nodule identification. The final classification result is obtained by integrating nine 2-D models. We test our method on the benchmark LUNA-16 and LIDC-IDRI datasets and compare it with state-of-the-art models. MT-CNN achieves the lowest false positive rate (3.2%) and highest AUC (97.3%) in LUNA-16 dataset and achieves an AUC of 95.59% in LIDC-IDRI. These results demonstrate the advantage of our method.","Lung,Cancer,Computed tomography,Solid modeling,Task analysis,Image reconstruction,Computational modeling,Multi-task learning,lung nodule classification,image reconstruction,computer-aided diagnosis,convolutional neural network",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,"CONVOLUTIONAL,NEURAL-NETWORK,FALSE-POSITIVE,REDUCTION,SUBSOLID,PULMONARY,NODULES,AUTOMATIC,DETECTION,IMAGES,ENSEMBLE,TEXTURE,SYSTEM",IEEE ACCESS,https://doi.org/10.1109/access.2020.3027812,
62,A Review on EEG Based Epileptic Seizure Prediction Using Machine Learning Techniques,1039,,384-391,"Patel Vibha,Buch Sanjay,Ganatra Amit","Patel V,Buch S,Ganatra A",Patel V,10.1007/978-3-030-30465-2_43,UKA Tarsadia University,"Epilepsy is a typical neurological disorder which influences the person with epilepsy both socially and culturally, especially in India. Epileptic seizures are caused by abnormal activities in brain that can affect patient's health.
If the occurrence of seizure could be predicted well in advance, it could be prevented through medication or proper actions. Electroencephalogram (EEG) is generally used to detect epilepsy as EEG is capable of capturing the electrical activity of brain. In literature, many machine learning techniques were used to extract features from EEG recordings and predict the occurrence seizures. However, techniques with competent performance and clinical applicability are still desirable. We can use the power of modern machine learning techniques to improve the results of seizure prediction which would help to warn the patients for upcoming seizure.","Epilepsy,Prediction,Machine learning",Proceedings Paper,"SPRINGER INTERNATIONAL PUBLISHING AG, GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND",Computer Science,,,,,,
63,Network Output Visualization to Uncover Limitations of Deep Learning Detection of Pneumothorax,11316,,,"Crosby Jennie,Chen Sophia,Li Feng,MacMahon Heber,Giger Maryellen","Crosby J,Chen S,Li F,MacMahon H,Giger M",Crosby J,10.1117/12.2550066,University of Chicago,"The overlapping structures in a chest radiograph can make the detection of pneumothorax difficult. In addition, the visual signs of a pneumothorax, including a fine line at the edge of the lung and a change in texture outside the lung, can be subtle. Some published studies have reported high performances using deep learning for the detection of pneumothorax in chest radiographs using the publicly available ChestX-ray8 dataset. However, at the image input sizes these studies used, 256 x 256 or 224 x 224 pixels, the visual signs of a pneumothorax are typically not visible. In this study, radiographs labeled as pneumothorax in the ChestX-ray8 dataset were interpreted by a radiologist and then confirmed using the radiologist-defined truth from a pneumothorax challenge database. In addition, chest radiographs with and without pneumothorax were obtained from our institution and verified. Therefore, the entire dataset of 5,346 radiographs had truth confirmed by two radiologists. The dataset was used for fine-tuning a VGG19 neural network for the task of detecting pneumothorax in chest radiographs. After fine-tuning was complete, network visualization was performed using Grad-CAM to determine the most influential aspects of the radiograph for the network's classification. It was found that 67% of Grad-CAM heatmaps for correctly classified pneumothorax cases did not have regions of high influence that overlapped with the actual location of the pneumothorax. Overall, the independent test set yielded an AUC of 0.78 (95% confidence interval: 0.74, 0.82) in the task of distinguishing between radiographs with and without pneumothorax.","pneumothorax,chest radiograph,deep learning,convolutional neural networks,visualization,localization",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
64,Deep learning-based model observers that replicate human observers for PET imaging,11316,,,"Fan Fenglei,Ahn Sangtae,De Man Bruno,Wangerin Kristen A.,Wollenweber Scott D.,Abbey Craig K.,Kinahan Paul E.","Fan FL,Ahn S,De Man B,Wangerin KA,Wollenweber SD,Abbey CK,Kinahan PE",Fan FL,10.1117/12.2547505,General Electric,"Model observers that replicate human observers are useful tools for assessing image quality based on detection tasks. Linear model observers including nonprewhitening matched filters (NPWMFs) and channelized Hotelling observers (CHOs) have been widely studied and applied successfully to evaluate and optimize detection performance. However, there is still room for improvement in predicting human observer responses in detection tasks. In this study, we used a convolutional neural network to predict human observer responses in a two-alternative forced choice (2AFC) task for PET imaging. Lesion-absent and lesion-present images were reconstructed from clinical PET data with simulated lesions added to the liver and lungs and were used for the 2AFC task. We trained the convolutional neural network to discriminate images that human observers chose as lesion-present and lesion-absent in the 2AFC task. We evaluated the performance of the trained network by calculating the concordance between human observer responses and predicted responses from the network output and compared it to those of NPWMF and CHO. The trained network showed better agreement with human observers than the linear NPWMF and CHO model observers. The results demonstrate the potential for convolutional neural networks as model observers that better predict human performance. Such model observers can be used for optimizing scanner design, imaging protocols, and image reconstruction to improve lesion detection in PET imaging.","PET imaging,lesion detection,model observer,deep learning,neural network",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,"NUMERICAL,OBSERVERS,ORDERED,SUBSETS,RECONSTRUCTION",,,
65,Investigating the Potential of a Gist-sensitive Computer-aided Detection Tool,11316,,,"Gandomkar Ziba,Ekpo Ernest U.,Lewis Sarah J.,Suleiman Moayyad E.,Tapia Kriscia,Li Tong,Taba Seyedamir Tavakoli,PhuongDung Trieu,Brennan Patrick","Gandomkar Z,Ekpo EU,Lewis SJ,Suleiman ME,Tapia K,Li T,Taba ST,Trieu P,Brennan P",Gandomkar Z,10.1117/12.2549921,University of Sydney,"This study explored the possibility of using the gist signal (radiologists' first impression about a case) for improving the performance of two recently developed deep learning-based breast cancer detection tools. We investigated whether by combining the cancer class probability from the networks with the gist signal, higher performance in identifying malignant cases can be achieved. In total, we recruited 53 radiologists, who provided an abnormality score on a scale from 0 to 100 to unilateral mammograms following a 500-millisecond presentation of the image. Twenty cancer cases, 40 benign cases, and 20 normal were included. Two state-of-the-art deep learning-based tools (M1 and M2) for breast cancer detection were adopted. The abnormality scores from the networks and the gist responses for each observer were fed into a support vector machine (SVM). The SVM was personalized for each radiologist and its performance was evaluated using leave-one-out cross-validation. We also considered the average reader; whose gist responses were the mean abnormality scores given by all 53 readers to each image. The mean and range of AUCs in the gist experiment were 0.643 and 0.492-0.794, respectively. The AUC values for M1 and M2 were 0.789 (0.632-0.892) and 0.814 (0.673-0.897), respectively. For the average reader, the AUC for gist, gist+M1, and gist+M2 were 0.760 (0.617-0.862), 0.847 (0.754-0.928), 0.897 (0.789-0.946). For 45 readers, the performance of at least one of the models improved after aggregating its output with the gist signal. The results showed that the gist signal has the potential to improve the performance of adopted deep learning-based tools.","Breast cancer,Computer-assisted detection,Deep learning,Gist,Holistic processing,Mammography",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,DECISION,,,
66,Combined Global and Local Information for Blind CT Image Quality Assessment via Deep Learning,11316,,,"Gao Qi,Li Sui,Zhu Manman,Li Danyang,Bian Zhaoying,Lv Qingwen,Zeng Dong,Ma Jianhua","Gao Q,Li S,Zhu MM,Li DY,Bian ZY,Lv QW,Zeng D,Ma JH",Ma JH,10.1117/12.2548953,Southern Medical University - China,"Image quality assessment (IQA) is an important step to determine whether the computed tomography (CT) images are suitable for diagnosis. Since the high dose CT images are usually not accessible in clinical practice, no-reference (NR) CT IQA should be used. Most NR-IQA methods for CT images based on deep learning strategy focus on global information and ignores local performance, i.e., contrast, edge of local region. In this work, to address this issue, we presented a new NR-IQA framework combining global and local information for CT images. For simplicity, the NR-IQA framework is termed as NR-GL-IQA. In particular, the presented NR-GL-IQA adopts a convolutional neural network to predict entire image quality blindly without a reference image. In this stage, an elaborate strategy is used to automatically label the entire image quality for neural network training to cope with the problem of time-consuming in manually massive CT images annotation. Second, in the presented NR-GL-IQA method, Perception-based Image QUality Evaluator (PIQUE) is used to predict the local region quality because the PIQUE can adaptively capture the local region characteristics. Finally, the overall image quality is estimated by combining the global and local IQA together. The experimental results with Mayo dataset demonstrate that the presented NR-GL-IQA method can accurately predicts CT image quality and the combination of global and local IQA is closer to the radiologist assessment than that with only one single assessment.","Computed tomography,image quality assessment,no-reference,convolutional neural network,PIQUE",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
67,Learning Efficient Channels with a Dual Loss Autoencoder,11316,,,"Granstedt Jason L.,Zhou Weimin,Anastasio Mark A.","Granstedt JL,Zhou WM,Anastasio MA",Anastasio MA,10.1117/12.2549363,University of Illinois System,"In medical imaging systems, task-based metrics have been advocated as a means of evaluating image quality. Mathematical observers are one method of computing such metrics. Although the Bayesian Ideal Observer (IO) is optimal by definition, it is frequently intractable and non-linear. Linear approximations to the IO are sometimes employed to obtain task-based statistics when computing the IO is infeasible. The optimal linear observer for maximizing the SNR of the test statistic is the Hotelling Observer (HO). However, the computational cost for computing the HO increases with image size and becomes intractable for larger images. Channelized methods of reducing the dimensionality of the data before computing the HO have become popular, with efficient channels capable of approximating the HO's performance at significantly reduced computational cost. State-of-the-art channels have been learned by using an autoencoder (AE) to encode data by employing a known signal template as the desired reconstruction, but the method is dependant on a high-quality estimate of the signal. An alternative to channels is approximating the test statistic directly using a feed-forward neural network (FFNN). However, this approach can overfit when the amount of training data is limited. In this work, a generalized method for learning channels utilizing an AE with dual losses (AEDL) is proposed. The AEDL framework jointly minimizes both task-specific and reconstruction losses to learn a set of efficient channels, even when the number of training images is relatively small. Preliminary results indicate that the proposed network outperforms state-of-the-art methods on the selected imaging task. Additionally, the AEDL framework suffers from less overfitting than the FFNN.","Objective assessment of image quality,imaging system optimization,autoencoders,neural networks,numerical observers",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,"IDEAL-OBSERVER,PERFORMANCE,DETECTION,TASKS",,,
68,Learning Numerical Observers using Unsupervised Domain Adaptation,11316,,,"He Shenghua,Zhou Weimin,Li Hua,Anastasio Mark A.","He SH,Zhou WM,Li H,Anastasio MA",Anastasio MA,10.1117/12.2549812,University of Illinois System,"Medical imaging systems are commonly assessed by use of objective image quality measures. Supervised deep learning methods have been investigated to implement numerical observers for task-based image quality assessment. However, labeling large amounts of experimental data to train deep neural networks is tedious, expensive, and prone to subjective errors. Computer-simulated image data can potentially be employed to circumvent these issues; however, it is often difficult to computationally model complicated anatomical structures, noise sources, and the response of real-world imaging systems. Hence, simulated image data will generally possess physical and statistical differences from the experimental image data they seek to emulate. Within the context of machine learning, these differences between the sets of two images is referred to as domain shift. In this study, we propose and investigate the use of an adversarial domain adaptation method to mitigate the deleterious effects of domain shift between simulated and experimental image data for deep learning-based numerical observers (DL-NOs) that are trained on simulated images but applied to experimental ones. In the proposed method, a DL-NO will initially be trained on computer-simulated image data and subsequently adapted for use with experimental image data, without the need for any labeled experimental images. As a proof of concept, a binary signal detection task is considered. The success of this strategy as a function of the degree of domain shift present between the simulated and experimental image data is investigated.","Numerical observers,unsupervised domain adaptation,image quality assessment,adversarial learning",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,http://arxiv.org/pdf/2002.03763,
69,Evaluation of Convolutional Neural Networks for Search in 1/f(2.8) Filtered Noise and Digital Breast Tomosynthesis Phantoms,11316,,,"Jonnalagadda Aditya,Lago Miguel A.,Barufaldi Bruno,Bakic Predrag R.,Abbey Craig K.,Maidment Andrew D. A.,Eckstein Miguel P.","Jonnalagadda A,Lago MA,Barufaldi B,Bakic PR,Abbey CK,Maidment ADA,Eckstein MP",Jonnalagadda A,10.1117/12.2549362,University of California System,"With the advent of powerful convolutional neural networks (CNNs), recent studies have extended early applications of neural networks to imaging tasks thus making CNNs a potential new tool for assessing medical image quality. Here, we compare a CNN to model observers in a search task for two possible signals (a simulated mass and a smaller simulated micro-calcification) embedded in filtered noise and single slices of Digital Breast Tomosynthesis (DBT) virtual phantoms. For the case of the filtered noise, we show how a CNN can approximate the ideal observer for a search task, achieving a statistical efficiency of 0.77 for the microcalcification and 0.8 for the mass. For search in single slices of DBT phantoms, we show that a Channelized Hotelling Observer (CHO) performance is affected detrimentally by false positives related to anatomic variations and results in detection accuracy below human observer performance. In contrast, the CNN learns to identify and discount the backgrounds, and achieves performance comparable to that of human observer and superior to model observers (Proportion Correct for the microcalcification: CNN = 0.96; Humans = 0.98; CHO = 0.84; Proportion Correct for the mass: CNN = 0.98; Humans = 0.83; CHO = 0.51). Together, our results provide an important evaluation of CNN methods by benchmarking their performance against human and model observers in complex search tasks.","CONTRAST DISCRIMINATION,OBSERVER DETECTION,EFFICIENCY,PERFORMANCE,SIGNALS,TASKS",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,"CONTRAST,DISCRIMINATION,OBSERVER,DETECTION,EFFICIENCY,PERFORMANCE,SIGNALS,TASKS",,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7237823,
70,Supervised Learning of Model Observers for Assessment of CT Image Reconstruction Algorithms,11316,,,"Ongie Gregory,Sidky Emil Y.,Reiser Ingrid S.,Pan Xiaochuan","Ongie G,Sidky EY,Reiser IS,Pan XC",Ongie G,10.1117/12.2549817,University of Chicago,"Given the wide variety of CT reconstruction algorithms currently available - from filtered back projection, to non-linear iterative algorithms, and now even deep learning approaches - there is a pressing need for reconstruction quality metrics that correlate well with task-specific goals. For detection tasks, metrics based on a model observer framework are an attractive option. In this framework, a reconstruction algorithm is assessed based on how well a statistically optimal ""model observer"" performs on a signal present/signal absent detection task. However, computing exact model observers requires a detailed description of the statistics of the reconstructed images, which are often unknown or computationally intractable to obtain, especially in the case of non-linear reconstruction algorithms. Instead, we study the feasibility of using supervised machine learning approaches to approximate model observers in a CT reconstruction setting. In particular, we show that we can well-approximate the Hotelling observer, i.e., the optimal linear classifier, for a signal-known-exactly/background-known-exactly task by training from labeled training images in the case of FBP reconstruction. We also investigate the feasibility of training multi-layer neural networks to approximate the ideal observer in the case of total variation constrained iterative reconstruction. Our results demonstrate that supervised machine learning methods achieve close to ideal performance in both cases.",,Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
71,Anthropomorphic ResNet18 for multi-vendor DBT image quality evaluation,11316,,,"Petrov Dimitar,Marshall Nicholas,Vancoillie Liesbeth,Cockmartin Lesley,Bosmans Hilde","Petrov D,Marshall N,Vancoillie L,Cockmartin L,Bosmans H",Petrov D,10.1117/12.2549000,KU Leuven,"Purpose: This work aims to develop an anthropomorphic convolutional neural network (CNN) classifier, based on the ResNet18 deep learning network and validate it for task based image quality evaluation of digital breast tomosynthesis (DBT) using a structured phantom with non-spiculated mass simulating lesions.
Methods: The phantom is constructed from an acrylic breast-shaped container, filled with acrylic spheres and water resembling the background. Five 3D printed non-spiculated mass targets are also inserted in the phantom each with differing size from 1.5mm to 5.7mm. The phantom was scanned 530 times on 8 different DBT systems with 3 dose levels. Half of the image dataset was read by human readers in 4-alternative forced choice (4-AFC) paradigm. The 4-AFC human scores were used to label the cropped signal present and signal absent images. A pre-trained ResNet18 neural network was used and modified for binary classification and the labeled images were used to further train the network for the specific non-spiculated mass detection task. With completed 50 training epochs, the resulting ResNet18 classifier was validated wit the second half of the image dataset against human results. During the training process the loss and accuracy were stored, and statistical analysis was performed for the validation of the ResNet18 against human observers.
Results and conclusions: The ResNet18 classifier shows good agreement against human observers for most of the DBT systems and reading sessions. The overall correlation was higher than 0.92. The study shows that a CNN can successfully approximate human scores and can be used for future DBT system image quality estimation studies.","Resnet18,Deep learning,Digital breast tomosynthesis,Image quality",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,PHANTOM,,,
72,Repeatability profiles towards consistent sensitivity and specificity levels for machine learning on breast DCE-MRI,11316,,,"Van Dusen Amy,Vieceli Michael,Drukker Karen,Abe Hiroyuki,Giger Maryellen L.,Whitney Heather M.","Van Dusen A,Vieceli M,Drukker K,Abe H,Giger ML,Whitney HM",Van Dusen A,10.1117/12.2548159,Wheaton College,"We evaluated a radiomics/machine learning method for dynamic contrast-enhanced magnetic resonance (DCE-MR) images of breast lesions and the impact of case-based classification repeatability on sensitivity and specificity. DCE-MR images of 1,169 unique breast lesions (267 benign, 902 malignant) were retrospectively collected under HIPAA/IRB. Lesions were automatically segmented using a fuzzy c-means method; thirty-eight radiomic features were extracted. Three classification tasks were investigated: (i) benign vs. malignant, (ii) pure ductal carcinoma in situ (DCIS) vs. DCIS with invasive ductal carcinoma (IDC), and (iii) luminal A or luminal B cancers vs. other molecular subtypes. Case-based repeatability of classifier output was constructed using 0.632+ bootstrap sampling (1000 iterations) with classification by support vector machine (SVM). Repeatability profiles were constructed for each task using the 95% confidence interval widths of the classifier output for cases in the test folds over all bootstrap iterations. The relationships between classifier output repeatability and variability in sensitivity and specificity over the bootstrap test folds were investigated. Most cases fell within the highest repeatability of classifier output over all three classification tasks. Sensitivity and specificity demonstrated more variability in the test folds than in the training folds at corresponding thresholds for the classifier output. Higher repeatability of classifier output was associated with lower variability in sensitivity and specificity in tasks (i) and (ii) but not in task (iii). Case-based repeatability profiles may be important for characterizing impact of using radiomics with desired sensitivity and specificity.","computer-aided diagnosis,radiomics,DCE-MR,breast cancer,repeatability,sensitivity,specificity,machine learning",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,"IMAGE-ANALYSIS,LESIONS,DIAGNOSIS,RADIOMICS",,,
73,Markov-Chain Monte Carlo Approximation of the Ideal Observer using Generative Adversarial Networks,11316,,,"Zhou Weimin,Anastasio Mark A.","Zhou WM,Anastasio MA",Anastasio MA,10.1117/12.2549732,University of Illinois System,"The Ideal Observer (IO) performance has been advocated when optimizing medical imaging systems for signal detection tasks. However, analytical computation of the IO test statistic is generally intractable. To approximate the IO test statistic, sampling-based methods that employ Markov-Chain Monte Carlo (MCMC) techniques have been developed. However, current applications of MCMC techniques have been limited to several object models such as a lumpy object model and a binary texture model, and it remains unclear how MCMC methods can be implemented with other more sophisticated object models. Deep learning methods that employ generative adversarial networks (GANs) hold great promise to learn stochastic object models (SOMs) from image data. In this study, we described a method to approximate the IO by applying MCMC techniques to SOMs learned by use of GANs. The proposed method can be employed with arbitrary object models that can be learned by use of GANs, thereby the domain of applicability of MCMC techniques for approximating the IO performance is extended. In this study, both signal-known-exactly (SKE) and signal-known-statistically (SKS) binary signal detection tasks are considered. The IO performance computed by the proposed method is compared to that computed by the conventional MCMC method. The advantages of the proposed method are discussed.","Ideal Observer,Markov-Chain Monte Carlo,generative adversarial networks,signal detection theory",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,http://arxiv.org/pdf/2001.09526,
74,Progressively-Growing AmbientGANs for learning Stochastic Object Models from imaging measurements,11316,,,"Zhou Weimin,Bhadra Sayantan,Brooks Frank J.,Li Hua,Anastasio Mark A.","Zhou WM,Bhadra S,Brooks FJ,Li H,Anastasio MA",Anastasio MA,10.1117/12.2549610,University of Illinois System,"The objective optimization of medical imaging systems requires full characterization of all sources of randomness in the measured data, which includes the variability within the ensemble of objects to-be-imaged. This can be accomplished by establishing a stochastic object model (SOM) that describes the variability in the class of objects to-be-imaged. Generative adversarial networks (GANs) can be potentially useful to establish SOMs because they hold great promise to learn generative models that describe the variability within an ensemble of training data. However, because medical imaging systems record imaging measurements that are noisy and indirect representations of object properties, GANs cannot be directly applied to establish stochastic models of objects to-be-imaged. To address this issue, an augmented GAN architecture named AmbientGAN was developed to establish SOMs from noisy and indirect measurement data. However, because the adversarial training can be unstable, the applicability of the AmbientGAN can be potentially limited. In this work, we propose a novel training strategy-Progressive Growing of AmbientGANs (ProAGAN)-to stabilize the training of AmbientGANs for establishing SOMs from noisy and indirect imaging measurements. An idealized magnetic resonance (MR) imaging system and clinical MR brain images are considered. The proposed methodology is evaluated by comparing signal detection performance computed by use of ProAGAN-generated synthetic images and images that depict the true object properties.","stochastic object model,generative adversarial networks,signal detection,objective assessment of image quality",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,http://arxiv.org/pdf/2001.09523,
75,Identifying episodes of sleep apnea in ECG by machine learning methods,,,588-593,"Ivanko Kateryna,Ivanushkina Nataliia,Rykhalska Anna","Ivanko K,Ivanushkina N,Rykhalska A",Ivanko K,,Ministry of Education & Science of Ukraine,"The paper is devoted to the application of machine learning methods for computerized sleep apnea detection based on single lead electrocardiographic signal (ECG). In order to explore the possibilities of machine learning for ECG-based apnea detection, the Apnea-ECG database provided by the PhysioNet resource was used in this study. A distinctive peculiarity of this database is that it contains the annotations for each minute of each recording indicating the presence or absence of apnea at the current moment of time. To evaluate the effectiveness of ECG derived parameters for sleep apnea detection, 80 ECG segments of 10 minutes duration, annotated as apnea, and 73 ECG segments of the same duration, annotated as normal sleeping were extracted and investigated.
The purpose of this work is to define and compare the informative features for identifying episodes of sleep apnea in ECG by heart rate variability analysis, as well as to choose the classification method that provides the highest accuracy for this task. The time-domain, frequency domain, spectraltemporal and wavelet features are considered. Using these feature sets, the performances of a number of classifiers based on decision trees, discriminant analysis, logistic regression, support vector machines, variations of k-nearest neighbors' method, and ensemble learning, were determined. Based on this, a combination of features and classifiers are proposed that provides the highest accuracy of sleep apnea episodes recognition in single lead ECG. The choice of model options for the best performing classifiers was investigated.","ECG,obstructive sleep apnea,heart rate variability,spectral analysis,classification,machine learning",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Science & Technology - Other Topics",,,HEART-RATE-VARIABILITY,2020 IEEE 40TH INTERNATIONAL CONFERENCE ON ELECTRONICS AND NANOTECHNOLOGY (ELNANO),,
76,Application of Machine Learning Methods for Artificial ECG with T-wave alternans,,,613-617,"Karnaukh Oleksandra);,Karplyuk Yevgeniy","Karnaukh O,Karplyuk Y",,,"
","This paper presents the modeling approach artificial electrocardiograms (ECG) with T-wave alternans based on extracted parameters from the T-wave alternas (TWA) database. The developed optimal TWA classification system was evaluated by signals from a mixed database, which consisted of the TWA database and artificial ECG modeled records. F1-score about 95,9 % was received for the Random Forest Classifier (RFC) and Sequential Forward Floating Feature Selection method. Using a wrapper method for feature selection, 14 significant features were selected that associate with TWA.","T-wave alternans,ECG modeling,feature extraction,artificial database,feature selection",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Science & Technology - Other Topics",,,"MODIFIED,MOVING,AVERAGE",2020 IEEE 40TH INTERNATIONAL CONFERENCE ON ELECTRONICS AND NANOTECHNOLOGY (ELNANO),,
77,Detection of cancerous tissue in histopathological images using Dual-Channel Residual Convolutional Neural Networks (DCRCNN),,,197-202,"Chakraborty Sabyasachi,Aich Satyabrata,Kumar Avinash,Sarkar Sobhangi,Sim Jong-Seong,Kim Hee-Cheol","Chakraborty S,Aich S,Kumar A,Sarkar S,Sim JS,Kim HC",Chakraborty S,,Inje University,"Computer-aided detection techniques to improve precision diagnostic capability and efficiency in the diagnosis process has been regarded as one of the most important topics in the field of computer vision. The medical imaging data with respect to a patient is primarily considered as one of the most important sources to derive the information regarding the biomarkers of a particular disease. But the successful detection of biomarkers requires the radiologist and the pathologist to have long term experience in this field. Therefore, the development of computer-aided detection is one of the primary concerns that need to be discussed. Moreover with the advent of Deep Learning and Artificial Intelligence, now the detection of anomalies and aneurysms in the medical imagery can become much more precise and efficient. Therefore this particular paper presents a dual-channel residual convolution neural network (CNN) model for the automated classification and detection of cancerous tissues in histopathological images. The proposed CNN model has been trained with 220,025 histopathological images and has achieved an overall accuracy of 96.475%, average recall of 95.72% and an average precision of 95.92% respectively.","DCRCNN,CNN,Deep Learning,Histopathology,Cancer",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Computer Science,Telecommunications",,,BREAST-CANCER,,,
78,Parkinson's Disease Detection from Spiral and Wave Drawings using Convolutional Neural Networks: A Multistage Classifier Approach,,,298-303,"Chakraborty Sabyasachi,Aich Satyabrata,Jong-Seong-Sim,Han Eunyoung,Park Jinse,Kim Hee-Cheol","Chakraborty S,Aich S,Jong-Seong-Sim,Han E,Park J,Kim HC",Kim HC,,Inje University,"Identification of the correct biomarkers with respect to particular health issues and detection of the same is of paramount importance for the development of clinical decision support systems. For the patients suffering from Parkinson's Disease (PD), it has been duly observed that impairment in the handwriting is directly proportional to the severity of the disease. Also, the speed and pressure applied to the pen while sketching or writing something are also much lower in patients suffering from Parkinson's disease. Therefore, correctly identifying such biomarkers accurately and precisely at the onset of the disease will lead to a better clinical diagnosis. Therefore, in this paper, a system design is proposed for analyzing Spiral drawing patterns and wave drawing patterns in patients suffering from Parkinson's disease and healthy subjects. The system developed in the study leverages two different convolutional neural networks (CNN), for analyzing the drawing patters of both spiral and wave sketches respectively. Further, the prediction probabilities are trained on a metal classifier based on ensemble voting to provide a weighted prediction from both the spiral and wave sketch. The complete model was trained on the data of 55 patients and has achieved an overall accuracy of 93.3%, average recall of 94%, average precision of 93.5% and average f1 score of 93.94%","Parkinson,CNN,deep learning,machine learning,classification",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Computer Science,Telecommunications",,,,,,
79,COUPLING PRINCIPLED REFINEMENT WITH BI-DIRECTIONAL DEEP ESTIMATION FOR ROBUST DEFORMABLE 3D MEDICAL IMAGE REGISTRATION,,,86-90,"Zhang Yuxi,Liu Risheng,Li Zi,Liu Zhu,Fan Xin,Luo Zhongxuan","Zhang YX,Liu RS,Li Z,Liu Z,Fan X,Luo ZX",Fan X,,Dalian University of Technology,"Deformable 3D medical image registration is challenging due to the complicated transformations between image pairs. Traditional approaches estimate deformation fields by optimizing a task-guided energy embedded with physical priors, achieving high accuracy while suffering from expensive computational loads for the iterative optimization. Recently, deep networks, encoding the information underlying data examples, render fast predictions but severely dependent on training data and have limited flexibility. In this study, we develop a paradigm integrating the principled prior into a bidirectional deep estimation process. Inheriting from the merits of both domain knowledge and deep representation, our approach achieves a more efficient and stable estimation of deformation fields than the state-of-the-art, especially when the testing pairs exhibit great variations with the training.","Medical image registration,optimization,deep learning,3D neural network",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
80,ENHANCED IMAGE REGISTRATION WITH A NETWORK PARADIGM AND INCORPORATION OF A DEFORMATION REPRESENTATION MODEL,,,91-94,"Sang Yudi,Ruan Dan","Sang YD,Ruan D",Sang YD,,University of California System,"In conventional registration methods, regularization functionals and balancing hyper-parameters need to be designed and tuned. Even so, heterogeneous tissue property and balance requirement remain challenging. In this study, we propose a registration network with a novel deformation representation model to achieve spatially variant conditioning on the deformation vector field (DVF). In the form of a convolutional auto-encoder, the proposed representation model is trained with a rich set of DVFs as a feasibility descriptor. Then the auto-encoding discrepancy is combined with fidelity in training the overall registration network in an unsupervised learning paradigm. The trained network generates DVF estimates from paired images with a single forward inference evaluation run. Experiments with synthetic images and 3D cardiac MRIs demonstrate that the method can accomplish registration with physically and physiologically more feasible DVFs, sub-pixel registration errors and millisecond execution time, and incorporation of the representation model improved the registration network performance significantly.","Deep Learning,image registration,representation model",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
81,AGE-CONDITIONED SYNTHESIS OF PEDIATRIC COMPUTED TOMOGRAPHY WITH AUXILIARY CLASSIFIER GENERATIVE ADVERSARIAL NETWORKS,,,109-112,"Kan Chi Nok Enoch,Maheenaboobacker Najibakram,Ye Dong Hye","Kan CNE,Maheenaboobacker N,Ye DH",Kan CNE,,Marquette University,"Deep learning is a popular and powerful tool in computed tomography (CT) image processing such as organ segmentation, but its requirement of large training datasets remains a challenge. Even though there is a large anatomical variability for children during their growth, the training datasets for pediatric CT scans are especially hard to obtain due to risks of radiation to children. In this paper, we propose a method to conditionally synthesize realistic pediatric CT images using a new auxiliary classifier generative adversarial network (ACGAN) architecture by taking age information into account. The proposed network generated age-conditioned high-resolution CT images to enrich pediatric training datasets.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
82,SIMULTANEOUS CLASSIFICATION AND SEGMENTATION OF INTRACRANIAL HEMORRHAGE USING A FULLY CONVOLUTIONAL NEURAL NETWORK,,,118-121,"Guo Danfeng,Wei Haihua,Zhao Pengfei,Pan Yue,Yang Hao-Yu,Wang Xin,Bai Junjie,Cao Kunlin,Song Qi,Xia Jun","Guo DF,Wei HH,Zhao PF,Pan Y,Yang HY,Wang X,Bai JJ,Cao KL,Song Q,Xia J",Guo DF; Gao F; Yin YB,,"CuraCloud Corp, Seattle, WA 98195 USA.","Intracranial hemorrhage (ICH) is a critical disease that requires immediate diagnosis and treatment. Accurate detection, subtype classification and volume quantification of ICH are critical aspects in ICH diagnosis. Previous studies have applied deep learning techniques for ICH analysis but usually tackle the aforementioned tasks in a separate manner without taking advantage of information sharing between tasks. In this paper, we propose a multi-task fully convolutional network, ICHNet, for simultaneous detection, classification and segmentation of ICH. The proposed framework utilizes the inter-slice contextual information and has the flexibility in handling various label settings and task combinations. We evaluate the performance of our proposed architecture using a total of 1176 head CT scans and show that it improves the performance of both classification and segmentation tasks compared with single-task and baseline models.","intracranial hemorrhage,segmentation,classification,multi-task learning,fully convolutional network",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
83,CNN DETECTION OF NEW AND ENLARGING MULTIPLE SCLEROSIS LESIONS FROM LONGITUDINAL MRI USING SUBTRACTION IMAGES,,,127-130,"Sepahvand Nazanin Mohammadi,Arnold Douglas L.,Arbel Tal","Sepahvand NM,Arnold DL,Arbel T",Sepahvand NM,,McGill University,"Accurate detection and segmentation of new lesional activity in longitudinal Magnetic Resonance Images (MRIs) of patients with Multiple Sclerosis (MS) is important for monitoring disease activity, as well as for assessing treatment effects. In this work, we present the first deep learning framework to automatically detect and segment new and enlarging (NE) T2w lesions from longitudinal brain MRIs acquired from relapsing-remitting MS (RRMS) patients. The proposed framework is an adapted 3D U-Net [1] which includes as inputs the reference multi-modal MRI and T2-weighted lesion maps, as well an attention mechanism based on the subtraction MRI (between the two timepoints) which serves to assist the network in learning to differentiate between real anatomical change and artifactual change, while constraining the search space for small lesions. Experiments on a large, proprietary, multi-center, multi-modal, clinical trial dataset consisting of 1677 multi-modal scans illustrate that network achieves high overall detection accuracy (detection AUC=.95), outperforming (1) a U-Net without an attention mechanism (detection AUC=.93), (2) a framework based on subtracting independent T2-weighted segmentations (detection AUC=.57), and (3) DeepMedic (detection AUC=.84) [2], particularly for small lesions. In addition, the method was able to accurately classify patients as active/inactive with (sensitivities of .69 and specificities of .97).","Multiple Sclerosis,Longitudinal Magnetic Resonance Imaging,Longitudinal Lesion Detection,Deep Learning,New Lesions,Enlarging Lesions",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
84,BRAIDED NETWORKS FOR SCAN-AWARE MRI BRAIN TISSUE SEGMENTATION,,,136-139,"Mostapha Mahmoud,Mailhe Boris,Ghen Xiao,Ceccaldi Pascal,Yoo Youngjin,Nadar Mariappan","Mostapha M,Mailhe B,Ghen X,Ceccaldi P,Yoo Y,Nadar M",Mostapha M,,University of North Carolina,"Recent advances in supervised deep learning, mainly using convolutional neural networks, enabled the fast acquisition of high-quality brain tissue segmentation from structural magnetic resonance brain images (MRI). However, the robustness of such deep learning models is limited by the existing training datasets acquired with a homogeneous MRI acquisition protocol. Moreover, current models fail to utilize commonly available relevant non-imaging information (i.e., meta-data). In this paper, the notion of a braided block is introduced as a generalization of convolutional or fully connected layers for learning from paired data (meta-data, images). For robust MRI tissue segmentation, a braided 3D U-Net architecture is implemented as a combination of such braided blocks with scanner information, MRI sequence parameters, geometrical information, and task-specific prior information used as meta-data. When applied to a large ( > 16,000 scans) and highly heterogeneous (wide range of MRI protocols) dataset, our method generates highly accurate segmentation results (Dice scores > 0.9) within seconds**.","MRI,Tissue Segmentation,Model Robustness,Convolutional Neural Networks,Multimodal Learning",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
85,7T GUIDED 3T BRAIN TISSUE SEGMENTATION USING CASCADED NESTED NETWORK,,,140-143,"Wei Jie,Bui Duc Toan,Wu Zhengwang,Wang Li,Xia Yong,Liz Gang,Shen Dinggang","Wei J,Bui DT,Wu ZW,Wang L,Xia Y,Liz G,Shen DG",Xia Y,,Northwestern Polytechnical University,"Accurate segmentation of the brain into major tissue types, e.g., the gray matter, white matter, and cerebrospinal fluid, in magnetic resonance (MR) imaging is critical for quantification of the brain anatomy and function. The availability of 7T MR scanners can provide more accurate and reliable voxel-wise tissue labels, which can be leveraged to supervise the training of the tissue segmentation in the conventional 3T brain images. Specifically, a deep learning based method can be used to build the highly non-linear mapping from the 3T intensity image to the more reliable label maps obtained from the 7T images of the same subject. However, the misalignment between 3T and 7T MR images due to image distortions poses a major obstacle to achieving better segmentation accuracy. To address this issue, we measure the quality of the 3T-7T alignment by using a correlation coefficient map. Then we propose a cascaded nested network (CaNes-Net) for 3T MR image segmentation and a multi-stage solution for training this model with the ground-truth tissue labels from 7T images. This paper has two main contributions. First, by incorporating the correlation loss, the above mentioned obstacle can be well addressed. Second, the geodesic distance maps are constructed based on the intermediate segmentation results to guide the training of the CaNes-Net as an iterative coarse-to-fine process. We evaluated the proposed CaNes-Net with the state-of-the-art methods on 18 in-house acquired subjects. We also qualitatively assessed the performance of the proposed model and U-Net on the ADNI dataset. Our results indicate that the proposed CaNes-Net is able to dramatically reduce mis-segmentation caused by the misalignment and achieves substantially improved accuracy over all the other methods.","7T MR,tissue segmentation,cascaded nested network",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
86,Fully Unsupervised Probabilistic Noise2Void,,,154-158,"Prakash Mangal,Lalit Manan,Tomancak Pavel,Krull Alexander,Jug Florian","Prakash M,Lalit M,Tomancak P,Krull A,Jug F",Prakash M,,"Ctr Syst Biol Dresden CSBD, Dresden, Germany.","Image denoising is the first step in many biomedical image analysis pipelines and Deep Learning (DL) based methods are currently best performing. A new category of DL methods such as Noise2Void or Noise2Self can be used fully unsupervised, requiring nothing but the noisy data. However, this comes at the price of reduced reconstruction quality. The recently proposed Probabilistic Noise2Void (PN2V) improves results, but requires an additional noise model for which calibration data needs to be acquired. Here, we present improvements to PN2V that (i) replace histogram based noise models by parametric noise models, and (ii) show how suitable noise models can be created even in the absence of calibration data. This is a major step since it actually renders PN2V fully unsupervised. We demonstrate that all proposed improvements are not only academic but indeed relevant.","unsupervised denoising,deep learning,microscopy,noise model,gaussian mixture model,bootstrapping",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,http://arxiv.org/pdf/1911.12291,
87,REMOVING STRUCTURED NOISE WITH SELF-SUPERVISED BLIND-SPOT NETWORKS,,,159-163,"Broaddus Coleman,Krull Alexander,Weigert Martin,Schmidt Uwe,Myers Gene","Broaddus C,Krull A,Weigert M,Schmidt U,Myers G",Broaddus C,,Max Planck Society,"Removal of noise from fluorescence microscopy images is an important first step in many biological analysis pipelines. Current state-of-the-art supervised methods employ convolutional neural networks that are trained with clean (ground-truth) images. Recently, it was shown that self-supervised image denoising with blind spot networks achieves excellent performance even when ground-truth images are not available, as is common in fluorescence microscopy. However, these approaches, e.g. Noise2Void ( N2V), generally assume pixel-wise independent noise, thus limiting their applicability in situations where spatially correlated (structured) noise is present. To overcome this limitation, we present Structured Noise2Void (STRUCTN2V), a generalization of blind spot networks that enables removal of structured noise without requiring an explicit noise model or ground truth data. Specifically, we propose to use an extended blind mask (rather than a single pixel/blind spot), whose shape is adapted to the structure of the noise. We evaluate our approach on two real datasets and show that STRUCTN2V considerably improves the removal of structured noise compared to existing standard and blind-spot based techniques.","image denoising,deep learning,CNN,self-supervised,structured noise",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
88,FINE-GRAINED MULTI-INSTANCE CLASSIFICATION IN MICROSCOPY THROUGH DEEP ATTENTION,,,169-173,"Fan Mengran,Chakraborti Tapabrata,Chang Eric I-Chao,Xu Yan,Rittscher Jens","Fan MR,Chakraborti T,Chang EIC,Xu Y,Rittscher J",Rittscher J,,University of Oxford,"Fine-grained object recognition and classification in biomedical images poses a number of challenges. Images typically contain multiple instances (e.g. glands) and the recognition of salient structures is confounded by visually complex backgrounds. Due to the cost of data acquisition or the limited availability of specimens, data sets tend to be small. We propose a simple yet effective attention based deep architecture to address these issues, specially to improve background suppression and recognition of important instances per image. Attention maps per instance are learnt in an end-to-end fashion. Microscopic images of fungi (new data) and a publicly available Breast Cancer Histology benchmark dataset are used to demonstrate the performance of the proposed approach. Experimental results suggest that the proposed approach advances the state-of-the-art.","attention models,fine-grained classification,object recognition,medical image analysis,deep learning,convolutional neural networks",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,https://ora.ox.ac.uk/objects/uuid:2e9a4171-0df0-4725-b920-9bf1ee58b778/download_file?safe_filename=Fan%2520et%2520al%2520Fine-grained%2520multi-instance%2520classification.pdf&type_of_work=Conference+item,
89,TWO-LAYER RESIDUAL SPARSIFYING TRANSFORM LEARNING FOR IMAGE RECONSTRUCTION,,,174-177,"Zheng Xuehang,Ravishankar Saiprasad,Long Yong,Klasky Marc Louis,Wohlberg Brendt","Zheng XH,Ravishankar S,Long Y,Klasky ML,Wohlberg B",Long Y,,Shanghai Jiao Tong University,"Signal models based on sparsity, low-rank and other properties have been exploited for image reconstruction from limited and corrupted data in medical imaging and other computational imaging applications. In particular, sparsifying transform models have shown promise in various applications, and offer numerous advantages such as efficiencies in sparse coding and learning. This work investigates pre-learning a two-layer extension of the transform model for image reconstruction, wherein the transform domain or filtering residuals of the image are further sparsified in the second layer. The proposed block coordinate descent optimization algorithms involve highly efficient updates. Preliminary numerical experiments demonstrate the usefulness of a two-layer model over the previous related schemes for CT image reconstruction from low-dose measurements.","Low-dose CT,statistical image reconstruction,sparse representation,transform learning,unsupervised learning",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,MODEL,,http://arxiv.org/pdf/1906.00165,
90,TRANSFER-GAN: MULTIMODAL CT IMAGE SUPER-RESOLUTION VIA TRANSFER GENERATIVE ADVERSARIAL NETWORKS,,,195-198,"Xiao Yao,Peters Keith R.,Fox W. Christopher,Rees John H.,Rajderkar Dhanashree A.,Arreola Manuel M.,Barreto Izabella,Bolch Wesley E.,Fang Ruogu","Xiao Y,Peters KR,Fox WC,Rees JH,Rajderkar DA,Arreola MM,Barreto I,Bolch WE,Fang RG",Fang RG,,State University System of Florida,"Multimodal CT scans, including non-contrast CT, CT perfusion, and CT angiography, are widely used in acute stroke diagnosis and therapeutic planning. While each imaging modality has its advantage in brain cross-sectional feature visualizations, the varying image resolution of different modalities hinders the ability of the radiologist to discern consistent but subtle suspicious findings. Besides, higher image quality requires a high radiation dose, leading to increases in health risks such as cataract formation and cancer induction. In this work, we propose a deep learning-based method Transfer-GAN that utilizes generative adversarial networks and transfer learning to improve multimodal CT image resolution and to lower the necessary radiation exposure. Through extensive experiments, we demonstrate that transfer learning from multimodal CT provides substantial visualization and quantity enhancement compare to the training without learning the prior knowledge.","Image Super-Resolution,Multimodal CT,Transfer Learning,Generative Adversarial Network",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
91,A FULLY 3D CASCADED FRAMEWORK FOR PANCREAS SEGMENTATION,,,207-211,"Wang Wenzhe,Song Qingyu,Feng Ruiwei,Chen Tingting,Chen Jintai,Chen Danny Z.,Wu Jian","Wang WZ,Song QY,Feng RW,Chen TT,Chen JT,Chen DZ,Wu J",Wu J,,Zhejiang University,"Convolutional Neural Networks (CNNs) have achieved remarkable results for many medical image segmentation tasks. However, segmenting small and polymorphous organs (e.g., pancreas) in 3D CT images is still highly challenging due to the complexity of such organs and the difficulties in 3D context information learning restricted by limited GPU memory. In this paper, we present a Fully 3D Cascaded Framework for pancreas segmentation in 3D CT images. We develop a 3D detection network (PancreasNet) to regress the locations of pancreas regions, and two different scales of a 3D segmentation network (SEVoxNet) to segment pancreas in a cascaded manner based on the detection results of PancreasNet. Experiments on the public NIH pancreas segmentation dataset show that we achieve 85.93% in the mean DSC and 75.38% in the mean JI, outperforming state-of-the-art results and with the fastest inference time ever reported (similar to 200 times faster).","pancreas segmentation,3D CT images,deep neural networks,cascaded framework",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
92,SSN: A STAIR-SHAPE NETWORK FOR REAL-TIME POLYP SEGMENTATION IN COLONOSCOPY IMAGES,,,225-229,"Feng Ruiwei,Lei Biwen,Wang Wenzhe,Chen Tingting,Chen Jintai,Chen Danny Z.,Wu Jian","Feng RW,Lei BW,Wang WZ,Chen TT,Chen JT,Chen DZ,Wu J",Feng RW,,Zhejiang University,"Colorectal cancer is one of the most life-threatening malignancies, commonly occurring from intestinal polyps. Currently, clinical colonoscopy exam is an effective way for early detection of polyps and is often conducted in real-time manner. But, colonoscopy analysis is time-consuming and suffers a high miss rate. In this paper, we develop a novel stair-shape network (SSN) for real-time polyp segmentation in colonoscopy images (not merely for simple detection). Our new model is much faster than U-Net, yet yields better performance for polyp segmentation. The model first utilizes four blocks to extract spatial features at the encoder stage. The subsequent skip connection with a Dual Attention Module for each block and a final Multi-scale Fusion Module are used to fully fuse features of different scales. Based on abundant data augmentation and strong supervision of auxiliary losses, our model can learn much more information for polyp segmentation. Our new polyp segmentation method attains high performance on several datasets (CVC-ColonDB, CVC-ClinicDB, and EndoScene), outperforming state-of-the-art methods. Our network can also be applied to other imaging tasks for real-time segmentation and clinical practice.","Colorectal cancer,real-time polyp segmentation,colonoscopy images,deep learning networks",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
93,MULTIPLE INSTANCE LEARNING VIA DEEP HIERARCHICAL EXPLORATION FOR HISTOLOGY IMAGE CLASSIFICATION,,,235-238,"Hering Jan,Kybic Jan","Hering J,Kybic J",Hering J,,Czech Technical University Prague,"We present a fast hierarchical method to detect a presence of cancerous tissue in histological images. The image is not examined in detail everywhere but only inside several small regions of interest, called glimpses. The final classification is done by aggregating classification scores from a CNN on leaf glimpses at the highest resolution. Unlike in existing attention-based methods, the glimpses form a tree structure, low resolution glimpses determining the location of several higher resolution glimpses using weighted sampling and a CNN approximation of the expected scores. We show that it is possible to perform the classification with just a small number of glimpses, leading to an important speedup with only a small performance deterioration. Learning is possible using image labels only, as in the multiple instance learning (MIL) setting.","image segmentation,hierarchical,histology,microscopy,CNN",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
94,WEAKLY SUPERVISED PROSTATE TMA CLASSIFICATION VIA GRAPH CONVOLUTIONAL NETWORKS,,,239-243,"Wang Jingwen,Chen Richard J.,Lu Ming Y.,Baras Alexander,Mahmood Faisal","Wang JW,Chen RJ,Lu MY,Baras A,Mahmood F",Wang JW,,Harvard University,"Histology-based grade classification is clinically important for many cancer types in stratifying patients into distinct treatment groups. In prostate cancer, the Gleason score is a grading system used to measure the aggressiveness of prostate cancer from the spatial organization of cells and the distribution of glands. However, the subjective interpretation of Gleason score often suffers from large interobserver and intraobserver variability. Previous work in deep learning-based objective Gleason grading requires manual pixel-level annotation. In this work, we propose a weakly-supervised approach for grade classification in tissue micro-arrays (TMA) using graph convolutional networks (GCNs), in which we model the spatial organization of cells as a graph to better capture the proliferation and community structure of tumor cells. We learn the morphometry of each cell using a contrastive predictive coding (CPC)-based self-supervised approach. Using five-fold cross-validation we demonstrate that our method can achieve a 0.9637 +/- 0.0131 AUC using only TMA-level labels. Our method also demonstrates a 36.36% improvement in AUC over standard GCNs with texture features and a 15.48% improvement over GCNs with VGG19 features. Our proposed pipeline can be used to objectively stratify low and high-risk cases, reducing inter- and intra-observer variability and pathologist workload.","Gleason Score Grading,Graph Convolutional Networks,Deep Learning,Histopathology Calassification,Objective Grading,Patient Stratification",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,http://arxiv.org/pdf/1910.13328,
95,CIRCULAR ANCHORS FOR THE DETECTION OF HEMATOPOIETIC CELLS USING RETINANET,,,249-253,"Graebel Philipp,Oezkan Oezcan,Crysandt Martina,Herwartz Reinhild,Baumann Melanie,Klinkhammer Barbara M.,Boor Peter,Bruemmendorf Tim H.,Merhof Dorit","Grabel P,Ozkan O,Crysandt M,Herwartz R,Baumann M,Klinkhammer BM,Boor P,Brummendorf TH,Merhof D",Grabel P,,RWTH Aachen University,"Analysis of the blood cell distribution in bone marrow is necessary for a detailed diagnosis of many hematopoietic diseases, such as leukemia. While this task is performed manually on microscope images in clinical routine, automating it could improve reliability and objectivity.
Cell detection tasks in medical imaging have successfully been solved using deep learning, in particular with RetinaNet, a powerful network architecture that yields good detection results in this scenario. It utilizes axis-parallel, rectangular bounding boxes to describe an object's position and size. However, since cells are mostly circular, this is suboptimal.
We replace RetinaNet's anchors with more suitable Circular Anchors, which cover the shape of cells more precisely. We further introduce an extension to the Non-maximum Suppression algorithm that copes with predictions that differ in size. Experiments on hematopoietic cells in bone marrow images show that these methods reduce the number of false positive predictions and increase detection accuracy.","Cell Detection,RetinaNet,Bone Marrow",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
96,SEMI-SUPERVISED BRAIN LESION SEGMENTATION USING TRAINING IMAGES WITH AND WITHOUT LESIONS,,,279-282,"Liu Chenghao,Pang Fengqian,Liu Yanlin,Liang Kongming,Li Xiuli,Zeng Xiangzhu,Ye Chuyang","Liu CH,Pang FQ,Liu YL,Liang KM,Li XL,Zeng XZ,Ye CY",Ye CY,,Beijing Institute of Technology,"Semi-supervised approaches have been developed to improve brain lesion segmentation based on convolutional neural networks (CNNs) when annotated data is scarce. Existing methods have exploited unannotated images with lesions to improve the training of CNNs. In this work, we explore semi-supervised brain lesion segmentation by further incorporating images without lesions. Specifically, using information learned from annotated and unannotated scans with lesions, we propose a framework to generate synthesized lesions and their annotations simultaneously. Then, we attach them to normal-appearing scans using a statistical model to produce synthesized training samples, which are used together with true annotations to train CNNs for segmentation. Experimental results show that our method outperforms competing semi-supervised brain lesion segmentation approaches.","Semi-supervised learning,brain lesion segmentation,training sample synthesis",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
97,AN IMPROVED DEEP LEARNING APPROACH FOR THYROID NODULE DIAGNOSIS,,,296-299,"Guo Xiangdong,Zhao Haifeng,Tang Zhenyu","Guo XD,Zhao HF,Tang ZY",Tang ZY,,Beihang University,"Although thyroid ultrasonography (US) has been widely applied, it is still difficult to distinguish benign and malignant nodules. Currently, convolutional neural network (CNN) based methods have been proposed and shown promising performance for benign and malignant nodules classification. It is known that the US images are usually captured in multiangles, and the same thyroid in different US images have inconsistent content. However, most of the existing CNN based methods extract features using fixed convolution kernels, which could be a big issue for processing US images. Moreover, fully-connected (FC) layers are usually adopted in CNN, which could cause the loss of inter-pixel relations. In this paper, we propose a new CNN which is integrated with squeeze-and-excitation (SE) module and maximum retention of inter-pixel relations module (CNN-SE-MPR). It can adaptively select features from different US images and preserve the inter-pixel relations. Moreover, we introduce transfer learning to avoid problems such as local optimum and data insufficiency. The proposed network is tested on 407 thyroid US images collected from cooperated hospitals. Confirmed by ablation experiments and the comparison experiments under the state-of-the-art methods, it is shown that our method improves the accuracy of the diagnosis results.","Thyroid,ultrasonography,CNN,classification,transfer learning",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
98,PROGRESSIVE ABDOMINAL SEGMENTATION WITH ADAPTIVELY HARD REGION PREDICTION AND FEATURE ENHANCEMENT,,,329-332,"Wang Qin,Zhao Weibing,Zhang Ruimao,Li Zhen,Cui Shuguang","Wang Q,Zhao WB,Zhang RM,Li Z,Cui SG",Li Z,,Chinese University of Hong Kong,"Abdominal multi-organ segmentation achieves much attention in recent medical image analysis. In this paper, we propose a novel progressive framework to promote the segmentation accuracy of abdominal organs with various shapes and small sizes. The entire framework consists of three parts: 1) a Global Segmentation Module extracting the pixel-wise global feature representation; 2) a Localization Module adaptively discovering the top-n hard local regions and effective both in training and testing phase; 3) an Enhancement Module enhancing the features of hard local regions and aggregating with the global features to refine the final representation. Specifically, we pre-define 512 region proposals on the cross-sectional view of the CT image to generate pseudo labels which can supervise Localization Module. In the training phase, we calculate the segmentation error of each region proposal and select the eight ones with the lowest Dice scores as the hard regions. Once these hard regions are determined, their center coordinates are adopted as the pseudo labels to train the Localization Network by using Manhattan Distance Loss. For inference, the entire model directly accomplishes the hard region localization and feature enhancement to promote pixel-wise accuracy. Without bells and whistles, extensive experimental results demonstrate that the proposed method outperforms its counterparts.","Progressive Multi-organ Segmentation,Adaptive Hard Region Localization,Feature Enhancement",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
99,ANNOTATION-FREE GLIOMAS SEGMENTATION BASED ON A FEW LABELED GENERAL BRAIN TUMOR IMAGES,,,354-358,"Dong Hexin,Yu Fei,Jiang Han,Zhang Hua,Dong Bin,Li Quanzheng,Zhang Li","Dong HX,Yu F,Jiang H,Zhang H,Dong B,Li QZ,Zhang L",Zhang L,,Peking University,"Pixel-level labeling for medical image segmentation is time-consuming and sometimes infeasible. Therefore, using a small amount of labeled data in one domain to help train a reasonable segmentation model for unlabeled data in another domain becomes an important need in medical image segmentation. In this work, we propose a new segmentation framework based on unsupervised domain adaptation and semi-supervised learning, which uses a small amount of labeled general brain tumor images and learns an effective model to segment independent brain gliomas images. Our method contains two major parts. First, we use unsupervised domain adaptation to generate synthetic general brain tumor images from the brain gliomas images. Then, we apply semi-supervised learning method to train a segmentation model with a small number of labeled general brain tumor images and the unlabeled synthetic images. The experimental results show that our proposed method can use approximate 10% of labeled data to achieve a comparable accuracy of the model trained with all labeled data.","Unsupervised Domain Adaptation,Semi-supervised Learning,Semantic Segmentation,MixMatch,Brain Gliomas Segmentation",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
100,6-MONTH INFANT BRAIN MRI SEGMENTATION GUIDED BY 24-MONTH DATA USING CYCLE-CONSISTENT ADVERSARIAL NETWORKS,,,359-362,"Bui Than Duc,Wang Li,Lin Weili,Li Gang,Shen Dinggang","Bui TD,Wang L,Lin WL,Li G,Shen DG",Wang L; Li G; Shen DG,,University of North Carolina,"Due to the extremely low intensity contrast between the white matter (WM) and the gray matter (GM) at around 6 months of age (the isointense phase), it is difficult for manual annotation, hence the number of training labels is highly limited. Consequently, it is still challenging to automatically segment isointense infant brain MRI. Meanwhile, the contrast of intensity images in the early adult phase, such as 24 months of age, is a relatively better, which can be easily segmented by the well-developed tools, e.g., FreeSurfer. Therefore, the question is how could we employ these high-contrast images (such as 24-month-old images) to guide the segmentation of 6-month-old images. Motivated by the above purpose, we propose a method to explore the 24-month-old images for a reliable tissue segmentation of 6-month-old images. Specifically, we design a 3D-cycleGAN-Seg architecture to generate synthetic images of the isointense phase by transferring appearances between the two time-points. To guarantee the tissue segmentation consistency between 6-month-old and 24-month-old images, we employ features from generated segmentations to guide the training of the generator network. To further improve the quality of synthetic images, we propose a feature matching loss that computes the cosine distance between unpaired segmentation features of the real and fake images. Then, the transferred of 24-month-old images is used to jointly train the segmentation model on the 6-month-old images. Experimental results demonstrate a superior performance of the proposed method compared with the existing deep learning-based methods.","6-month infant brain segmentation,cycleGAN,synthetic image",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8375399,
