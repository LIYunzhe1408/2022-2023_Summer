,title,Volume,Issue,Pages,whole__author_name,simply_author_name,reprint author,DOI,reprint address,Abstract,Keywords,Document Type,Publisher,Research Domain,Published Date,impact_factor,Keywords_plus,joural,pdf_link,Download_SuccessOrDefeat
1,TCAD-Machine Learning Framework for Device Variation and Operating Temperature Analysis With Experimental Demonstration,8,,992-1000,"Wong Hiu Yung,Xiao Ming,Wang Boyan,Chiu Yan Ka,Yan Xiaodong,Ma Jiahui,Sasaki Kohei,Wang Han,Zhang Yuhao","Wong HY,Xiao M,Wang BY,Chiu YK,Yan XD,Ma JH,Sasaki K,Wang H,Zhang YH",Wong HY,10.1109/JEDS.2020.3024669,California State University System,"This work, for the first time, experimentally demonstrates a TCAD-Machine Learning (TCAD-ML) framework to assist the analysis of device-to-device variation and operating (ambient) temperature without the need of physical quantities extraction. The ML algorithm used in this work is the Principal Component Analysis (PCA) followed by third order polynomial regression. After calibrated to limited 'expensive' experimental data, 'low cost' TCAD simulation is used to generate a large amount of device data to train the ML model. The ML was then used to identify the root cause of device variation and operating temperature from any given experimental current-voltage (I-V) characteristics. We applied this framework to study the ultra-wide-bandgap gallium oxide (Ga2O3) Schottky barrier diode (SBD), an emerging device technology that holds great promise for temperature sensing, RF, and power applications in harsh environments. After calibration, over 150,000 electrothermal TCAD simulations are performed with random variation of physical parameters (anode effective work function, drift layer doping, and drift layer thickness) and operating temperature. An ML model is trained using these TCAD data and we found 1,000-10,000 TCAD data can train an accurate machine. We show that without physical quantities extraction, performing PCA is essential for the TCAD trained ML model to be applicable to analyze experimental characteristics. The physical parameters and temperatures predicted by the ML model show good agreement with experimental analysis. Our TCAD-ML framework shows great promise to accelerate the development of new device technologies with a significantly more efficient process of material and device experimentation.","Silicon,Data models,Schottky barriers,Semiconductor process modeling,Analytical models,Performance evaluation,Principal component analysis,TCAD simulation,machine learning,variation,principal component analysis,ultra-wide bandgap,gallium oxide",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA",Engineering,,2.452,BETA-GA2O3,IEEE JOURNAL OF THE ELECTRON DEVICES SOCIETY,https://ieeexplore.ieee.org/ielx7/6245494/8949832/09199850.pdf,
2,Locomo-Net: A Low -Complex Deep Learning Framework for sEMG-Based Hand Movement Recognition for Prosthetic Control,8,,,"Gautam Arvind,Panwar Madhuri,Wankhede Archana,Arjunan Sridhar P.,Naik Ganesh R.,Acharyya Amit,Kumar Dinesh K.","Gautam A,Panwar M,Wankhede A,Arjunan SP,Naik GR,Acharyya A,Kumar DK",Acharyya A,10.1109/JTEHM.2020.3023898,Indian Institute of Technology System (IIT System),"Background: The enhancement in the performance of the myoelectric pattern recognition techniques based on deep learning algorithm possess computationally expensive and exhibit extensive memory behavior. Therefore, in this paper we report a deep learning framework named 'Low-Complex Movement recognition-Net' (LoCoMo-Net) built with convolution neural network (CNN) for recognition of wrist and finger flexion movements; grasping and functional movements; and force pattern from single channel surface electromyography (sEMG) recording. The network consists of a two-stage pipeline: 1) input data compression; 2) data-driven weight sharing. Methods: The proposed framework was validated on two different datasets- our own dataset (DS1) and publicly available NinaPro dataset (DS2) for 16 movements and 50 movements respectively. Further, we have prototyped the proposed LoCoMo-Net on Virtex-7 Xilinx field-programmable gate array (FPGA) platform and validated for 15 movements from DS1 to demonstrate its feasibility for real-time execution. Results: The effectiveness of the proposed LoCoMo-Net was verified by a comparative analysis against the benchmarked models using the same datasets wherein our proposed model outperformed Twin- Support Vector Machine (SVM) and existing CNN based model by an average classification accuracy of 8.5 % and 16.0 % respectively. In addition, hardware complexity analysis is done to reveal the advantages of the two-stage pipeline where approximately 27 %, 49 %, 50 %, 23 %, and 43 % savings achieved in lookup tables (LUT's), registers, memory, power consumption and computational time respectively. Conclusion: The clinical significance of such sEMG based accurate and low-complex movement recognition system can be favorable for the potential improvement in quality of life of an amputated persons.","Electrodes,Task analysis,Pattern recognition,Muscles,Feature extraction,Real-time systems,Wrist,sEMG,movement classification,signal processing,CNN,data compression,weights compression",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA",Engineering,,3.192,"EMG,PATTERN-RECOGNITION,CLASSIFICATION,IMPLEMENTATION,SENSORS,FPGA",IEEE JOURNAL OF TRANSLATIONAL ENGINEERING IN HEALTH AND MEDICINE,https://ieeexplore.ieee.org/ielx7/6221039/8943243/09197671.pdf,
3,Learning Path Recommendation System for Programming Education Based on Neural Networks,18,1,36-64,"Saito Tomohiro,Watanobe Yutaka","Saito T,Watanobe Y",Saito T,10.4018/IJDET.2020010103,University of Aizu,"Programming education has recently received increased attention due to growing demand for programming and information technology skills. However, a lack of teaching materials and human resources presents a major challenge to meeting this demand. One way to compensate for a shortage of trained teachers is to use machine learning techniques to assist learners. This article proposes a learning path recommendation system that applies a recurrent neural network to a learner's ability chart, which displays the learner's scores. In brief, a learning path is constructed from a learner's submission history using a trial-and-error process, and the learner's ability chart is used as an indicator of their current knowledge. An approach for constructing a learning path recommendation system using ability charts and its implementation based on a sequential prediction model and a recurrent neural network, are presented. Experimental evaluation is conducted with data from an e-learning system.","Ability Chart,Adaptive Learning,Awareness Computing,Deep Learning,Educational Data Mining,E-learning System,Online Judge System,Programming Education",Article,"IGI GLOBAL, 701 E CHOCOLATE AVE, STE 200, HERSHEY, PA 17033-1240 USA",Education & Educational Research,,,,INTERNATIONAL JOURNAL OF DISTANCE EDUCATION TECHNOLOGIES,,
4,Interpretability of Deep Learning Classification for Low-Carbon Steel Microstructures,61,8,1584-1592,"Maemura Tatsuya,Terasaki Hidenori,Tsutsui Kazumasa,Uto Kyohei,Hiramatsu Shogo,Hayashi Kotaro,Moriguchi Koji,Morito Shigekazu","Maemura T,Terasaki H,Tsutsui K,Uto K,Hiramatsu S,Hayashi K,Moriguchi K,Morito S",Maemura T,10.2320/matertrans.MT-M2020131,Kumamoto University,"In this paper, a model is developed to identify the microstructure of low-carbon steel by deep learning. In classifying steel microstructures using a machine learning model, predictions are interpreted using local interpretable model-agnostic explanations (LIME) for the first time. The constructed model can accurately distinguish between eight microstructure types, including upper bainite, lower bainite, martensite, and their mixed structures. The model accuracy is 94.1% when individually predicted and 97.9% when predicted by majority vote. In addition, as a result of interpreting the predictions of the model by LIME, it is evident that the recognition criterion of the constructed model is partially consistent with the classic recognition criterion.","low-carbon steel,martensite,bainite,scanning electron microscopy image,deep learning,local interpretable model-agnostic explanations",Article,"JAPAN INST METALS & MATERIALS, 1-14-32, ICHIBANCHO, AOBA-KU, SENDAI, 980-8544, JAPAN","Materials Science,Metallurgy & Metallurgical Engineering",,1.215,FEATURES,MATERIALS TRANSACTIONS,https://www.jstage.jst.go.jp/article/matertrans/61/8/61_MT-M2020131/_pdf,
5,Performance Comparison of Machine Learning Techniques for Epilepsy Classification and Detection in EEG Signal,1042,,425-438,"Janghel Rekh Ram,Verma Archana,Rathore Yogesh Kumar","Janghel RR,Verma A,Rathore YK",Janghel RR,10.1007/978-981-32-9949-8_29,National Institute of Technology (NIT System),"Epilepsy is a neurological affliction that in impact around 1% of humankind. Around 10% of the United States populace involvement with minimum a solitary convulsion in their activity. Epilepsy has recognized respectively tendency of the cerebrum outcomes unforeseen blasts of weird electrical action which disturbs the typical working of the mind. Since spasms by and large happen once in a while and are unforeseeable, seizure identification frameworks are proposed for seizure discovery amid long haul electroencephalography (EEG). In this exploration, we utilize DWT for highlight extraction and do correlation for all kind of Machine learning order like SVM, Nearest Neighbor Classifiers, Logistic relapse, Ensemble classifiers and so on. In this examination classification accuracy of Fine Gaussian SVM recorded as 100% and it has better as compare to other existing machine learning approaches.","Decision tree,Ensemble classifier,Electroencephalogram (EEG),Epileptic seizure,k-Nearest neighbor,Support vector machine",Proceedings Paper,"SPRINGER-VERLAG SINGAPORE PTE LTD, 152 BEACH ROAD, #21-01/04 GATEWAY EAST, SINGAPORE, 189721, SINGAPORE",Computer Science,,,"SEIZURE,DETECTION,LONG-TERM,WAVELET,TRANSFORM,NEURAL-NETWORK,PREDICTION,ARTIFACTS,REMOVAL,SYSTEM",,,
6,Coronary Artery Disease Diagnosis Using Feature Selection Based Hybrid Extreme Learning Machine,,,341-346,"Shahid Afzal Hussain,Singh Maheshwari Prasad,Roy Bishwajit,Aadarsh Aashish","Shahid AH,Singh MP,Roy B,Aadarsh A",Shahid AH,10.1109/ICICT50521.2020.00060,National Institute of Technology (NIT System),"Coronary artery disease (CAD) is the most common cardiovascular disease (CVD) that cause millions of deaths worldwide due to heart failure, heart attack, and angina. The symptoms of the CAD do not appear in the early stage of the disease and it causes deadly conditions; therefore, accurate and early diagnosis of CAD is necessary to take appropriate and timely action for preventing or minimizing such conditions. Angiography, being the most accurate method for diagnosis of CAD, is often used by the clinicians to diagnose the CAD but this is an invasive procedure, costly, and may cause side effects. Therefore, researchers are trying to develop alternative diagnostic modalities for the efficient diagnosis of CAD. To that end, machine learning and data mining techniques have been widely employed. This paper proposes and develops hybrid Particle swarm optimization based Extreme learning machine (PSO-ELM) for diagnosis of CAD using the publicly available Z-Alizadeh sani dataset. To enhance the performance of the proposed model, a feature selection algorithm, namely Fisher, is used to find more discriminative feature subset. In the training period, the PSO algorithm is used to calibrate the ELM input weights and hidden biases. Further, the performance of the proposed model is compared with the basic ELM in terms of accuracy, Pearson correlation coefficient (R-2) and Root mean square error (RMSE) goodness-of-fit functions. The results show that the performance of the proposed model is better than the basic ELM. The obtained CAD classification performance in terms of sensitivity, accuracy, specificity, and F1-measure is competitive to the known approaches in the literature.","coronary artery disease,cardiovascular disease,particle swarm optimization,extreme learning machine,feature selection method",Proceedings Paper,"IEEE COMPUTER SOC, 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA","Computer Science,Engineering",,,NETWORK,2020 3RD INTERNATIONAL CONFERENCE ON INFORMATION AND COMPUTER TECHNOLOGIES (ICICT 2020),,
7,AUTOMATED DETECTION OF MALARIAL RETINOPATHY USING TRANSFER LEARNING,,,18-21,"Kurup A.,Soliz P.,Nemeth S.,Joshi V","Kurup A,Soliz P,Nemeth S,Joshi V",Kurup A,,"VisionQuest Biomed Inc, Albuquerque, NM 87106 USA.","Cerebral Malaria (CM) is a severe neurological syndrome of malaria mainly found in children and is associated with highly specific retinal lesions. The manifestation of these indications of CM in the retina is called malarial retinopathy (MR). All patients showing clinical signs of CM are commonly diagnosed and treated accordingly; however, 23% of them are misdiagnosed as they suffer from another infection with identical clinical symptoms. Due to these underlying symptoms, the false positive cases may go untreated and could result in death of the patients. A diagnostic test is needed that is highly specific in order to reduce false positives. The purpose of this study to demonstrate a technique based on a transfer learning technique using images from three different retinal cameras to identify the hemorrhages and whitening lesions in the retina which can accurately identify the patients with MR. The MR detection model gives a specificity of 100% and a sensitivity of 90% with an AUC of 0.98. The algorithm demonstrates the potential of accurate MR detection with a low-cost retinal camera.","Malarial Retinopathy (MR),Cerebral malaria (CM),Transfer learning,Convolutional neural network (CNN),Data Augmentation",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Computer Science,Imaging Science & Photographic Technology",,,"CEREBRAL,MALARIA,CHILDREN",,,
8,Combined Detection and Segmentation of Cell Nuclei in Microscopy Images Using Deep Learning,,,26-29,"Ram Sundaresh,Nguyen Vicky T.,Limesand Kirsten H.,Rodriguez Jeffrey J.","Ram S,Nguyen VT,Limesand KH,Rodriguez JJ",Ram S,,University of Michigan System,"We propose a 3D convolutional neural network to simultaneously segment and detect cell nuclei in confocal microscopy images. Mirroring the co-dependency of these tasks, our proposed model consists of two serial components: the first part computes a segmentation of cell bodies, while the second module identifies the centers of these cells. Our model is trained end-to-end from scratch on a mouse parotid salivary gland stem cell nuclei dataset comprising 107 3D images from three independent cell preparations, each containing several hundred individual cell nuclei in 3D. In our experiments, we conduct a thorough evaluation of both detection accuracy and segmentation quality, on two different datasets. The results show that the proposed method provides significantly improved detection and segmentation accuracy compared to existing algorithms. Finally, we use a previously described test-time drop-out strategy to obtain uncertainty estimates on our predictions and validate these estimates by demonstrating that they are strongly correlated with accuracy.","Cell nucleus detection,image segmentation,convolutional neural networks,deep learning,confocal microscopy",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Computer Science,Imaging Science & Photographic Technology",,,,,,
9,A Machine Learning Model for Exploring Aberrant Functional Network Connectivity Transition in Schizophrenia,,,112-115,"Sendi Mohammad S. E.,Zendehrouh Elaheh,Fu Zening,Mahmoudi Babak,Miller Robyn L.,Calhoun Vince D.","Sendi MSE,Zendehrouh E,Fu ZN,Mahmoudi B,Miller RL,Calhoun VD",Sendi MSE,,University System of Georgia,"Schizophrenia (SZ) is a severe neuropsychiatric disorder with a hallmark of functional dysconnectivity between numerous brain regions. With an implicit assumption of stationary brain interactions during the scanning period, most of the resting-state functional magnetic resonance imaging (fMRI) studies are conducted on static functional network connectivity (sFNC). Dynamic functional network connectivity (dFNC) that explores temporal patterns of functional connectivity (FC) might provide additional information to its static counterpart. In this work, we first estimate latent features (called connectivity states) by applying k-means clustering on dFNC. Next, using the estimated latent features, we trained and tested a classifier, which can differentiate SZ from healthy control (HC) subjects with 71% accuracy. Using a feature selection method embedded in the classifier, we have highlighted the role of transition probabilities between states as potential biomarkers and identified the role of lightly modularized transient connectivity state in pulling healthy subjects out of both highly modularized and very disconnected states. This will offer some new understandings about the way the healthy brain shifts between the most and the least connected states of whole brain connectivity.","Schizophrenia,resting-state fMRI,dynamic functional network connectivity,machine learning,feature learning",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Computer Science,Imaging Science & Photographic Technology",,,SELECTION,,,
10,DEEP LEARNING CLASSIFICATION OF CHEST X-RAY IMAGES,,,116-119,"Majdi Mohammad S.,Salman Khalil N.,Morris Michael F.,Merchant Nirav C.,Rodriguez Jeffrey J.","Majdi MS,Salman KN,Morris MF,Merchant NC,Rodriguez JJ",Majdi MS,,University of Arizona,"We propose a deep learning based method for classification of commonly occurring pathologies in chest X-ray images. The vast number of publicly available chest X-ray images provides the data necessary for successfully employing deep learning methodologies to reduce the misdiagnosis of thoracic diseases. We applied our method to the classification of two example pathologies, pulmonary nodules and cardiomegaly, and we compared the performance of our method to three existing methods. The results show an improvement in AUC for detection of nodules and cardiomegaly compared to the existing methods.","chest X-ray,deep learning,classification,pulmonary nodule,cardiomegaly",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Computer Science,Imaging Science & Photographic Technology",,,TUBERCULOSIS,,http://arxiv.org/pdf/2005.09609,
11,Detection and Classification of Tumors Using Medical Imaging Techniques: A Survey,33,,363-372,"Garg Sheetal,Bhagyashree S. R.","Garg S,Bhagyashree SR",Garg S,10.1007/978-3-030-28364-3_35,"ATME Coll Engn, Mysuru, India.","Cancer (tumors) is the cause of every sixth death around the world. This makes cancer a second leading cause of death. Globally 42 million people across the world suffer from cancer and this figure is continuously increasing. In India around 2.5 million people are suffering from different types of cancer. If detected in early stage, then with proper treatment it can be cured. This paper presents details of a few methods used for detection of diseases like Breast cancer, brain tumor, liver cancer, lung cancer and Spine tumor. This paper also speaks about the different machine learning techniques used to classify the diseases into malignant & benign.","Positron Emission Tomography/ComputedTomography(PET/CT),Magnetic Resonance Imaging (MRI),Artificial Neural Network (ANN),Fuzzy c means classifier,Support Vector Machine (SVM),Region of interest",Proceedings Paper,"SPRINGER INTERNATIONAL PUBLISHING AG, GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND","Computer Science,Telecommunications",,,,,,
12,Galectin-9-based immune risk score model helps to predict relapse in stage I-III small cell lung cancer,8,2,,"Chen Peixin,Zhang Liping);,Zhang Wei);,Sun Chenglong,Wu Chunyan,He Yayi,Zhou Caicun","Chen PX,Zhang LP,Zhang W,Sun CL,Wu CY,He YY,Zhou CC",Zhou CC,10.1136/jitc-2020-001391,Tongji University,"Background For small cell lung cancer (SCLC) therapy, immunotherapy might have unique advantages to some extent. Galectin-9 (Gal-9) plays an important role in antitumor immunity, while little is known of its function in SCLC. Materials and methods By mean of immunohistochemistry (IHC), we tested the expression level of Gal-9 and other immune markers on both tumor cells and tumor-infiltrating lymphocytes (TILs) in 102 surgical-resected early stage SCLC clinical samples. On the basis of statistical analysis and machine learning results, the Gal-9-based immune risk score model was constructed and its predictive performance was evaluated. Then, we thoroughly explored the effects of Gal-9 and immune risk score on SCLC immune microenvironment and immune infiltration in different cohorts and platforms. Results In the SCLC cohort for IHC, the expression level of Gal-9 on TILs was statistically correlated with the levels of program death-1 (p=0.001), program death-ligand 1 (PD-L1) (p<0.001), CD3 (p<0.001), CD4 (p<0.001), CD8 (p<0.001), and FOXP3 (p=0.047). High Gal-9 protein expression on TILs indicated better recurrence-free survival (30.4 months, 95% CI: 23.7-37.1 vs 39.4 months, 95% CI: 31.6-47.3, p=0.009). The immune risk score model which consisted of Gal-9 on TILs, CD4, and PD-L1 on TILs was established and validated so as to differentiate high-risk or low-risk patients with SCLC. The prognostic predictive performance of immune risk score model was better than single immune biomarker (area under the curve 0.671 vs 0.621-0.644). High Gal-9-related enrichment pathways in SCLC were enriched in immune system diseases and rheumatic disease. Furthermore, we found that patients with SCLC with low immune risk score presented higher fractions of activated memory CD4 T cells than patients with high immune risk score (p=0.048). Conclusions Gal-9 is markedly related to tumor-immune microenvironment and immune infiltration in SCLC. This study emphasized the predictive value and promising clinical applications of Gal-9 in stage I-III SCLC.","lung neoplasms,tumor microenvironment,lymphocytes,tumor-infiltrating,programmed cell death 1 receptor,T-lymphocytes",Article,"BMJ PUBLISHING GROUP, BRITISH MED ASSOC HOUSE, TAVISTOCK SQUARE, LONDON WC1H 9JR, ENGLAND","Oncology,Immunology",,,"T-CELLS,PROTEIN,EXPRESSION,PROGNOSTIC-FACTOR,SUPPRESSES,APOPTOSIS,PROLIFERATION,MACROPHAGES,GENERATION,RESISTANCE,SURVIVAL",JOURNAL FOR IMMUNOTHERAPY OF CANCER,https://jitc.bmj.com/content/jitc/8/2/e001391.full.pdf,
13,EPICARDIAL FAT TISSUE VOLUMETRY: COMPARISON OF SEMI-AUTOMATIC MEASUREMENT AND THE MACHINE LEARNING ALGORITHM,60,9,46-54,"Chernina V. Y.,Pisov M. E.,Belyaev M. G.,Bekk I. V,Zamyatina K. A.,Korb T. A.,Aleshina O. O.,Shukina E. A.,Solovev A. V,Skvortsov R. A.","Chernina VY,Pisov ME,Belyaev MG,Bekk IV,Zamyatina KA,Korb TA,Aleshina OO,Shukina EA,Solovev AV,Skvortsov RA",Chernina VY,10.18087/cardio.2020.9.n1111,"Moscow Hlth Care Dept, Res & Pract Clin Ctr Diagnost & Telemedicine Tech, Moscow, Russia.",,"COMPUTED-TOMOGRAPHY,ADIPOSE-TISSUE,PERICARDIAL FAT,ASSOCIATION,DISEASE,CT",Article,"RUSSIAN HEART FAILURE SOC, 215, 5, BEREGOVOY PROEZD, MOSCOW, 121087, RUSSIA",Cardiovascular System & Cardiology,,0.317,"COMPUTED-TOMOGRAPHY,ADIPOSE-TISSUE,PERICARDIAL,FAT,ASSOCIATION,DISEASE,CT",KARDIOLOGIYA,https://lib.ossn.ru/jour/article/download/1111/819,
14,Discrimination of Genuine and Acted Emotional Expressions Using EEG Signal and Machine Learning,8,,191080-191089,"Alex Meera,Tariq Usman,Al-Shargie Fares,Mir Hasan S.,Al Nashash Hasan","Alex M,Tariq U,Al-Shargie F,Mir HS,Al Nashash H",Al-Shargie F,10.1109/ACCESS.2020.3032380,American University of Sharjah,"We present here one of the first studies that attempt to differentiate between genuine and acted emotional expressions, using EEG data. We present the first EEG dataset (available here) with recordings of subjects with genuine and fake emotional expressions. We build our experimental paradigm for classification of smiles; genuine smiles, fake/acted smiles and neutral expression. We propose multiple methods to extract intrinsic features from three EEG emotional expressions; genuine, neutral, and fake/acted smile. We extracted EEG features using three time-frequency analysis methods: discrete wavelet transforms (DWT), empirical mode decomposition (EMD), and incorporating DWT into EMD (DWT-EMD) at three frequency bands. We then evaluated the proposed methods using several classifiers including, k-nearest neighbors (KNN), support vector machine (SVM), and artificial neural network (ANN). We carried out an experimental paradigm on 28-subjects underwent three types of emotional expressions, genuine, neutral and fake/acted. The results showed that incorporating DWT into EMD extracted more hidden features than sole DWT or sole EMD method. The power spectral feature extracted by DWT, EMD, and DWT-EMD showed different neural patterns across the three emotional expressions at all the frequency bands. We performed binary classification experiments and achieved acceptable accuracy reaching a maximum of 84% in all type of emotions, classifiers and bands using sole DWT or EMD. Meanwhile, a combination of DWT-EMD achieved the highest classification accuracy with ANN in classifying true emotional expressions from fake expressions in the alpha and beta bands with an average accuracy of 94.3% and 84.1%, respectively. Our results suggest combining DWT-EMD for future emotion studies and highlight the association of alpha and beta frequency bands with emotions.","Emotion recognition,electroencephalogram (EEG),discrete wavelet transforms (DWT),empirical mode decomposition (EMD),classification",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,"RECOGNITION,SELECTION,FUSION",IEEE ACCESS,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09233330.pdf,
15,"Reliable Tuberculosis Detection Using Chest X-Ray With Deep Learning, Segmentation and Visualization",8,,191586-191601,"Rahman Tawsifur,Khandakar Amith,Kadir Muhammad Abdul,Islam Khandaker Rejaul,Islam Khandakar F.,Mazhar Rashid,Hamid Tahir,Islam Mohammad Tariqul,Kashem Saad,Bin Mahbub Zaid","Rahman T,Khandakar A,Kadir MA,Islam KR,Islam KF,Mazhar R,Hamid T,Islam MT,Kashem S,Bin Mahbub Z",Ayari MA,10.1109/ACCESS.2020.3031384,Qatar University,"Tuberculosis (TB) is a chronic lung disease that occurs due to bacterial infection and is one of the top 10 leading causes of death. Accurate and early detection of TB is very important, otherwise, it could be life-threatening. In this work, we have detected TB reliably from the chest X-ray images using image preprocessing, data augmentation, image segmentation, and deep-learning classification techniques. Several public databases were used to create a database of 3500 TB infected and 3500 normal chest X-ray images for this study. Nine different deep CNNs (ResNet18, ResNet50, ResNet101, ChexNet, InceptionV3, Vgg19, DenseNet201, SqueezeNet, and MobileNet) were used for transfer learning from their pre-trained initial weights and were trained, validated and tested for classifying TB and non-TB normal cases. Three different experiments were carried out in this work: segmentation of X-ray images using two different U-net models, classification using X-ray images and that using segmented lung images. The accuracy, precision, sensitivity, F1-score and specificity of best performing model, ChexNet in the detection of tuberculosis using X-ray images were 96.47%, 96.62%, 96.47%, 96.47%, and 96.51% respectively. However, classification using segmented lung images outperformed that with whole X-ray images; the accuracy, precision, sensitivity, F1-score and specificity of DenseNet201 were 98.6%, 98.57%, 98.56%, 98.56%, and 98.54% respectively for the segmented lung images. The paper also used a visualization technique to confirm that CNN learns dominantly from the segmented lung regions that resulted in higher detection accuracy. The proposed method with state-of-the-art performance can be useful in the computer-aided faster diagnosis of tuberculosis.","Tuberculosis detection,TB screening,deep learning,transfer learning,lungs segmentation,image processing",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,"CONVOLUTIONAL,NEURAL-NETWORKS,CLASSIFICATION,ABNORMALITIES,ERROR",IEEE ACCESS,https://doi.org/10.1109/access.2020.3031384,
16,Human Pose Estimation-Based Real-Time Gait Analysis Using Convolutional Neural Network,8,,191542-191550,"Rohan Ali,Rabah Mohammed,Hosny Tarek,Kim Sung-Ho","Rohan A,Rabah M,Hosny T,Kim SH",Rohan A,10.1109/ACCESS.2020.3030086,Dongguk University,"Gait analysis is widely used in clinical practice to help in understanding the gait abnormalities and its association with a certain underlying medical condition for better diagnosis and prognosis. Several technologies embedded in the specialized devices such as computer-interfaced video cameras to measure patient motion, electrodes placed on the surface of the skin to appreciate muscle activity, force platforms embedded in a walkway to monitor the forces and torques produced between the ambulatory patient and the ground, Inertial Measurement Unit (IMU) sensors, and wearable devices are being used for this purpose. All of these technologies require an expert to translate the data recorded by the said embedded specialized devices, which is typically done by a medical expert but with the recent improvements in the field of Artificial Intelligence (AI), especially in deep learning, it is possible now to create a mechanism where the translation of the data can be performed by a deep learning tool such as Convolutional Neural Network (CNN). Therefore, this work presents an approach where human pose estimation is combined with a CNN for classification between normal and abnormal gait of a human with an ability to provide information about the detected abnormalities form an extracted skeletal image in real-time.","Pose estimation,Two dimensional displays,Convolutional neural networks,Deep learning,Training,Principal component analysis,Visualization,Convolutional neural network,deep learning,gait analysis,pose estimation",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,"DISCRIMINANT-ANALYSIS,RECOGNITION,PERFORMANCE,EIGENFACES",IEEE ACCESS,https://doi.org/10.1109/access.2020.3030086,
17,A Topic Learning Pipeline for Curating Brain Cognitive Researches,8,,191758-191774,"Sheng Ying,Chen Jianhui,He Xiaobo,Xu Zhe,Gao Jiangfan,Lin Shaofu","Sheng Y,Chen JH,He XB,Xu Z,Gao JF,Lin SF",Lin SF,10.1109/ACCESS.2020.3032173,"Beijing Univ Technol, Beijing Inst Smart City, Beijing 100124, Peoples R China.","Cognition is the most basic but complex process of human beings. Benefit from noninvasive neuroimaging technologies, a series of important brain projects have been carried out to model cognition from different aspects and levels. Because modeling such a complex phenomenon requires characterizations of numerous entities and cannot only depend on the efforts of one or more laboratories within a project cycle, a lot of neuroimaging text mining researches have focused on curating neuroimaging-based brain cognitive raw data, derived data and result data, to collect multi-aspect information about brain cognitive researches for comprehensively and objectively characterizing key entities of brain cognition. However, the data-centric perspective leads to the shortcomings of poor topic semantics and topic independent results. This paper proposes a brand-new perspective of big data sharing in neuroimaging, that is, curating brain cognitive researches. A new task definition of neuroimaging text mining and a topic learning pipeline integrating the heterogeneous deep neural networks and density clustering of topic relations are designed to realize this new perspective. The experimental results on actual data sets show that the proposed method can obtain more accurate and complete research topics for effectively characterizing brain cognition and its researches.","Neuroimaging text mining,topic learning,data curation,brain cognition",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,,IEEE ACCESS,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09229383.pdf,
18,Leak Detection Using Flow-Induced Vibrations in Pressurized Wall-Mounted Water Pipelines,8,,188673-188687,"Virk Mati-Ur-Rasool Ashraf,Mysorewala Muhammad Faizan,Cheded Lahouari);,Ali Ibrahim Mohamed","Virk MURA,Mysorewala MF,Cheded L,Ali IM",Mysorewala MF,10.1109/ACCESS.2020.3032319,King Fahd University of Petroleum & Minerals,"Wireless sensor networks (WSN) provide a powerful solution to the task of monitoring the operational conditions of buried and non-buried pipes of different lengths and materials. Due to the limited energy stored in the sensor nodes, the use of low-power vibration sensors becomes the preferred choice. However, the monitoring of vibrations for leak detection in wall-mounted pipelines, and the associated complexities are not adequately dealt with in the literature. This article offers to fill this gap by presenting a feasibility study of leak detection in wall-mounted water pipelines through vibrations measurements using low-power accelerometers. The work is divided into two steps: Firstly, a careful analysis is performed to understand the effect of various fittings such as clamps, bends, and leaks of various sizes, on the vibrations produced. Then this knowledge is used to find the best locations for placement of nodes in order to efficiently detect leaks of various sizes. This analysis revealed two important facts: (a) difficulty in detecting medium-size leaks as their vibrations and those from the no-leak condition are very indistinguishable, (b) vibrations measured away from the leak are of a small benefit to the leak detection process. Consequently, 3 different learning models are applied, all fed with information from multiple nodes, in order to reliably detect leaks and classify their size. Comparing the performances of these models shows that the Support Vector Machine (SVM)-based model gives the best results, in that for the worst case of medium-size leaks and with the use of one sensor, the worst accuracies for leak detection and leak size classification have remarkably been improved from being respectively 51x0025; and 36x0025; with one sensor, to being 88x0025; and 93x0025;, respectively, with only a moderate increase in the number of sensors to four.","Accelerometers,leak detection,machine learning,vibration measurement,wall-mounted water pipelines,sensor networks",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,"NETWORKS,SENSOR",IEEE ACCESS,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09229405.pdf,
19,Energy and Materials-Saving Management via Deep Learning for Wastewater Treatment Plants,8,,191694-191705,"Wang Jianhui,Wan Keyi,Gao Xu,Cheng Xuhong,Shen Yu,Wen Zheng,Tariq Usman,Piran Md Jalil","Wang JH,Wan KY,Gao X,Cheng XH,Shen Y,Wen Z,Tariq U,Piran MJ",Shen Y,10.1109/ACCESS.2020.3032531,Chongqing Technology & Business University,"With the increasing public attention on sustainability, conservation of energy and materials has been a general demand for wastewater treatment plants (WWTPs). To meet the demand, efficient optimal management and decision mechanism are expected to reasonably configure resource of energy and materials.In recent years, advanced computational techniques such as neural networks and genetic algorithm provided data-driven solutions to overcome some industrial problems. They work from the perspective of statistical learning, mining invisible latent rules from massive data. This paper proposes energy and materials-saving management via deep learning for WWTPs, using real-world business data of a wastewater treatment plant located in Chongqing, China. Treatment processes are modeled through neural networks, and materials cost that satisfies single indexes can be estimated on this basis. Then, genetic algorithm is selected as the decision scheme to compute overall cost that is able to simultaneously satisfy all the indexes. Empirically, experimental results evaluate that with the proposed management method, total energy and materials cost can be reduced by 10%-15%.","Wastewater treatment,deep learning,optimal management,genetic algorithm",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,"CONVOLUTIONAL,NETWORKS,DATA,ANALYTICS,OPTIMIZATION,EFFICIENCY,SYSTEM,MODEL",IEEE ACCESS,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09233427.pdf,
20,Automated Pterygium Detection Using Deep Neural Network,8,,191659-191672,"Zamani N. Syahira M.,Zaki Wan Mimi Diyana Wan,Huddin Aqilah Baseri,Hussain Aini,Mutalib Haliza Abdul,Ali Aziah","Zamani NSM,Zaki WMDW,Huddin AB,Hussain A,Mutalib HA,Ali A",Zaki WMDW,10.1109/ACCESS.2020.3030787,Universiti Kebangsaan Malaysia,"Ocular imaging has developed rapidly and plays a critical role in clinical care and ocular disease management. Development of image processing technologies pertinent to ocular diseases has paved the way for automated diagnostic systems including detection techniques using deep learning (DL) approaches. The prevalence of an abnormal tissue layer in the conjunctiva, known as pterygium eye disease, is increasing due to lack of awareness. Despite the non-cancerous/benign nature of pterygium, a clinical diagnosis from an ophthalmologist is still required to prevent the pterygium tissues from extending into the pupil, which would result in blurred vision. However, current diagnostic methods are mostly dependent on human expertise. Automated detection can potentially serve as an assistive method to reduce diagnosis time by applying a DL approach. Considering the lack of comprehensive research work on pterygium detection using DL, we propose a new architecture consisting of an improved CNN-based trained network named VggNet16-wbn that is derived from VggNet16, a pre-trained CNN algorithm. This paper presents an overview of the DL as a core approach to the transfer learning (TL) concept, as well as current efforts towards automated ocular detection approaches. A new architecture of a CNN-based trained network was proposed based on a network assessment from six CNN pre-trained networks to detect pterygium. This work consists of two main modules, namely, data acquisition and DCNN classification. The proposed trained network, VggNet16-wbn, shows the best performance with 99.22% accuracy, 98.45% sensitivity, and a perfect score on specificity and area under the curve metrics. This work has high potential for creating a pterygium screening system that can be used as a baseline for fully automated detection using a DL approach.","Ocular imaging,pterygium detection,deep learning,transfer learning",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,"ULTRAVIOLET-RADIATION,PREVALENCE,CLASSIFICATION,PINGUECULA,LIGHT,POPULATION,ALGORITHM,CATARACT,SYSTEM,RISK",IEEE ACCESS,https://doi.org/10.1109/access.2020.3030787,
21,Consecutive Context Perceive Generative Adversarial Networks for Serial Sections Inpainting,8,,190417-190430,"Zhang Siqi,Wang Lei,Zhang Jie,Gu Ling,Jiang Xiran,Zhai Xiaoyue,Sha Xianzheng,Chang Shijie","Zhang SQ,Wang L,Zhang J,Gu L,Jiang XR,Zhai XY,Sha XZ,Chang SJ",Chang SJ,10.1109/ACCESS.2020.3031973,China Medical University,"Image inpainting is a hot topic in computer vision research and has been successfully applied to both traditional and digital mediums, such as oil paintings or old photos mending, image or video denoising and super-resolution. With the introduction of artificial intelligence (AI), a series of algorithms, represented by semantic inpainting, have been developed and better results were achieved. Medical image inpainting, as one of the most demanding applications, needs to meet both the visual effects and strict content correctness. 3D reconstruction of microstructures, based on serial sections, could provide more spatial information and help us understand the physiology or pathophysiology mechanism in histology study, in which extremely high-quality continuous images without any defects are required. In this article, we proposed a novel Consecutive Context Perceive Generative Adversarial Networks (CCPGAN) for serial sections inpainting. Our method can learn semantic information from its neighboring image, and restore the damaged parts of serial sectioning images to maximum extent. Validated with 2 sets of serial sectioning images of mouse kidney, qualitative and quantitative results suggested that our method could robustly restore breakage of any size and location while achieving near realtime performance.","Serial sectioning images,generative adversarial network,consecutive context perceive GAN",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,"3-DIMENSIONAL,RECONSTRUCTION,IMAGE,MOUSE",IEEE ACCESS,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09229053.pdf,
22,Wearable Device-Independent Next Day Activity and Next Night Sleep Prediction for Rehabilitation Populations,8,,,"Fellger Allison,Sprint Gina,Weeks Douglas,Crooks Elena,Cook Diane J.","Fellger A,Sprint G,Weeks D,Crooks E,Cook DJ",Sprint G,10.1109/JTEHM.2020.3014564,Gonzaga University,"Wearable sensor-based devices are increasingly applied in free-living and clinical settings to collect fine-grained, objective data about activity and sleep behavior. The manufacturers of these devices provide proprietary software that labels the sensor data at specified time intervals with activity and sleep information. If the device wearer has a health condition affecting their movement, such as a stroke, these labels and their values can vary greatly from manufacturer to manufacturer. Consequently, generating outcome predictions based on data collected from patients attending inpatient rehabilitation wearing different sensor devices can be challenging, which hampers usefulness of these data for patient care decisions. In this article, we present a data-driven approach to combining datasets collected from different device manufacturers. With the ability to combine datasets, we merge data from three different device manufacturers to form a larger dataset of time series data collected from 44 patients receiving inpatient therapy services. To gain insights into the recovery process, we use this dataset to build models that predict a patient's next day physical activity duration and next night sleep duration. Using our data-driven approach and the combined dataset, we obtained a normalized root mean square error prediction of 9.11% for daytime physical activity and 11.18% for nighttime sleep duration. Our sleep result is comparable to the accuracy we achieved using the manufacturer's sleep labels (12.26%). Our device-independent predictions are suitable for both point-of-care and remote monitoring applications to provide information to clinicians for customizing therapy services and potentially decreasing recovery time.","Medical treatment,Machine learning,Data collection,Switching circuits,Sociology,Statistics,Biomedical monitoring,Actigraphy,activity and sleep prediction,inpatient rehabilitation,machine learning,wearable sensors",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA",Engineering,,3.192,"TRAUMATIC,BRAIN-INJURY,PHYSICAL-ACTIVITY,ACTIVITY,MONITORS,ACTIGRAPHY,VALIDATION,METAANALYSIS,RECOGNITION,VALIDITY,PHASE,WRIST",IEEE JOURNAL OF TRANSLATIONAL ENGINEERING IN HEALTH AND MEDICINE,https://ieeexplore.ieee.org/ielx7/6221039/8943243/09159670.pdf,
23,Hand gesture recognition by using bioacoustic responses,41,2,521-524,"Asakura Takumi,Iida Shizuru","Asakura T,Iida S",Asakura T,10.1250/ast.41.521,Tokyo University of Science,,"Wearable device,Machine learning,Bioacoustics,Gesture recognition,Multi-layer perceptron",Letter,"ACOUSTICAL SOC JAPAN, NAKAURA 5TH-BLDG. 2F, 2-18-20 SOTOKANDA, CHIYODA-KU, TOKYO, 101-0021, JAPAN",Acoustics,,,,ACOUSTICAL SCIENCE AND TECHNOLOGY,https://www.jstage.jst.go.jp/article/ast/41/2/41_E1949/_pdf,
24,A New Real Time Clinical Decision Support System Using Machine Learning for Critical Care Units,8,,185676-185687,"El-Ganainy Noha Ossama,Balasingham Ilangko,Halvorsen Per Steinar,Rosseland Leiv Arne","El-Ganainy NO,Balasingham I,Halvorsen PS,Rosseland LA",El-Ganainy NO,10.1109/ACCESS.2020.3030031,Norwegian University of Science & Technology (NTNU),"Mean arterial pressure (MAP) is an important clinical parameter to evaluate the health of critically ill patients in intensive care units. Thus, the real time clinical decision support systems detecting anomalies and deviations in MAP enable early interventions and prevent serious complications. The state-of-the-art decision support systems are based on a three-phase method that applies offline training, transfer learning, and retraining at the bedside. Their applicability in critical care units is challenging with delay and inaccuracy. In this article, we propose a real time clinical decision support system forecasting the MAP status at the bedside using a new machine learning structure. The proposed system works in real time at the bedside without requiring the offline phase for training using large datasets. It thereby enables timely interventions and improved healthcare services. The proposed machine learning structure includes two stages. Stage I applies online learning using hierarchical temporal memory (HTM) to enable real time stream processing and provides unsupervised predictions. To the best of our knowledge, this is the first time it is applied to medical signals. Stage II is a long short-term memory (LSTM) classifier that forecasts the status of the patients MAP ahead of time based on Stage I stream predictions. We perform a thorough performance evaluation of the proposed system and compare it with the state-of-the-art systems employing logistic regression (LR). The comparison shows the proposed system outperforms LR in terms of the classification accuracy, recall, precision, and area under the receiver operation curve (AUROC).","Real-time systems,Decision support systems,Machine learning,Support vector machines,Radio frequency,Training,Clinical decision support,classification,hierarchical temporal memory (HTM),long short-term memory (LSTM),machine learning,real time prediction",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,"ACUTE,KIDNEY,INJURY",IEEE ACCESS,https://doi.org/10.1109/access.2020.3030031,
25,Decision Boundary Re-Sampling in Imbalanced Learning for Ulcer Detection,8,,186274-186278,"Lee Changhoo,Shin Dongwook,Min Junki,Cha Jaemyung,Lee Seungkyu","Lee C,Shin D,Min J,Cha J,Lee S",Lee S,10.1109/ACCESS.2020.3029259,Kyung Hee University,"Data imbalance problem between normal and lesion endoscopy images makes it difficult to employ deep learning approaches in automatic Ulcer detection and classification. Due to the large variety of normal images in their appearance, characterizing ulcer with limited training samples is not a trivial task. In this work, we propose decision boundary re-sampling (DBR) in imbalanced learning that extrapolates ulcer samples in a latent space of deep convolutional neural network. Proposed method shows improved ulcer classification performance on wireless endoscopy images compared to state-of-the-art methods.","Distributed Bragg reflectors,Training,Lesions,Machine learning,Endoscopes,Training data,Convolutional neural networks,Decision boundary re-sampling,convolutional neural network,ulcer classification",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,"POLYP,DETECTION,SMOTE",IEEE ACCESS,https://ieeexplore.ieee.org/ielx7/6287639/8948470/09216051.pdf,
26,Breast Cancer Classification Using Deep Learning Approaches and Histopathology Image: A Comparison Study,8,,187531-187552,"Shahidi Faezehsadat,Mohd Daud Salwani,Abas Hafiza,Ahmad Noor Azurati,Maarop Nurazean","Shahidi F,Daud SM,Abas H,Ahmad NA,Maarop N",Daud SM,10.1109/ACCESS.2020.3029881,Universiti Teknologi Malaysia,"Convolutional Neural Network (CNN) models are a type of deep learning architecture introduced to achieve the correct classification of breast cancer. This paper has a two-fold purpose. The first aim is to investigate the various deep learning models in classifying breast cancer histopathology images. This study identified the most accurate models in terms of the binary, four, and eight classifications of breast cancer histopathology image databases. The different accuracy scores obtained for the deep learning models on the same database showed that other factors such as pre-processing, data augmentation, and transfer learning methods can impact the ability of the models to achieve higher accuracy. The second purpose of our manuscript is to investigate the latest models that have no or limited examination done in previous studies. The models like ResNeXt, Dual Path Net, SENet, and NASNet had been identified with the most cutting-edge results for the ImageNet database. These models were examined for the binary, and eight classifications on BreakHis, a breast cancer histopathology image database. Furthermore, the BACH database was used to investigate these models for four classifications. Then, these models were compared with the previous studies to find and propose the most state-of-the-art models for each classification. Since the Inception-ResNet-V2 architecture achieved the best results for binary and eight classifications, we have examined this model in our study as well to provide a better comparison result. In short, this paper provides an extensive evaluation and discussion about the experimental settings for each study that had been conducted on the breast cancer histopathology images.","Breast cancer,Databases,Histopathology,Deep learning,Image color analysis,Breast cancer,histopathology medical images,deep learning,transfer learning,data augmentation,pre-processing,classification",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,"DUCTAL,CARCINOMA,DIAGNOSIS",IEEE ACCESS,https://ieeexplore.ieee.org/ielx7/6287639/8948470/09218931.pdf,
27,Data Acquisition Devices Towards a System for Monitoring Sensory Processing Disorders,8,,183596-183605,"Vicente-Samper Jose Maria,Avila-Navarro Ernesto,Sabater-Navarro Jose Maria","Vicente-Samper JM,Avila-Navarro E,Sabater-Navarro JM",Vicente-Samper JM,10.1109/ACCESS.2020.3029692,Universidad Miguel Hernandez de Elche,"People with autism spectrum disorder (ASD) manifest great heterogeneity in their atypical sensory behaviors. It is estimated that 95% of people with ASD have a Sensory Process Disorder (SPD). People with ASD feel the need to control what happens in their environment. However, it is inevitable that new situations occur during a persons daily life. Therefore, it is important to monitor most of the circumstances they face in an attempt to predict the appearance of disorders that end up affecting their behavior. This paper presents the first steps towards the development of a system for knowing the value and effect on the SPD of different biological and environmental parameters. To obtain those variables, two electronic devices have been designed. The first one is an electronic system for capturing environmental variables such as luminosity or humidity, which is portable and mobile. The second electronic device is a soft wearable wristband which gets biological parameters. To know the effect of those variables on the SPD, a complete software platform has been implemented. Both devices upload day-to-day data to a cloud database where the information is stored in timeseries data of different parameters. The system uses the data to learn a personalized model that is designed to manage the SPD of the user. The main novelty is the use of sensor integration, data processing and machine learning techniques to develop a system able to classify the sensory load supported by a user with ASD while performing different activities. The results obtained so far prove the feasibility of the approach.","Monitoring,Biology,Data acquisition,Biomedical monitoring,Software,Atmospheric measurements,Temperature measurement,Autism spectrum disorder,Bluetooth low energy,machine learning,PPG,sensory processing disorder,wearable devices",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,"AUTISM,SPECTRUM,DISORDERS,CHILDREN,PREVALENCE",IEEE ACCESS,https://ieeexplore.ieee.org/ielx7/6287639/8948470/09217592.pdf,
28,Multi-Layer Basis Pursuit for Compressed Sensing MR Image Reconstruction,8,,186222-186232,"Wahid Abdul,Shah Jawad Ali,Khan Adnan Umar,Ahmed Manzoor,Razali Hanif","Wahid A,Shah JA,Khan AU,Ahmed M,Razali H",Shah JA,10.1109/ACCESS.2020.3028877,"UniKL British Malaysian Inst, Elect Sect, Kuala Lumpur 53100, Malaysia.","Compressive Sensing (CS) is a widely used technique in biomedical signal acquisition and reconstruction. The technique is especially useful for reducing acquisition time for magnetic resonance imaging (MRI) signal acquisitions and reconstruction, where effects of patient fatigue and Claustrophobia need mitigation. In addition to improving patient experience, faster MRI scans are important for time sensitive imaging, such as functional or cardiac MRI, where target movement is unavoidable. Inspired from recent research works on multi-layer convolutional sparse coding (ML-CSC) theory to model deep neural networks, this work proposes a multi-layer basis pursuit framework which combines the benefit from objective-based CS reconstructions and deep learning-based reconstruction by employing iterative thresholding algorithms for successfully training a CS-MRI restoration framework on GPU and reconstruct test images using parameters of the trained model. Extensive experiments show the effectiveness of the proposed framework on four MRI datasets in terms of faster convergence, improved PSNR/SSIM, and better restoration efficiency as compared to the state of the art frameworks with different CS ratios.","Image reconstruction,Magnetic resonance imaging,Compressed sensing,Machine learning,Iterative algorithms,Training,Compressive sensing,inverse problems in imaging,iterative thresholding algorithms,multi-layered convolutional sparse coding",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,,"GENERATIVE,ADVERSARIAL,NETWORKS,THRESHOLDING,ALGORITHM,INVERSE,PROBLEMS,CNN",IEEE ACCESS,https://doi.org/10.1109/access.2020.3028877,
29,A Concrete Dam Deformation Prediction Method Based on LSTM With Attention Mechanism,8,,185177-185186,"Yang Dashan,Gu Chongshi,Zhu Yantao,Dai Bo,Zhang Kang,Zhang Zhiduan,Li Bo","Yang DS,Gu CS,Zhu YT,Dai B,Zhang K,Zhang ZD,Li B",Gu CS; Zhu YT,10.1109/ACCESS.2020.3029562,Hohai University,"Dams are the main water retaining structures in the hydraulic engineering field. Safe operations of dams are important foundations to ensure the hydraulic functionalities of these engineering structures. Deformation, as the most intuitive feature of the dams' operation behaviors, can comprehensively reflect the dam structural states. In this case, the analysis of the dam prototype deformation data and the establishment of a real-time prediction model become frontier research contents in the field of dam safety monitoring. Considering the multi-nonlinear relationships between dam deformation and relative influential factors as well as the time lag effect of these influential factors, this article adopts long-short-term memory (LSTM) network algorithm in deep learning to deal with the long-term dependence existing in dam deformation and explore the deformation law. The method proposed in this work can effectively avoid the gradient disappearance and gradient explosion problems by using the recurrent neural network (RNN). In addition, this work adopts the Attention mechanism to screen the information that has significant influence on deformation, combining the Adam optimization algorithm that has high calculation efficiency and low memory requirement to improves the learning accuracy and speed of the LSTM. The model overfitting is avoided by applying the Dropout mechanism. The effectiveness of this proposed model in studing the long time series deformation prediction of concrete dams is confirmed by case studies, whose MSE (mean square error) and other 4 error indexes can be reduced.","Deformation prediction,LSTM network,Adam optimization,Attention mechanism,Dropout mechanism",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,,IEEE ACCESS,https://ieeexplore.ieee.org/ielx7/6287639/8948470/09217601.pdf,
30,Deep Longitudinal Feature Representations for Detection of Postradiotherapy Brain Injury at Presymptomatic Stage,8,,184710-184721,"Zhong Liming,Zhang Xiao,Xi Yuhua,Lian Zhouyang,Feng Qianjin,Chen Wufan,Zhang Shuixing,Yang Wei","Zhong LM,Zhang X,Xi YH,Lian ZY,Feng QJ,Chen WF,Zhang SX,Yang W",Yang W,10.1109/ACCESS.2020.3030060,Southern Medical University - China,"Temporal lobe injury (TLI), a form of nervous system damage in the brain, is a major neurological complication after radiation therapy (RT). TLI must be highly valued because of the irreversible brain injury. This article aims to develop a predictive pipeline, called deep longitudinal feature representations (DLFR), to detect TLI at the presymptomatic stage accurately via the learning of effective deep longitudinal feature representations. DLFR characterizes high-level information and developmental changes within and across subjects The DLFR consists of four components: (i) extraction of deep features from a pretrained ResNet50 model; (ii) compression of learned highly representative features by the global max pooling; (iii) fusion of deep longitudinal features for the fully use of all follow-up data; (iv) random forest-based prediction of the diagnostic status. In total, 244 nasopharyngeal carcinoma patients before and after RT with a follow-up period of 0 similar to 9 years were included for analysis. All patients were divided into four different latency groups, and the current latency was used for training to predict the diagnostic status of the next latency. The AUCs of the predicted three different latency groups using DLFR were 0.64 +/- 0.11, 0.76 +/- 0.10, and 0.88 +/- 0.05, while those of radiomics features were 0.56 +/- 0.06, 0.63 +/- 0.03, and 0.53 +/- 0.04, and those of histogram of oriented gradients features were 0.60 +/- 0.09, 0.52 +/- 0.03, and 0.58 +/- 0.06. Most importantly, the AUCs of the predicted three different latency groups for white matter regions were 0.66 +/- 0.10, 0.80 +/- 0.09, and 0.78 +/- 0.09. Our proposed method can dynamically detect TLI at the presymptomatic stage, which can enable the administration of preventive neurological intervention.","Temporal lobe injury,nasopharyngeal carcinoma,deep longitudinal features,white matter",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,"MODULATED,RADIATION-THERAPY,VOXEL-BASED,MORPHOMETRY,NASOPHARYNGEAL,CARCINOMA,WHITE-MATTER,CLASSIFICATION,MRI,RADIOTHERAPY,PATTERNS,NECROSIS,IMAGES",IEEE ACCESS,https://doi.org/10.1109/access.2020.3030060,
31,Brain Decoding of Viewed Image Categories via Semi-Supervised Multi-View Bayesian Generative Model,68,,5769-5781,"Akamatsu Yusuke,Harakawa Ryosuke,Ogawa Takahiro,Haseyama Miki","Akamatsu Y,Harakawa R,Ogawa T,Haseyama M",Akamatsu Y,10.1109/TSP.2020.3028701,Hokkaido University,"Brain decoding has shown that viewed image categories can be estimated from evoked functional magnetic resonance imaging (fMRI) activity. Recent studies attempted to estimate viewed image categories that were not used for training previously. Nevertheless, the estimation performance is limited since it is difficult to collect a large amount of fMRI data for training. This paper presents a method to accurately estimate viewed image categories not used for training via a semi-supervised multi-view Bayesian generative model. Our model focuses on the relationship between fMRI activity and multiple modalities, i.e., visual features extracted from viewed images and semantic features obtained from viewed image categories. Furthermore, in order to accurately estimate image categories not used for training, our semi-supervised framework incorporates visual and semantic features obtained from additional image categories in addition to image categories of training data. The estimation performance of the proposed model outperforms existing state-of-the-art models in the brain decoding field and achieves more than 95% identification accuracy. The results also have shown that the incorporation of additional image category information is remarkably effective when the number of training samples is small. Our semi-supervised framework is significant for the brain decoding field where brain activity patterns are insufficient but visual stimuli are sufficient.","Brain decoding,functional magnetic resonance imaging (fMRI),generative model,bayesian inference,multi-view learning,semi-supervised learning",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA",Engineering,,5.239,"NATURAL,IMAGES,FMRI,ACTIVITY,RECONSTRUCTION",IEEE TRANSACTIONS ON SIGNAL PROCESSING,,
32,Multi-level Binarized LSTM in EEG Classification for Wearable Devices,,,175-181,"Nazari Najmeh,Mirsalari Seyed Ahmad,Sinaei Sima,Salehi Mostafa E.,Daneshtalab Masoud","Nazari N,Mirsalari SA,Sinaei S,Salehi ME,Daneshtalab M",Nazari N,10.1109/PDP50117.2020.00033,University of Tehran,"Long Short-Term Memory (LSTM) is widely used in various sequential applications. Complex LSTMs could be hardly deployed on wearable and resourced-limited devices due to the huge amount of computations and memory requirements. Binary LSTMs are introduced to cope with this problem, however, they lead to significant accuracy loss in some applications such as EEG classification which is essential to be deployed in wearable devices. In this paper, we propose an efficient multi-level binarized LSTM which has significantly reduced computations whereas ensuring an accuracy pretty close to full precision LSTM. By deploying 5-level binarized weights and inputs, our method reduces area and delay of MAC operation about 31x and 27x in 65nm technology, respectively with less than 0.01% accuracy loss. In contrast to many compute-intensive deep-learning approaches, the proposed algorithm is lightweight, and therefore, brings performance efficiency with accurate LSTM-based EEG classification to real-time wearable devices.","Long Short-Term Memory (LSTM),Binarization,Embedded Systems",Proceedings Paper,"IEEE COMPUTER SOC, 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA",Computer Science,,,,,http://arxiv.org/pdf/2004.11206,
33,Reduction of Systematic Defects With Machine Learning from Design to Fab,11329,,,"Ma Yuansheng,Hong Le,Word James,Jiang Fan,Liubich Vlad,Cao Liang,Jayaram Srividya,Kwak Doohwan,Kim YoungChang);,Fenger Germain","Ma YS,Hong L,Word J,Jiang F,Liubich V,Cao L,Jayaram S,Kwak D,Kim Y,Fenger G",Ma YS,10.1117/12.2551703,Mentor Graphics Inc,"Maximizing yield in a modern semiconductor fab requires proper optimization of the design (layout), process technology, and fab process tool recipes. For the past decade the prevalence of systematic defects tied to design or design-process interactions have predominated over random defect sources. Previously Resolution Enhancement Technology (RET), Design For Manufacturability (DFM), and Design-Technology Co-optimization (DTCO) techniques were the successful response to eliminating systematic yield limiting patterns. Machine learning, with its ability to find trends and make predictions based on large volumes of data, provides a unique path towards further reduction in systematic defect levels.
This talk will present methods based on the use of design and process info with machine learning and computational lithography methods to identify and eliminate yield limiting patterns in the design, improve the accuracy of mask generation with etch and resist modeling and OPC, and improve the productivity and accuracy of fab defect detection and diagnostics.
This paper will present methods to improve EPE control and reduce systematic hotspots through both supervised and unsupervised machine learning. Specifically we will focus on 3 areas:
identifying and yield limiting patterns in the design phase.
improving the accuracy (EPE control) of mask generation with machine learning assisted etch and resist modeling and OPC.
improving the productivity and accuracy of fab defect detection and diagnostics with machine learning.","hotspots,yield,machine learning,artificial intelligence,EPE",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Science & Technology - Other Topics,Optics",,,,,,
34,Optimal etch recipe prediction for 3D NAND structures,11329,,,"Medina Leandro,Sundahl Bryan,Chopra Meghali C.,Bonnecaze Roger T.","Medina L,Sundahl B,Chopra MC,Bonnecaze RT",Chopra MC,10.1117/12.2552121,"SandBox Semicond Inc, Austin, TX 78704 USA.","We present a model-based experimental design methodology for accelerating 3D etch optimization with demonstration on 3D NAND structures. The design and optimization of etch recipes for such 3D structures face significant challenges requiring costly and time-consuming experiments in order to achieve the required tolerances. 3D NAND memory devices additionally require accurate nanofabrication of high aspect ratio trenches and isolation slits, which are challenging to manufacture reliably within specifications. Our model efficiently captures the relevant physical and chemical processes, which allows them to be calibrated using a limited number of experimental samples and can reproduce realistic 3D etch of multilayer materials, including bowing, necking, and tapering. Since our GPU-powered simulations run in a matter of minutes, the relevant process parameter space can be explored extensively in a short amount of time. The calibrated physics-based model can be used to train adaptive machine-learning- based heuristics which enable near-instant queries, for example for data visualization and analytics. With this approach, we show a rapid methodology for locating optimal windows in the process parameter space for etching 3D structures. Optimality metrics under consideration include both conformances to specified tolerances as well as robustness against process parameter variations. These techniques can reduce cost and time to market for complex multi-layer three-dimensional device designs and improve semiconductor device yields.","Etch,etch recipe creation,3D,3D NAND,experimental design",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Science & Technology - Other Topics,Optics",,,,,,
35,Machine Learning Methods in Assessing the Risks of Target Organ Damage in Masked Hypertension,60,5,107-114,"Geltser B. I.,Shakhgeldyan K. I.,Nazarov D. A.,Vetrova O. V.,Kotelnikov V. N.,Karpov R. S.","Geltser BI,Shakhgeldyan KI,Nazarov DA,Vetrova OV,Kotelnikov VN,Karpov RS",Geltser BI,10.18087/cardio.2020.5.n883,Far Eastern Federal University,,ASSOCIATION,Article,"RUSSIAN HEART FAILURE SOC, 215, 5, BEREGOVOY PROEZD, MOSCOW, 121087, RUSSIA",Cardiovascular System & Cardiology,,0.317,ASSOCIATION,KARDIOLOGIYA,https://lib.ossn.ru/jour/article/download/883/701,
36,Weakly-Supervised Lesion Segmentation on CT Scans using Co-Segmentation,11314,,,"Agarwal Vatsal,Tang Youbao,Xiao Jing,Summers Ronald M.","Agarwal V,Tang YB,Xiao J,Summers RM",Agarwal V,10.1117/12.2551106,National Institutes of Health (NIH) - USA,"Lesion segmentation on computed tomography (CT) scans is an important step for precisely monitoring changes in lesion/tumor growth. This task, however, is very challenging since manual segmentation is prohibitively time-consuming, expensive, and requires professional knowledge. Current practices rely on an imprecise substitute called response evaluation criteria in solid tumors (RECIST). Although these markers lack detailed information about the lesion regions, they are commonly found in hospitals' picture archiving and communication systems (PACS). Thus, these markers have the potential to serve as a powerful source of weak-supervision for 2D lesion segmentation. To approach this problem, this paper proposes a convolutional neural network (CNN) based weakly-supervised lesion segmentation method, which first generates the initial lesion masks from the RECIST measurements and then utilizes co-segmentation to leverage lesion similarities and refine the initial masks. In this work, an attention-based co-segmentation model is adopted due to its ability to learn more discriminative features from a pair of images. Experimental results on the NIH DeepLesion dataset demonstrate that the proposed co-segmentation approach significantly improves lesion segmentation performance, e.g the Dice score increases about 4.0% (from 85.8% to 89.8%).","Weakly-supervised lesion segmentation,segmentation,co-segmentation,attention,CT scans",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,http://arxiv.org/pdf/2001.08590,
37,DCGANs for Realistic Breast Mass Augmentation in X-ray Mammography,11314,,,"Alyafi Basel,Diaz Oliver,Marti Robert","Alyafi B,Diaz O,Marti R",Alyafi B,10.1117/12.2543506,Universitat de Girona,"Early detection has a major contribution to the curability of breast cancer, and using mammographic images, this can be achieved non-invasively. Supervised deep learning, the dominant computer-aided detection (CADe) tool currently, has played a great role in object detection in computer vision, but it suffers from a limiting property: the need of a large amount of labelled data. This becomes stricter when it comes to medical datasets which require high-cost and time-consuming annotations. Furthermore, medical datasets are usually imbalanced, a condition that often hinders classifiers performance. The aim of this paper is to learn the distribution of the minority class to synthesise new samples in order to improve lesion detection in mammography. Deep Convolutional Generative Adversarial Networks (DCGANs) can efficiently generate breast masses. They were trained on increasing-size subsets of a mammographic dataset and used to generate diverse and realistic breast masses. The effect of including the generated images and/or applying horizontal and vertical flipping was tested in an environment where an imbalanced dataset (ratio of 1:10) of masses and normal tissue patches was classified using a fully-convolutional network. A maximum of 0.09 improvement of F1 score was reported by using DCGANs along with flipping augmentation in contrast to using the original images. We show that DCGANs can be used for synthesising photo-realistic breast mass patches with a considerable diversity. It is demonstrated that appending synthetic images in this environment, along with flipping, outperforms the traditional augmentation method of flipping solely, offering faster improvements as a function of the training set size.","breast cancer,computer-aided detection,deep learning,deep convolutional generative adversarial networks,fully-convolutional networks,data augmentation",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,http://arxiv.org/pdf/1909.02062,
38,Combining Symmetric and Standard Deep Convolutional Representations for Detecting Brain Hemorrhage,11314,,,"Barman Arko,Lopez-Rivera Victor,Lee Songmi,Vahidy Farhaan S.,Fan James Z.,Savitz Sean I.,Sheth Sunil A.,Giancardo Luca","Barman A,Lopez-Rivera V,Lee S,Vahidy FS,Fan JZ,Savitz SI,Sheth SA,Giancardo L",Barman A,10.1117/12.2549384,University of Texas System,"Brain hemorrhage (BH) is a severe type of stroke resulting in high mortality and morbidity. Detection and diagnosis of BH is commonly performed using neuroimaging tools such as Computed Tomography (CT). We compare and contrast symmetry-aware, symmetry-naive feature representations and their combination for the detection of BH using CT imaging. One of the proposed architectures, e-DeepSymNet, achieves AUC 0.99 [0.97-1.00] for BH detection. An analysis of the activation values shows that both symmetry-aware and symmetry-naive representations offer complementary information with symmetry-aware representation naive contributing 20% towards the final predictions.","brain hemorrhage,computer-aided detection,deep symmetry-sensitive networks,precision medicine,machine learning,deep learning,quantitative imaging",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
39,Spatio-spectral deep learning methods for in-vivo hyperspectral laryngeal cancer detection,11314,,,"Bengs Marcel,Westermann Stephan,Gessert Nils,Eggert Dennis,Gerstner Andreas O. H.,Mueller Nina A.,Betz Christian,Laffers Wiebke,Schlaefer Alexander","Bengs M,Westermann S,Gessert N,Eggert D,Gerstner AOH,Mueller NA,Betz C,Laffers W,Schlaefer A",Bengs M,10.1117/12.2549251,Hamburg University of Technology,"Early detection of head and neck tumors is crucial for patient survival. Often, diagnoses are made based on endoscopic examination of the larynx followed by biopsy and histological analysis, leading to a high interobserver variability due to subjective assessment. In this regard, early non-invasive diagnostics independent of the clinician would be a valuable tool. A recent study has shown that hyperspectral imaging (HSI) can be used for non-invasive detection of head and neck tumors, as precancerous or cancerous lesions show specific spectral signatures that distinguish them from healthy tissue. However, HSI data processing is challenging due to high spectral variations, various image interferences, and the high dimensionality of the data. Therefore, performance of automatic HSI analysis has been limited and so far, mostly ex-vivo studies have been presented with deep learning. In this work, we analyze deep learning techniques for in-vivo hyperspectral laryngeal cancer detection. For this purpose we design and evaluate convolutional neural networks (CNNs) with 2D spatial or 3D spatiospectral convolutions combined with a state-of-the-art Densenet architecture. For evaluation, we use an in-vivo data set with HSI of the oral cavity or oropharynx. Overall, we present multiple deep learning techniques for in-vivo laryngeal cancer detection based on HSI and we show that jointly learning from the spatial and spectral domain improves classification accuracy notably. Our 3D spatio-spectral Densenet achieves an average accuracy of 81%.","Hyperspectral imaging,convolutional neural networks,optical biopsy,intraoperative imaging,head and neck cancer",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,"PREDICTORS,LESIONS",,http://arxiv.org/pdf/2004.10159,
40,Predicting treatment outcome of intracranial aneurysms using angiographic parametric imaging and recurrent neural networks,11314,,,"Bhurwani Mohammad Mahdi Shiraz,Waqas Muhammad,Williams Kyle A.,Rava Ryan A.,Podgorsak Alexander R.,Snyder Kenneth V.,Levy Elad I.,Davies Jason M.,Siddiqui Adnan H.,Ionita Ciprian N.","Bhurwani MMS,Waqas M,Williams KA,Rava RA,Podgorsak AR,Snyder KV,Levy EI,Davies JM,Siddiqui AH,Ionita CN",Bhurwani MMS,10.1117/12.2548635,State University of New York (SUNY) System,"Angiographic Parametric Imaging (API) is a tool based on the parametrization of Time-Density Curves (TDCs) from Digital Subtraction Angiography (DSA). Parameters derived from the TDCs correlate moderately with hemodynamics, yet underuse the hemodynamic information encoded in a TDC. To determine whether better diagnoses can be made through a more complete utilization of the information in the TDCs, we implemented an analysis using Recurrent Neural Networks (RNNs). These are a class of neural networks that analyze and make predictions using time sequences such as the TDCs. We investigated the feasibility of using RNNs to make treatment outcome predictions using TDCs obtained from angiograms of Intracranial Aneurysms (IAs) treated with Pipeline Embolization Devices (PED). Six-month follow-up angiograms were collected to create binary labels regarding treatment outcome (occluded/un-occluded). API parameters obtained were Mean Transit Time, Time to Peak, Time to Arrival, and Peak Height. Parameters were used to simulate TDCs which were normalized to account for variability between interventions. An RNN was trained and tested to predict IA treatment outcome. A 20-fold Monte Carlo Cross Validation was conducted to evaluate robustness of the RNN. The RNN predicted occlusion outcome of IAs with an average accuracy of 74.4% (95% CI, 72.6%-76.1%) and 65.6% (63.4%-67.2%) and average area under the receiver operating characteristic curve of 0.73 (0.70-0.76) and 0.56 (0.51-0.61) for normalized and un-normalized sub-groups respectively. This study proves the feasibility of using RNNs to predict treatment outcome of IAs treated with a PED using TDCs simulated from temporal features obtained through API.","Recurrent neural network,machine learning,angiographic parametric imaging,intracranial aneurysms",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,"ASYMMETRIC,VASCULAR,STENT,PIPELINE,EMBOLIZATION",,,
41,A Multidimensional Scaling and Sample Clustering to Obtain a Representative Subset of Training Data for Transfer Learning-based Rosacea Lesion Identification,11314,,,"Binol Hamidullah,Niazi M. Khalid Khan,Plotner Alisha,Sopkovich Jennifer,Kaffenberger Benjamin H.,Gurcan Metin N.","Binol H,Niazi MKK,Plotner A,Sopkovich J,Kaffenberger BH,Gurcan MN",Binol H,10.1117/12.2549392,Wake Forest University,"Rosacea is a common cutaneous disorder characterized by facial redness, swelling, and flushing, and it is usually diagnosed by a dermatologist after a visual examination. Qualitative human assessment often results in relatively high intra- and inter-observer variability, which can negatively affect patient outcomes. Computer-assisted image analysis may improve visual assessment by human observers because it enables quantitative, consistent, and accurate analysis. Here, we combine classical multidimensional scaling (MDS) with deep convolutional neural networks (CNNs) to create an efficient framework to identify rosacea lesions. MDS is utilized to determine an appropriate amount of training data, which are used to train Inception-ResNet-v2 to classify facial images into rosacea and non-rosacea regions. Using a leave-one-patient-out cross-validation scheme with 128 x 128 non-overlapping image patches, the method resulted in a class weighted average Dice coefficient (DC) of 82.1% +/- 2.4% and accuracy of 85.0% +/- 0.6%. While this average performance is almost identical to our previous results (81.7% +/- 2.7% and 84.9% +/- 0.6% for DC and accuracy, respectively), with the new scheme, we use approximately 90% less data to train the system. We also report the results of quantitative experiments with overlapping patches with a stride of 50 pixels. With the same experimental setup, speedups of 25.6 times (128 x 128), 23.4 times (192 X 192), and 23.2 times (256 x 256) have been observed when the network is trained with the entire training data as the baseline. The class weighted average DC for this experiment with the proposed method is 83.9% +/- 2.1% as in the case of 192 x 192 pixels overlapping patches, while it is 84.4% +/- 2.2% when the entire set is trained at each fold. We conclude that the proposed method can be an efficient way to train deep neural networks using only a small subset of the training data.","Classification,Clustering,Computer-assisted Diagnosis,Convolutional Neural Networks,Deep Learning,Facial Disorders,Multidimensional Scaling,Rosacea,Transfer Learning",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,"CLASSIFICATION,TRENDS",,,
42,Deformation Robust Texture Features for Polyp Classification via CT Colonography,11314,,,"Cao Weiguo,Pomeroy Marc J.,Zhang Shu,Pickhardt Perry J.,Lu Hongbing,Liang Zhengrong","Cao WG,Pomeroy MJ,Zhang S,Pickhardt PJ,Lu HB,Liang ZR",Liang ZR,10.1117/12.2550404,State University of New York (SUNY) System,"In this article, we introduce a deformation independent model to solve the shape and posture changing issue for polyp characterization in computer-aided diagnosis (CADx) via CT colonography. After volumetric data parameterization in a four-dimensional space, the first fundamental form (FFF) is employed to construct the polyp model which contains several excellent properties such as locality, symmetry, orientation robustness, shift and isometric invariance. In consideration of the scaling effects, gray level co-occurrence matrix (GLCM) is utilized to remove the scaling factor and extract texture descriptors. As a symmetrical square tensor, however, it is difficult to put the FFF into GLCM directly. To solve this problem, we perform matrix decomposition on FFF to extract its eigenvalues and eigenvectors which are used to construct three metric images as the input of GLCM. Then Haralick measures extracted from GLCM are applied to construct texture descriptors which are fed to a random forest classifier to perform polyp classification. Experiments show that the proposed method obtains an encouraging classification performance with area under the curve of receiver operating characteristics (AUC score) of 95.3% which is a significant improvement comparing with five existing methods.","Computer-aided detection and diagnosis,colon polyp,texture descriptor,machine learning,classification,feature selection",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
43,Automated Detection and Segmentation of Mediastinal and Axillary Lymph Nodes from CT Using Foveal Fully Convolutional Networks,11314,,,"Carolus Heike,Iuga Andra-Iza,Brosch Tom,Wiemker Rafael,Thiele Frank,Hoeink Anna,Maintz David,Puesken Michael,Klinder Tobias","Carolus H,Iuga AI,Brosch T,Wiemker R,Thiele F,Hoink A,Maintz D,Pusken M,Klinder T",Carolus H,10.1117/12.2549246,Philips,"The assessment of lymph nodes in CT examinations of cancer patients is essential for cancer staging with direct impact on therapeutic decisions. Automated detection and segmentation of lymph nodes is challenging, especially, due to significant variability in size, shape and location coupled with weak and variable image contrast. In this paper, we propose a joint detection and segmentation approach using a fully convolutional neural network based on 3D foveal patches. To enable network training, 89 publicly available CT data sets were carefully re-annotated yielding an extensive set of 4351 voxel-wise segmentations of thoracic lymph nodes. Based on these annotations, the 3D network was trained to perform per voxel classification. For enlarged potentially malignant lymph nodes, a detection rate of 79 % with 8.0 false-positive detections per volume was obtained. A DICE of 0.44 was achieved on average.","Lymph nodes,Segmentation,Convolutional Neural Networks,Deep Learning",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
44,Genetic Algorithm for Machine Learning Architecture Selection for Breast MRI Classification,11314,,,"Carras Peter S.,Pereira Carina,Biswas Debosmita,Lee Christoph,Partridge Savannah,Alessio Adam M.","Carras PS,Pereira C,Biswas D,Lee C,Partridge S,Alessio AM",Carras PS,10.1117/12.2547490,Michigan State University,"Convolutional neural networks (CNN) are increasingly used for image classification tasks. In general, the architectures of these networks are set ad hoc with little justification for selecting various components, such as the number of layers, layer depth, and convolution settings. In this work, we develop a stmctured approach to explore and select architectures that provide optimal classification performance. This was developed with an IRB-approved data set with 9,216 2-D maximum intensity projection (MIP) MRI breast images, containing breast cancer malignant or benign classes. This data set was divided into 7,372 training, 922 validation, and 922 test images The architecture search method employs a genetic algorithm to find optimal ResNet-based classification architectures through crossover and mutation. Each generation, new model architectures are created from the genetic algorithm and trained with supervised machine learning. This search method identifies the model with the highest area under the ROC curve (AUC). The genetic algorithm produced an optimal model architecture which classifies malignancy in images with 76% accuracy and achieves an AUC score of .83. This approach offers a rational framework for automatic architecture exploration, potentially leading to a set of more efficient and generalizable CNN-based classifiers.","CNN,convolutional,neural,networks,classification,MRI,genetic algorithm",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
45,Convolutional Neural Network-Based Decision Support System for Bladder Cancer Staging in CT Urography: Decision Threshold Estimation and Validation,11314,,,"Chapman-Sung Daniel Hoklai,Hadjiiski Lubomir,Gandikota Dhanuj,Chan Heang-Ping,Samala Ravi,Caoili Elaine M.,Cohan Richard H.,Weizer Alon,Alva Ajjai,Zhou Chuan","Chapman-Sung DH,Hadjiiski L,Gandikota D,Chan HP,Samala R,Caoili EM,Cohan RH,Weizer A,Alva A,Zhou C",Chapman-Sung DH,10.1117/12.2551309,University of Michigan System,"Stage T2 is the clinical threshold to administer neoadjuvant chemotherapy for bladder cancer. In this study a deep learning convolutional neural network (DL-CNN) was trained to aid clinicians in staging of bladder cancer in CT Urography (CTU). The DL-CNN utilized two datasets for training and testing. The primary training dataset included 84 bladder cancers from CTU scans of 76 clinically staged patients, 43 cancers were below stage T2, and 41 were stage T2 or above. The second dataset served as an independent test set containing 90 bladder cancers from CTU scans of 86 clinically staged patients, all bladder cancers were staged as T2 or above. Regions of interest (ROIs) were extracted from the lesions as input to the DL-CNN. The model structure and hyper-parameters were determined and asserted using the training dataset of 84 lesions split into two balanced partitions. Based on the lesion-based T2 likelihood score obtained by averaging the scores of all ROIs extracted from a given lesion, the decision threshold providing the highest classification accuracy was determined from the leave-one-out validation. The DL-CNN with the fixed decision threshold was then applied to the test ROIs. The classification accuracy for the independent test set of 90 cancers was 0.91. This performance is slightly higher than our previous radiomics approach based on SVM and BPNN models, which achieved 0.88 and 0.90 accuracy on the same test set, respectively, but the difference was not statistically significant. The results show the promise of using a DL-CNN in bladder cancer stage assessment.","Bladder Cancer Staging,Radiomics,Classification,Segmentation,Convolutional Neural Network,CNN",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,"CHEMOTHERAPY,NEOADJUVANT",,,
46,Hyperparameter selection for ResNet classification of malignancy from thyroid ultrasound images,11314,,,"Cox Joseph,Rubin Sydney,Adams Joe,Pereira Carina,Dighe Manjiri,Alessio Adam","Cox J,Rubin S,Adams J,Pereira C,Dighe M,Alessio A",Alessio A,10.1117/12.2550531,Michigan State University,"Thyroid nodules are extremely common, with a prevalence of up to 68% in adults. Ultrasound imaging is usually performed to detect and evaluate thyroid nodules for malignancy. Many patients undergo follow-up biopsy in the form of fine-needle aspiration (FNA) to determine if a nodule is malignant or benign, although most nodules are benign. In order to reduce the number of unnecessary FNAs, radiologists will often use classification systems such as Thyroid Imaging, Reporting, and Data System (TI-RADS) to provide risk stratification and a recommendation regarding whether FNA is necessary. This scoring is both subjective and time-consuming, leading to discrepancies between radiologists and recommendations that can be inaccurate. We hypothesize that a machine learned classifier can be identified with accurate and generalizable performance, potentially offering more consistent results than manual evaluation. We created a network from two ResNet-50 branches accepting two inputs, shear-wave elastography and B-mode ultrasound images. We performed a grid search to determine the optimal hyperparameters for our model, resulting in a network that predicted malignancy of nodules with 88.7% accuracy and an AUC of 0.91. Along with identifying the training hyperparameters with optimal classification accuracy, the grid search also allowed us to select training parameters that led to more generalizable model performance on test data sets. These initial performance results suggest that our model offers a promising strategy for thyroid nodule classification and a strategy to help identify more generalizable models.","Classification,ResNet,Thyroid Ultrasound,Hyperparameter Selection",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,"NODULES,SYSTEM",,,
47,Deep Learning for Pneumothorax Detection and Localization Using Networks Fine-Tuned with Multiple Institutional Datasets,11314,,,"Crosby Jennie,Rhines Thomas,Li Feng,MacMahon Heber,Giger Maryellen","Crosby J,Rhines T,Li F,MacMahon H,Giger M",Crosby J,10.1117/12.2549709,University of Chicago,"Pneumothorax, presenting as a fine line at the edge of the lung and a change in texture outside the lung, is a particularly difficult condition to detect on chest radiographs due to its wide range of sizes and subtle visual signs. Deep learning methods can be applied to chest radiographs to assist in the detection and localization of pneumothorax. The visual signs of pneumothorax are usually unable to be seen at typical neural network input sizes (256 x 256 or 224 x 224); therefore, increasing the resolution of the input images is expected to be beneficial for deep learning detection of pneumothorax. In this work, chest radiographs were separated into two apex images (top third of lung) and then 256 x 256 patches were extracted from the apex images. VGG19 neural networks were fine-tuned for the task of distinguishing between images with and without pneumothorax. One network was fine-tuned with the apex images (downsized to 256 x 256) and another fine-tuned with 256 x 256 patches within the apex images. These fine-tuned networks were tested on an independent test set and ROC analysis performed. The apex-based network yielded an AUC of 0.80 (95% confidence interval (CI): 0.79, 0.81) and the patch-based network yielded an AUC of 0.73 (95% CI: 0.71, 0.74) in the task of distinguishing between images with and without pneumothorax. When the outputs from the two networks were merged via soft voting, a statistically significant increase in performance was observed as compared to either network alone (AUC=0.83, p<0.001).","pneumothorax,chest radiograph,deep learning,convolutional neural networks",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
48,Automated breast cancer risk estimation on routine CT thorax scans by deep learning segmentation,11314,,,"De Buck Stijn,Bertels Jeroen,Vanbilsen Chelsey,Dewaele Tanguy,Van Ongeval Chantal,Bosmans Hilde,Vandevenne Jan,Suetens Paul","De Buck S,Bertels J,Vanbilsen C,Dewaele T,Van Ongeval C,Bosmans H,Vandevenne J,Suetens P",De Buck S,10.1117/12.2549585,"UZ Leuven Radiol, Herestr 49, Leuven, Belgium.","Automation of systematic scoring of breast glandularity on CT thorax examinations performed for another clinical reason could aid in detecting postmenopausal women with increased breast cancer risk. We propose a novel method that combines automated deep learning based breast segmentation from CT thorax examinations with computation of breast glandularity based on radiodensity and volumetric breast density. Reasonable segmentation Dice scores were found as well as very strong correlation between the risk measures computed on the ground truth and with the proposed approach. Hence, the proposed method can offer reliable breast cancer risk measures with limited additional workload for the radiologist.","Breast Cancer,Glandularity,Risk Assessment,Deep Learning",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
49,Performance Comparison of Different Loss Functions for Digital Breast Tomosynthesis Classification using 3D Deep Learning Model,11314,,,"Doganay Emine,Luo Yahong,Gao Long,Li Puchen,Berg Wendie,Wu Shandong","Doganay E,Luo YH,Gao L,Li PC,Berg W,Wu SD",Wu SD,10.1117/12.2551373,Pennsylvania Commonwealth System of Higher Education (PCSHE),"Artificial intelligence (AI) algorithms, especially deep learning methods have proven to be successful in many medical imaging applications. Computerized breast cancer image analysis can improve diagnosis accuracy. Digital Breast Tomosynthesis (DBT) imaging is a new modality and more advantageous compared to classical digital mammography (DM). Therefore, development of new deep learning algorithms compatible with DBT modality are potent to improve DBT imaging reading time efficiency and increase accuracy for breast cancer diagnosis when used as additional tool for radiologists. In this work, we aimed to build a 3D deep learning model to distinguish malignancy and benign breasts using DBT images. We also investigated effects of different loss functions in our deep learning models. We implemented and evaluated our method on a large data set of 546 patients (205 malignancy and 341 benign). Our results showed that different loss functions lead to an influence on the models performance in our classification tasks, and specific loss function may be selected or customized to adjust a specific performance metric for concrete applications.","breast cancer,digital breast tomosynthesis,3D deep learning,loss function",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
50,Deep learning predicts breast cancer recurrence in analysis of consecutive MRIs acquired during the course of neoadjuvant chemotherapy,11314,,,"Drukker Karen,Edwards Alexandra,Papaioannou John,Giger Maryellen","Drukker K,Edwards A,Papaioannou J,Giger M",Drukker K,10.1117/12.2549044,University of Chicago,"The purpose of this study was to assess long short-termmemory networks in the prediction of recurrence-free survival in breast cancer patients using features extracted from MRIs acquired during the course of neoadjuvant chemotherapy. In the I-SPY1 dataset, up to 4 MRI exams were available per patient acquired at pre-treatment, early-treatment, interregimen, and pre-surgery time points. Breast cancers were automatically segmented and 8 features describing kinetic curve characteristics were extracted. We assessed performance of long short-termmemory networks in the prediction of recurrence-free survival status at 2 years and at 5 years post-surgery. For these predictions, we analyzed MRIs from women who had at least 2 (or 5) years of recurrence-free follow-up or experienced recurrence or death within that timeframe: 157 women and 73 women, respectively. One approach used features extracted from all available exams and the other approach used features extracted from only exams prior to the second cycle of neoadjuvant chemotherapy. The areas under the ROC curve in the prediction of recurrence-free survival status at 2 years post-surgery were 0.80, 95% confidence interval [0.68; 0.88] and 0.75 [0.62; 0.83] for networks trained with all 4 available exams and only the 'early' exams, respectively. Hazard ratios at the lowest, median, and highest quartile cut-points were 6.29 [2.91; 13.62], 3.27 [1.77; 6.03], 1.65 [0.83; 3.27] and 2.56 [1.20; 5.48], 3.01 [1.61; 5.66], 2.30 [1.14; 4.67]. Long short-termmemory networks were able to predict recurrence-free survival in breast cancer patients, also when analyzing only MRIs acquired 'early on' during neoadjuvant treatment.","long short-term memory networks,breast cancer survival,radiomics,MRI",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,"FREE,SURVIVAL,PROGNOSTIC,VALUE,TUMOR,VOLUME,LESIONS",,,
51,Accurately identifying vertebral levels in large datasets,11314,,,"Elton Daniel C.,Sandfort Veit,Pickhardt Perry J.,Summers Ronald M.","Elton DC,Sandfort V,Pickhardt PJ,Summers RM",Elton DC; Summers RM,10.1117/12.2551247,National Institutes of Health (NIH) - USA,"The vertebral levels of the spine provide a useful coordinate system when making measurements of plaque, muscle, fat, and bone mineral density. Correctly classifying vertebral levels with high accuracy is challenging due to the similar appearance of each vertebra, the curvature of the spine, and the possibility of anomalies such as fractured vertebrae, implants, lumbarization of the sacrum, and sacralization of L5. The goal of this work is to develop a system that can accurately and robustly identify the L1 level in large heterogeneous datasets. The first approach we study is using a 3D U-Net to segment the L1 vertebra directly using the entire scan volume to provide context. We also tested models for two class segmentation of L1 and T12 and a three class segmentation of L1, T12 and the rib attached to T12. By increasing the number of training examples to 249 scans using pseudo-segmentations from an in-house segmentation tool we were able to achieve 98% accuracy with respect to identifying the L1 vertebra, with an average error of 4.5 mm in the craniocaudal level. We next developed an algorithm which performs iterative instance segmentation and classification of the entire spine with a 3D U-Net. We found the instance based approach was able to yield better segmentations of nearly the entire spine, but had lower classification accuracy for L1.","machine learning,deep learning,segmentation,instance segmentation,spine,vertebrae,U-Net",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,"CONVOLUTIONAL,NEURAL-NETWORKS,SEGMENTATION",,http://arxiv.org/pdf/2001.10503,
52,Machine learning methods to predict presence of intestine damage in patients with Crohn's disease,11314,,,"Enchakalody Binu E.,Henderson Brianna,Wang Stewart C.,Su Grace L.,Wasnik Ashish P.,Al-Hawary Mahmoud M.,Stidham Ryan W.","Enchakalody BE,Henderson B,Wang SC,Su GL,Wasnik AP,Al-Hawary MM,Stidham RW",Enchakalody BE,10.1117/12.2549326,University of Michigan System,"The diagnosis of Crohn's disease (CD) can be challenging given variation in anatomic disease distribution, morphology, and proportion of intestine affected. Subsequently, the appearance and presentation of disease on cross-sectional imaging are a heterogeneous combination of shapes and image features, making differentiation of normal vs. diseased small intestine prone to inter-observer variation. Applying machine learning methods to cross-sectional, imaging interpretation may improve the accuracy of CD diagnosis and distinguish normal from diseased intestine by automated approaches. Using a set of 207 CT-enterography (CTE) scans, two independent radiologists labeled the presence of disease vs. non-disease at 7.5mm intervals along the length of the bowel (mini-segments), generating a dataset of 10,552 observations for model training and testing. We introduce two types of classifiers to quantitatively assess CD related intestinal damage for each mini-segment. The sensitivity, specificity and AUC for the best performing ensemble and CNN models are 84.9%, 84.7%, 0.93, and 90.9%, 78.6%, 0.92 respectively. The accuracy for classifying full segments as diseased vs. normal using ensemble and CNN models are 96.3% and 90.7% respectively.","Crohn's Disease,Image analysis,Computer aided diagnosis,Radiomics,Deep Learning,Automated Bowel Morphomics (ABM),3-D CNN,Morphomic Analysis Group (MAG)",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,ENTEROGRAPHY,,,
53,Attention-Guided Classification of Abnormalities in Semi-Structured Computed Tomography Reports,11314,,,"Faryna Khrystyna,Tushar Fakrul I.,D'Anniballe Vincent,Hou Rui,Rubin Geoffrey D.,Lo Joseph Y.","Faryna K,Tushar FI,D'Anniballe V,Hou R,Rubin GD,Lo JY",Faryna K,10.1117/12.2551370,Universitat de Girona,"Lack of annotated data is a major challenge to machine learning algorithms, particularly in the field of radiology. Algorithms that can efficiently extract labels in a fast and precise manner are in high demand. Weak supervision is a compromise solution, particularly, when dealing with imaging modalities like Computed Tomography (CT), where the number of slices can reach 1000 per case. Radiology reports store crucial information about clinicians' findings and observations in CT slices. Automatic generation of labels from CT reports is not a trivial task due to the complexity of sentences and diversity of expression in free-text narration. In this study, we focus on abnormality classification in lungs, liver and kidneys. Firstly, a rule-based model is used to extract weak labels at the case level. Afterwards, attention guided recurrent neural network (RNN) is trained to perform binary classification of radiology reports in terms of whether the organ is normal or abnormal. Additionally, a multi-label RNN with attention mechanism is trained to perform binary classification by aggregating its output for four representative diseases (lungs: emphysema, mass-nodule, effusion and atelectasis-pneumonia; liver: dilatation, fatty infiltration-steatosis, calcification-stone-gallstone, lesion-mass; kidneys: atrophy, cyst, stone-calculi, lesion) into a single abnormal class. Performance has been evaluated using the receiver operating characteristic (ROC) area under the curve (AUC) on 274, 306 and 278 reports for lungs, liver and kidneys correspondingly, manually annotated by radiology experts. The change in performance was evaluated for different sizes of training dataset for lungs. The AUCs of multi-label pretrained models: lungs-0.929, liver-0.840, kidney-0.844; multi-label models: lungs-0.903, liver-0.848, kidney-0.906; binary pretrained models: lungs-0.922, liver-0.826, kidneys-0.928.","weak supervision,attention RNN,rule-based model,Computed Tomography",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
54,Mask R-CNN based Coronary Artery Segmentation in Coronary Computed Tomography Angiography,11314,,,"Fu Yabo,Guo Bangjun,Lei Yang,Wang Tonghe,Liu Tian,Curran Walter,Zhang Longjiang,Yang Xiaofeng","Fu YB,Guo BJ,Lei Y,Wang TH,Liu T,Curran W,Zhang LJ,Yang XF",Yang XF,10.1117/12.2550588,Emory University,"Automated segmentation of the coronary artery in coronary computed tomographic angiography (CCTA) is important for clinicians in evaluating patients with coronary artery disease. Tradition visual interpretation of coronary artery stenosis exist inter-observer variability and time-consuming. The purpose of this work is to develop a deep learning-based framework for coronary artery segmentation on CCTA. We propose to use Mask R-CNN for the coronary artery segmentation. To avoid the interferences from pulmonary vessels, we propose to mask out the lung region prior to Mask R-CNN training. The network was trained using 20 patients' CCTA datasets and tested using another 5 patients' CCTA datasets. The mean of the Dice similarity coefficient (DSC) were 0.90 +/- 0.01 respectively, which demonstrated the segmentation accuracy of the proposed method.","coronary artery,segmentation,CTA,deep learning",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,"CT,ANGIOGRAPHY,QUANTIFICATION,EXTRACTION",,,
55,3D U-Net for Segmentation of Pulmonary Nodules in Volumetric CT Scans from Multi-Annotator Truth Estimation,11314,,,"Funke William,Veasey Benjamin,Zurada Jacek,Frigui Hichem,Amini Amir","Funke W,Veasey B,Zurada J,Frigui H,Amini A",Funke W,10.1117/12.2548496,University of Louisville,"Low dose CT screening has been shown to significantly reduce mortality rates due to lung cancer. To assist radiologists, CAD systems continue to be developed for automatically detecting, segmenting, and categorizing potentially malignant lung nodules. Deep learning with the U-Net architecture has shown to be effective for automatic segmentation of 2-D images. The network consists of a down-sampling and up -sampling path, similar to an auto-encoder. However, the concept driving its success is skip-connections between down-sampling and up -sampling, allowing the network to preserve details and for easier backpropagation of error to deep layers. This concept has previously been brought into 3-D and successfully applied to image volumes such as MRI and CT scans. This paper applies concepts from these works (skip connections, batch normalization, Dice similarity coefficient loss, and stride convolution for down sampling) to a 3-D Convolutional Neural Network for segmenting nodule image patches in thoracic CT scans from the LIDC-IDRI database. This database contains scans from 1018 cases with annotations from up to 4 human experts. Within each scan, nodules are delineated by each of these experts. It is well known that manual delineation can be subjective between annotators. This paper proposes a model trained on a ground truth estimation from the four expert annotations using the STAPLE algorithm. Experiments in this paper show that when trained on STAPLE, automatic segmentation with a 3-D U-Net can result in improved similarity scores to human annotators than similarity scores between human annotators.",,Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
56,Multi-modal component subspace-similarity-based multi-kernel SVM for schizophrenia classification,11314,,,"Gao Shuang,Calhoun Vince D.,Sui Jing","Gao S,Calhoun VD,Sui J",Sui J,10.1117/12.2550339,Chinese Academy of Sciences,"Heterogeneous multi-modal medical imaging data need to be properly handled in classification. Currently, generating models using multi-modal imaging data has become a common practice and greatly benefits the brain disorder diagnosis, which also holds considerable clinical potential. Although the majority of classification studies focus on using features from single modality, there is substantial evidence suggesting that classification based on multi-modal features is on upward trend. Hence, effective integration of heterogeneous data is in urgent demand. Here, we proposed a multi-kernel SVM for schizophrenia classification with nested 10-fold cross validation, which could integrate multi-modal data using the subspace similarity of the decomposed components in each MRI modality. To validate the effectiveness of the proposed method, we performed experiments on two independent datasets with three different modalities to classify schizophrenia patients and healthy controls. Specifically, multi-modal fusion method was first applied on preprocessed fIVIRI, DTI and sMRI data to generate components that could be used for classification. Then multi-kernel SVM models were trained on the selected component features using subspace similarity measures, and were tested on independent validation data across sites. The results on both datasets demonstrated that our method achieved accuracies of 87.6% and 79.9% separately on two datasets when combining all three modalities, which outperformed alternative methods and might provide potential biomarkers for cross-site classification and co-varying components among different modalities.","Multi-modal,mCCA plus jICA,Subspace similarity,Multi-kernel SVM,Schizophrenia",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
57,Advanced Magnetic Resonance Imaging Based Algorithm for Local Grading of Glioma,11314,,,"Gates Evan D. H.,Lin Jonathan S.,Weinberg Jeffrey S.,Prabhu Sujit S.,Hamilton Jackson,Hazle John D.,Fuller Gregory N.,Baladandayuthapani Veera,Fuentes David T.,Schellingerhout Dawid","Gates EDH,Lin JS,Weinberg JS,Prabhu SS,Hamilton J,Hazle JD,Fuller GN,Baladandayuthapani V,Fuentes DT,Schellingerhout D",Schellingerhout D,10.1117/12.2549607,University of Texas System,"The purpose of this work is to determine the strength of correlations between imaging data and local tumor grade using spatially specific tumor samples to validate against a histologic gold-standard This improves our understanding of diagnostic imaging by correlating with underlying biology.
Glioma patients were enrolled in an IRB approved prospective clinical imaging trial between 2013 and 2016. MR imaging was performed with anatomic (T1, T2, FLAIR, T1 post-contrast, and susceptibility), diffusion tensor, dynamic susceptibility and dynamic contrast sequences. During surgery stereotactic biopsy were collected prior to resection along with image space coordinates of the samples. A random forest were built to predict the grade of each sample using preoperative imaging data. The model was assessed based on classification accuracy, Cohen's kappa, and sensitivity to higher grade disease
Twenty-three patients with fifty-two total biopsy samples were analyzed. The Random Forest method predicted tumor grade at 94% accuracy using four inputs (T2, ADC, CBV and Ktrans). Using conventional imaging only, the overall accuracy decreased (89% overall, kappa = 0.78) and 71% of high grade samples were misclassified as lower grade disease.
We found that pathologic features can be predicted to high accuracy using clinical imaging data. Advanced imaging data contributed significantly to this accuracy, adding value over accuracies obtained using conventional imaging only. Confirmatory imaging trials are justified.","Glioma,Glioblastoma,MRI,Machine Learning,Stereotactic Biopsy,Random Forest,Dynamic Contrast",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
58,Melanoma detection with electrical impedance spectroscopy and dermoscopy using joint deep learning models,11314,,,"Gessert Nils,Bengs Marcel,Schlaefer Alexander","Gessert N,Bengs M,Schlaefer A",Gessert N,10.1117/12.2548974,Hamburg University of Technology,"The initial assessment of skin lesions is typically based on dermoscopic images. As this is a difficult and time-consuming task, machine learning methods using dermoscopic images have been proposed to assist human experts. Other approaches have studied electrical impedance spectroscopy (EIS) as a basis for clinical decision support systems. Both methods represent different ways of measuring skin lesion properties as dermoscopy relies on visible light and EIS uses electric currents. Thus, the two methods might carry complementary features for lesion classification. Therefore, we propose joint deep learning models considering both EIS and dermoscopy for melanoma detection. For this purpose, we first study machine learning methods for EIS that incorporate domain knowledge and previously used heuristics into the design process. As a result, we propose a recurrent model with state-max-pooling which automatically learns the relevance of different EIS measurements. Second, we combine this new model with different convolutional neural networks that process dermoscopic images. We study ensembling approaches and also propose a cross-attention module guiding information exchange between the EIS and dermoscopy model. In general, combinations of EIS and dermoscopy clearly outperform models that only use either EIS or dermoscopy. We show that our attention-based, combined model outperforms other models with specificities of 34.4 % (CI 31.3-38.4), 34.7 % (CI 31.0-38.8) and 53.7 % (CI 50.1-57.6) for dermoscopy, EIS and the combined model, respectively, at a clinically relevant sensitivity of 98 %.","Dermoscopy,Electrical Impedance Spectroscopy,Deep Learning,Melanoma",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,SYSTEM,,http://arxiv.org/pdf/1911.02322,
59,Artificially Augmenting Data or Adding more Samples? A Study on a 3D CNN for Lung Nodule Classification,11314,,,"Gonidakis Panagiotis,Jansen Bart,Vandemeulebroucke Jef","Gonidakis P,Jansen B,Vandemeulebroucke J",Gonidakis P,10.1117/12.2549810,Vrije Universiteit Brussel,"Convolutional neural networks are known to require large amounts of data to achieve optimal performance. In addition, data is commonly computationally augmented using a variety of geometric and intensity transformations to further extent the set of training samples. In medical imaging, annotated data is often scarce or costly to obtain, and there is considerable interest in methods to reduce the amount of data needed. In this work, we investigate the relative benefit of increasing the amount of original data, with respect to computationally augmenting the amount of training samples, for the case of false positive reduction of lung nodules candidates. To this end, we have implemented a previously published topology for classification, shown to achieve state of the art results on the publicly available Luna16 dataset. Numerous models were trained using different amounts of unique training samples and different degrees of data augmentation involving rotations and translations, and the performance was compared. Results indicate that in general, better performance is achieved when increasing the amount of data, or augmenting the data more extensively, as expected. Surprisingly however, we observed that after reaching a certain amount of unique training samples, data augmentation leads to significantly better performance compared to adding the same number of new samples to the training dataset. We hypothesize that the augmentation has aided in learning more general rotation and translation invariant features, leading to improved performance on unseen data. Future experiments include more detailed characterization of this behavior, and relating this to the topology and amount of parameters to be trained.","3D CNN,convolutional neural networks,deep learning,data augmentation,false positive reduction,pulmonary nodule detection,lung cancer",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
60,Multi-modal deep learning for predicting progression of Alzheimer's disease using bi-linear shake fusion,11314,,,"Goto Tsubasa,Wang Caihua,Li Yuanzhong,Tsuboshita Yukihiro","Goto T,Wang CH,Li YZ,Tsuboshita Y",Goto T,10.1117/12.2549483,Fujifilm Corporation,"Alzheimer's Disease (AD) which causes declination of cognitive function is one of the most severe social issues in the world. It has already been known that AD cannot be cured and treatment can only delay its progression. Therefore, it is very important to detect AD in early stage and prevent it to be worse. Furthermore, sooner the progression is detected, better the prognosis will be. In this research, we developed a novel multi-modal deep learning method to predict conversion from Mild Cognitive Impairment (MCI), which is the stage between cognitively normal older people and AD. In our method, the multi-modal input data are defined as structural Magnetic Resonance Imaging (MRI) images and clinical data including several cognitive scores, APOE genotype, gender and age obtained from Alzheimer's Disease Neuroimaging Initiative cohort (ADNI). Our criteria of selecting these input data are that they are mostly obtained by non-invasive examination. The proposed method integrates features obtained from MRI images and clinical data effectively by using bi-linear fusion. Bi-linear fusion computes the products of all elements between image and clinical features, where the correlation between them are included. That led to a big improvement of prediction accuracy in the experiment. The prediction model using bi-linear fusion achieved to predict conversion in one year with 0.86 accuracy, comparing with 0.76 accuracy using linear fusion. The proposed method is useful for screening examination for AD or deciding a stratification approach within clinical trials since it achieved a high accuracy while the input data is relatively easy to be obtained.","Alzheimer's disease,multi-modal deep learning,bi-linear fusion",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA",,,,"MILD,COGNITIVE,IMPAIRMENT,DIAGNOSIS",,,
61,Benign and Malignant Thyroid Classification Using Computed Tomography Radiomics,11314,,,"Guo Bang Jun,He Xiuxiu,Wang Tonghe,Lei Yang,Curran Walter J.,Liu Tian,Zhang Long Jiang,Yang Xiaofeng","Guo BJ,He XX,Wang TH,Lei Y,Curran WJ,Liu T,Zhang LJ,Yang XF",Yang XF,10.1117/12.2549087,Emory University,"Thyroid cancer (TC) is a prevalent malignancy with a high predicated new case number and estimated death in 2019. Although four to seven percent of the adult population has a palpable thyroid nodule, however only one of twenty clinically identified TNs is malignant. Imaging modalities, including US, CT, and magnetic resonance (MRI), have been widely used for thyroid nodule evaluation, but the reliability is low. We propose a learning method for the classification of thyroid using thyroid non-enhanced thyroid computed tomography and radiomics study. Ninety-two patients with suspected or known to have abnormal thyroid nodules in their thyroid were enrolled. The thyroid on the non-enhanced thyroid CT was manually segmented. One hundred radiomic features of the thyroid were extracted. The most informative and non-redundant features were selected to train a Support Vector Machine (SVM) to differentiate benign thyroid and malignant thyroid (with malignant TNs). Analysis of the predictions showed that the reported method has accuracy 0.8185 +/- 0.0366 and area under the receiver operating characteristic curve (AUC) 0.8376 +/- 0.0343. This study shows that thyroid-radiomic features derived from non-enhanced thyroid CT data can be used to classify benign vs. malignant thyroid. The radiomic features of thyroid from non-enhanced thyroid CT could be a useful tool for determining benign or malignant thyroid.","Thyroid cancer,classification,radiomics,machine learning",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,"NODULE,MRI",,,
62,Multi-task Learning for Mortality Prediction in LDCT Images,11314,,,"Guo Hengtao,Kruger Melanie,Wang Ge,Kalra Mannudeep K.,Yan Pingkun","Guo HT,Kruger M,Wang G,Kalra MK,Yan PK",Yan PK,10.1117/12.2549387,Rensselaer Polytechnic Institute,"Low-dose CT (LDCT) has been commonly used for lung cancer screening and it is much desirable to computerize the image analysis for risk evaluation to reduce healthcare disparities. While informative structural image features can be extracted from medical images using state-of-the-art deep neural networks, other quantitative clinical measurements can also contribute to the overall assessment but are often ignored by researchers and also difficult to obtain. This work introduces a multi-task learning framework, which can simultaneously extract image features from LDCT images and estimate the clinical measurements for all-cause mortality risk prediction. The proposed method is a hybrid neural network with multi-scale input and multi-task supervision labels. The presented work shows that the extracted feature vectors have improved mortality prediction as they are generated to include both abstracted image features and high-level clinical knowledge.","Low-dose CT,mortality risk prediction,deep learning,multi-task learning,clinical knowledge",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
63,False positive reduction of vasculature for pulmonary nodule detection,11314,,,"Hansen Colin B.,Zhao Yiyuan,Yerebakan Halid,Bogoni Luca,Jerebko Anna","Hansen CB,Zhao YY,Yerebakan H,Bogoni L,Jerebko A",Hansen CB,10.1117/12.2549323,Vanderbilt University,"Lung cancer stands as the deadliest cancer worldwide, and early detection of pulmonary nodules is the focus of many studies to enhance the survival rate. As with many diseases, deep learning is becoming a commonly used technique for computer-aided diagnosis (CAD) in detecting lung nodules. Most lung CAD systems rely on a detection module followed by a false positive (FP) reduction module (FPR); however, FPR removes FPs as well as true positives (TPs). Thus, as a tradeoff, in order to retain high sensitivity, a large number of FPs remain. In our experience, small pulmonary vessels have been the primary source of FPs. Hence, we propose an additional module cascaded on normal FPR module to specifically reduce the number of FPs due to pulmonary vessel. Utilizing a 3D deep learning architecture, we find that the inclusion of various fields of view (FOVs) improves the accuracy of the chosen model. We explore the impact of the selection of the FOVs, the method used to integrate the features from each FOV, and using the FOV as a data augmentation method. We show that this vessel specific FPR module significantly improves the CAD system's FP rate while only sacrificing 5% of the previously achieved sensitivity.","Lung,pulmonary,nodule,false positive,reduction,vessel,deep learning,ensemble",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,"COMPUTER-AIDED,DETECTION,CT,CAD,PERFORMANCE",,,
64,Usefulness of fine-tuning for deep learning based multi-organ regions segmentation method from non-contrast CT volumes using small training dataset,11314,,,"Hayashi Yuichiro,Shen Chen,Roth Holger R.,Oda Masahiro,Misawa Kazunari,Jinzaki Masahiro,Hashimoto Masahiro,Kumamaru Kanako K.,Aoki Shigeki,Mori Kensaku","Hayashi Y,Shen C,Roth HR,Oda M,Misawa K,Jinzaki M,Hashimoto M,Kumamaru KK,Aoki S,Mori K",Hayashi Y,10.1117/12.2551022,Nagoya University,"This paper presents segmentation of multiple organ regions from non-contrast CT volume based on deep learning. Also, we report usefulness of fine-tuning using a small number of training data for multi-organ regions segmentation. In medical image analysis system, it is vital to recognize patient specific anatomical structures in medical images such as CT volumes. We have studied on a multi-organ regions segmentation method from contrast-enhanced abdominal CT volume using 3D U-Net. Since non-contrast CT volumes are also usually used in the medical field, segmentation of multi-organ regions from non-contrast CT volume is also important for the medical image analysis system. In this study, we extract multi-organ regions from non-contrast CT volume using 3D U-Net and a small number of training data. We perform fine-tuning from a pre-trained model obtained from the previous studies. The pre-trained 3D U-Net model is trained by a large number of contrast enhanced CT volumes. Then, fine-tuning is performed using a small number of non-contrast CT volumes. The experimental results showed that the fine-tuned 3D U-Net model could extract multi-organ regions from non-contrast CT volume. The proposed training scheme using fine-tuning is useful for segmenting multi-organ regions using a small number of training data.","Segmentaion,Deep learning,fine-tuning,CT",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
65,Classification of Lesion Specific Myocardial Ischemia Using Cardiac Computed Tomography Radiomics,11314,,,"He Xiuxiu,Guo Bang Jun,Wang Tonghe,Lei Yang,Liu Tian,Zhang Long Jiang,Yang Xiaofeng","He XX,Guo BJ,Wang TH,Lei Y,Liu T,Zhang LJ,Yang XF",Yang XF,10.1117/12.2548471,Emory University,"Lesion-specific myocardial ischemia is a common heart disorder and a significant cause of cardiovascular morbidity and mortality. It alters left ventricular myocardial thickness progressively. Clinical decision-making is based on Fractional flow reserve (FFR), which is invasive and may prolong the surgery time and with extra radiation exposure. Although coronary computed tomography angiogram (CCTA) has high accuracy and negative predictive value (NPV) in the evaluation of coronary artery disease (CAD), it has low specificity in the diagnosis of lesion-specific myocardial ischemia. We propose a learning method for the assessment of lesion-specific myocardial ischemia using noninvasive CCTA and radiomics study. Sixty patients with suspected or known to have CAD were enrolled. The left ventricular myocardial on the CCTA was manually segmented. One hundred radiomic features of left ventricular myocardial were extracted. The most informative and non-redundant features were selected to train a Support Vector Machine (SVM) is to differentiate lesion-specific myocardial ischemia and without lesion-specific myocardial ischemia (normal). Analysis of the predictions showed that the reported method consistently predicted lesion-specific myocardial ischemia with the accuracy of 0.8550 +/- 0.0333 and area under the receiver operating characteristic curve (AUC) 0.8952 +/- 0.0370. This study shows that LVM-radiomic features derived from CCTA data can be used to classify lesion specific myocardial ischemia. The radiomic features of left ventricular myocardial from CCTA could be a useful tool for determining lesion specific myocardial ischemia","Lesion-specific myocardial ischemia,classification,radiomics,machine learning",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,"FRACTIONAL,FLOW,RESERVE,CORONARY-ARTERY-DISEASE,DIAGNOSTIC,PERFORMANCE,INTRATUMOR,HETEROGENEITY,ANGIOGRAPHY,EVOLUTION",,,
66,3D Thyroid Segmentation in CT Using Self-attention Convolutional Neural Network,11314,,,"He Xiuxiu,Guo Bang Jun,Lei Yang,Liu Yingzi,Wang Tonghe,Curran Walter J.,Zhang Long Jiang,Liu Tian,Yang Xiaofeng","He XX,Guo BJ,Lei Y,Liu YZ,Wang TH,Curran WJ,Zhang LJ,Liu T,Yang XF",Liu T; Yang XF,10.1117/12.2549786,Emory University,"The thyroid gland is a butterfly-shaped organ and belongs to the endocrine system. The abnormality in shape and volume of thyroid can reveal the occurrence of various diseases. Ultrasound (US) imaging is currently the most popular diagnostic tool for diagnosing thyroid diseases. However, most physicians would still make decisions depending on computed tomography (CT) because of its excellent resolution to show more details of the thyroid and its surroundings The thyroid CT imaging before surgery is important because it can assist in determining the anatomical distribution of a lesion and its involvement in adjacent organs or tissues. However, precise segmentation of the thyroid relies heavily on the experience of the physician and is very time-consuming. In this work, we propose to use a 3D deep attention U-Net method to segment the thyroid from CT image automatically. The quantitative evaluation of the segmentation performance of the proposed method, we calculated the Dice similarity coefficient (DSC), sensitivity, specificity, and mean surface distance (MSD) indices between the ground truth and automatic segmentation We demonstrated high accuracy and robustness of the proposed deep-learning-based segmentation method visually and quantitatively. The resultant DSC, precision, and recall were 85% 6%, 86% 5% and 90% 5%, respectively.","Thyroid segmentation,self-attention,deep learning",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
67,Direct Classification of Type 2 Diabetes From Retinal Fundus Images in a Population-based Sample From The Maastricht Study,11314,,,"Heslinga Friso G.,Pluim Josien P. W.,Houben A. J. H. M.,Schram Miranda T.,Henry Ronald M. A.,Stehouwer Coen D. A.,van Greevenbroek Marleen J.,Berendschot Tos T. J. M.,Veta Mitko","Heslinga FG,Pluim JPW,Houben AJHM,Schram MT,Henry RMA,Stehouwer CDA,van Greevenbroek MJ,Berendschot TTJM,Veta M",Heslinga FG,10.1117/12.2549574,Eindhoven University of Technology,"Type 2 Diabetes (T2D) is a chronic metabolic disorder that can lead to blindness and cardiovascular disease. Information about early stage T2D might be present in retinal fundus images, but to what extent these images can be used for a screening setting is still unknown. In this study, deep neural networks were employed to differentiate between fundus images from individuals with and without T2D. We investigated three methods to achieve high classification performance, measured by the area under the receiver operating curve (ROC-AUC). A multi-target learning approach to simultaneously output retinal biomarkers as well as T2D works best (AUC = 0.746 [+/- 0.001]). Furthermore, the classification performance can be improved when images with high prediction uncertainty are referred to a specialist. We also show that the combination of images of the left and right eye per individual can further improve the classification performance (AUC = 0.758 [+/- 0.003]), using a simple averaging approach. The results are promising, suggesting the feasibility of screening for T2D from retinal fundus images.","Deep Learning,Retinal Image Analysis,Type 2 Diabetes,Classification Uncertainty,The Maastricht Study",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,"PREDICTION,MORTALITY",,http://arxiv.org/pdf/1911.10022,
68,A combination of intra- and peri-tumoral deep features from prostate bi-parametric MRI can distinguish clinically significant and insignificant prostate cancer,11314,,,"Hiremath Amogh,Shiradkar Rakesh,Braman Nathaniel,Prasanna Prateek,Rastinehad Art,Purysko Andrei,Madabhushi Anant","Hiremath A,Shiradkar R,Braman N,Prasanna P,Rastinehad A,Purysko A,Madabhushi A",Hiremath A,10.1117/12.2550963,Case Western Reserve University,"Bi-parametric MRI (bpMRI: T2W MRI and Apparent Diffusion Coefficient maps (ADC) derived from diffusion weighted imaging) is increasingly being used to characterize prostate cancer (PCa). However, inter- and intra-reader variability hinders interpretation of MRI. Deep learning networks may aid in PCa characterization and may allow for non-invasively distinguishing clinically significant PCa (csPCa) defined as gleason grade group (GGG) >1 and insignificant PCa (ciPCa) defined as GGG=1. Recent studies using radiomic features have shown that signatures from peri-tumoral (PT) region on MRI scans add significant value to those from intra-tumoral (IT) region for disease detection and characterization. In this study we explore the utility of deep features of a convolutional neural network (CNN) extracted from IT and PT regions in distinguishing csPCa and ciPCa. A retrospective study of N=192 PCa patients with data from two institutions acquired from three different scanners was conducted in this study. All patients underwent 3T multi-parametric MRI (mpMRI), and we used bpMRI consisting of T2W MRI and ADC maps. The entire dataset with N=192 patients was randomly sampled into training (60%, N=114 patients), validation (20%, N=39 patients) and test (20%, N=39 patients) set. A radiologist from each institute delineated PCa regions of interest (ROI) by matching whole mount sections with bpMRI or by taking biopsy reports into account. In this work, we present a multi-sequence multi-instance learning convolutional neural network (siNET) trained by extracting 2D patches centered at PCa ROIs on prostate bpMRI to distinguish csPCa and ciPCa. The trained network is used to extract pooled features from both the IT and PT regions, which are then used to train a random forest classifier. Models trained on the training set using IT (D-IT) and PT (D-pT) deep features alone resulted in an area under the curve (AUC) of 0.85 and 0.72 on the test set respectively. The model D-IP trained on combination of IT and PT deep features resulted in an AUC of 0.89 on the test set. Therefore, combining IT and PT features helps in improving the overall classifier performance in distinguishing csPCa and ciPCa.","Clinically significant prostate cancer,peri-tumoral,deep learning,multiple instance learning",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
69,Combining Deep and Hand-crafted MRI Features for Identifying Sex-specific Differences in Autism-Spectrum Disorder versus Controls,11314,,,"Hiremath Yashas,Ismail Marwa,Verma Ruchika,Antunes Jacob,Tiwari Pallavi","Hiremath Y,Ismail M,Verma R,Antunes J,Tiwari P",Tiwari P,10.1117/12.2551341,Case Western Reserve University,"Autism spectrum disorder (ASD) is a collection of neuro-developmental disorders with many symptoms, most prominently social impairment. It is known that there is a significant prevalence (4:1) of ASD in males compared to females. This suggests that there will likely be distinct neuro-anatomical structures across males and females that contribute to the distinct disease etiologies across the two sexes. Hence, in this work, we seek to develop ""sexspecific"" machine learning models that attempt to capture neuroanatomical differences in brain morphometry across ASD versus normal controls using structural MRI scans. Specifically, we train two different machine-learning models (one for male and one for female cohorts) consisting of ""hand-crafted"" morphometric features such as shape and surface area of brain parcellations from Tlw MRI, with ""deep"" features learned from a Dense Convolutional Network (DenseNet). Our methodology consists of first computing morphometric hand-crafted features (i.e. volume and surface features of different brain parcellations) from the training cohort obtained from the ABIDE-II dataset consisting of 210 males and 98 females from structural Tlw MRI scans. We then employ feed-forward feature selection within a linear discriminant analysis classifier trained separately for the male and female cohorts, to distinguish ASD from normal controls. Additionally, we train a DenseNet model with 4 dense blocks (with 2 layers each) to extract deep features from Tlw MRI scans to distinguish ASD versus controls, separately for males and females. The deep features allow for capturing complementary data-driven feature differences in brain morphometry specific to male and female ASD cohort (versus normal controls). Finally, we combine the top morphometric and DenseNet features obtained from the training model and test them on the ABIDE I dataset for the male (n=85) and female (n=19) cohort separately, to distinguish ASD from normal controls. Our results demonstrated training and testing accuracies of 78% and 79% using hand-crafted features alone, 68% and 57% using DenseNet features alone, and 87% and 79% respectively using integrated hand-crafted and deep features for the male cohort. For the female cohort, we obtained accuracies of 81% and 84% with hand-crafted features alone, 71% and 62% for DenseNet features alone, and 81% and 84% with integrated hand-crafted and deep features, on training and testing sets respectively. With further optimization of deep features along with inclusion of a large multi-site cohort, our presented sex-specific ML approach could allow for improved diagnosis of ASD from controls, across males and females, using structural MRI scans.","Autism Spectrum Disorder,DenseNet,Discriminant Analysis,Surface,Volume,Parcellations",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
70,A multitask deep learning method in simultaneously predicting occult invasive disease in ductal carcinoma in-situ and segmenting microcalcifications in mammography,11314,,,"Hou Rui,Grimm Lars J.,Mazurowski Maciej A.,Marks Jeffrey R.,King Lorraine M.,Maley Carlo C.,Hwang E. Shelley,Lo Joseph Y.","Hou R,Grimm LJ,Mazurowski MA,Marks JR,King LM,Maley CC,Hwang ES,Lo JY",Hou R,10.1117/12.2549669,Duke University,"We proposed a two-branch multitask learning convolutional neural network to solve two different but related tasks at the same time. Our main task is to classify occult invasive disease in biopsy proven Ductal Carcinoma in-situ, with an auxiliary task of segmenting microcalcifications. In this study, we collected digital mammography with 1.5 or 1.8 magnification views from 604 patients. 400 of them were diagnosed with Ductal Carcinoma in-situ at biopsy, 66 of which were subsequently diagnosed as invasive breast cancer at the stage of definite surgery. Additional training was performed using data from 74 patients diagnosed with Invasive Ductal Carcinoma and 130 patients with Atypical Ductal Hyperplasia. The latter two classes were included only in training as a forced-labeling method to aid the classification problems based on our previous experiments. The model used patches with size of 512x512 extracted within a radiologist masked ROIs as input. The output included noisy microcalcifications segmentations obtained from our previous computer vision algorithms, and classification labels from final diagnosis at patients' definite surgery. We utilized a deep multitask model by combining both Unet segmentation networks and classification networks. Specifically, the encoder layers from Unet were also shared by classification networks, with 2 convolutional layers and 2 fully connected layers followed by classification. A hand-crafted weighted loss function was applied to enhance the converging of classification network. Case-based classifications were assigned by highest classification values from patches. The model achieved a patch-based ROC-AUC of 0.69, with a case-based ROC-AUC of 0.61. Segmentation results achieved a dice coefficient of 0.49.","Multitask learning,Ductal Carcinoma in-situ,invasive breast cancer,classification,segmentation",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
71,Using ResNet feature extraction in computer-aided diagnosis of breast cancer on 927 lesions imaged with multiparametric MRI,11314,,,"Hu Qiyuan,Whitney Heather M.,Giger Maryellen L.","Hu QY,Whitney HM,Giger ML",Hu QY,10.1117/12.2548872,University of Chicago,"In this study, we aim to develop a multiparametric breast MRI computer-aided diagnosis (CADx) methodology using residual neural network (ResNet) deep transfer learning to incorporate information from both dynamic contrast-enhanced (DCE)-MRI and T2-weighted (T2w) MRI in the task of distinguishing between benign and malignant breast lesions. This retrospective study included 927 unique lesions from 616 women who underwent breast MR exams A pre-trained ResNet50 was used to extract features from the maximum intensity projection (MIP) images of the second postcontrast subtraction DCE series and the center slice of the T2w series separately. Support vector machine classifiers were trained on the ResNet features to differentiate between benign and malignant lesions. The benefit of pooling features extracted from multiple levels of the network was examined on DCE MIPs. Three multiparametric methods were investigated, where information from the two sequences was integrated at the image level, feature level, or classifier level. Classification performances were evaluated with five-fold cross-validation using the area under the receiver operating characteristic curve (AUC) as the figure of merit. Using pooled features extracted from multiple layers of the ResNet statistically significantly outperformed only using features extracted from the end of the network (P =.002, 95% CI of Delta AUC: [0.007, 0.029]). The multiparametric classifiers using pooled features yielded AUC(ImageFusion)=0.85 +/- 0.01, AUC(FeatureFusion)-0.87 +/- 0.01, and AUC(classifierFusion)=0.86 +/- 0.01, respectively. The feature fusion method statistically significantly outperformed using DCE alone (P =.01, 95% CI of Delta AUC: [0.004, 0.022]), and all three methods statistically significantly outperformed using T2w alone (P <.001).","breast cancer,MRI,multiparametric,ResNet,machine learning,deep learning,transfer learning,computer-aided diagnosis",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,"CONVOLUTIONAL,NEURAL-NETWORKS,ROC,CURVES,SEQUENCES,RISK",,,
72,Visualising decision-reasoning regions in computer-aided pathological pattern diagnosis of endoscytoscopic images based on CNN weights analysis,11314,,,"Itoh Hayato,Lu Zhongyang,Mori Yuichi,Misawa Masashi,Oda Masahiro,Kudo Shin-ei,Mori Kensaku","Itoh H,Lu ZY,Mori Y,Misawa M,Oda M,Kudo S,Mori K",Itoh H,10.1117/12.2549532,Nagoya University,"Purpose of this paper is to present a method for visualising decision-reasoning regions in computer-aided pathological pattern diagnosis of endocytoscopic images. Endocytoscope enables us to perform direct observation of cells and their nuclei on the colon wall at maximum 500-times ultramagnification. For this new modality, computer-aided pathological diagnosis system is strongly required for the support of non-expert physicians. To develop a CAD system, we adopt convolutional neural network (CNN) as the classifier of endocytoscopic images. In addition to this classification function, based on CNN weights analysis, we develop a filter function that visualises decision-reasoning regions on classified images. This visualisation function helps novice endocytoscopists to develop their understanding of pathological pattern on endocytoscopic images for accurate endocytoscopic diagnosis. In numerical experiment, our CNN model achieved 90 % classification accuracy. Furthermore, experimental results show that decision-reasoning regions suggested by our filter function contain characteristic pit patterns in real endocytoscopic diagnosis.",,Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
73,Bladder Wall Segmentation using U-Net based Deep Learning,11314,,,"Ivanitskiy Michael,Hadjiiski Lubomir,Chan Heang-Ping,Samala Ravi,Cohan Richard H.,Caoili Elaine M.,Weizer Alon,Alva Ajjai,Wei Jun,Zhou Chuan","Ivanitskiy M,Hadjiiski L,Chan HP,Samala R,Cohan RH,Caoili EM,Weizer A,Alva A,Wei J,Zhou C",Ivanitskiy M,10.1117/12.2551343,University of Michigan System,"We are developing a deep learning based U-Net (U-DL) model for bladder wall segmentation in CT urography (CTU) as a component of a computer-assisted pipeline for bladder cancer detection and treatment response assessment. This task is challenging due to variations in size and shape of the wall among cases, low contrast between the bladder wall and surrounding structures, and some walls being extremely thin and occasionally invisible compared to the overall size of the bladder. Our previous method used a deep-learning convolution neural network and level sets (DCNN-LS) within a user-input bounding box. In the current study, we propose two new methods for bladder wall segmentation: 1) the outer and inner bladder wall contour masks are generated to train two different U-DLs and the segmented bladder regions are subtracted to obtain the final bladder wall; 2) a combined wall mask for the bladder wall is generated by subtracting the hand-outlined bladder inner and outer contour masks, and a single U-DL is trained to segment the bladder wall. The new methods use only U-Net without level-set post-processing. Hand-segmented contours from 67 training and 14 validation cases were used for this study. The combined wall mask training method in particular shows promise in improving both accuracy and reducing pipeline complexity.","Computer-Aided Diagnosis,Deep-Learning,Segmentation,CT Urography,Bladder",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
74,Cup-disc and retinal nerve fiber layer features fusion for diagnosis glaucoma,11314,,,"Kang Hong,Li Xiaoxing,Su Xiu","Kang H,Li XX,Su X",Li XX,10.1117/12.2548546,"Beijing Shanggong Med Technol Co LTd, Beijing 100000, Peoples R China.","Early detection of glaucoma is important for slowing disease progression and preventing total vision loss. The diagnosis of glaucoma is closely related to the shape of the optic disc and cup (cup-disc) and whether there is a defect in the retinal nerve fiber layer (RNFL). In previous studies, it was common to predict glaucoma by analyzing changes in cup-to-disc ratio, or to directly classify fundus images for glaucoma using a deep learning classification model. This paper proposes a method for diagnosing glaucoma by combining the cup-disc shape information and retinal nerve fiber layer defect (RNFLD) information. We use a fully convolutional neural network that based on a multi-scale attention mechanism (AM-CNN) to identify cup-disc morphology and RNFLD regions, further use previous methods and image processing methods to extract features in these two spaces. Finally, we use the SVM method in machine learning to classify the sample for glaucoma based on the features fusion of the two spaces. Specifically, we first establish a small database with both the cup-disc mark, retinal nerve fiber layer defect mark and glaucoma diagnosis results, which includes 735 fundus images labeled with either positive glaucoma (356) or negative glaucoma (379). Then, a semantic segmentation model based on attention is designed. By adding attention to the context information of the model, a more accurate segmentation image is obtained, not only has a good effect on the segmentation of the cup-disc, but also has a significant effect on the recognition of RNFLDs. Finally, the four-dimensional features were extracted from the cup-disc segmentation map by the previous method, and the four-dimensional features such as distance and area were extracted from the retinal nerve fiber layer segmentation map. Combine the two kinds of features using SVM algorithm to establish a classification model for glaucoma classification. The experiment results show that adding the attention module to the decoder can improve the effect of segmentation tasks for more complex problems and the classification model fusion cup-disc shape and RNFLD information significantly advances glaucoma detection.","computer aided diagnosis,fundus images,glaucoma,retinal nerve fiber layer defect,cup-disc segmentation,attention,fully convolutional network",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
75,Segmentation of Retinal Low-Cost Optical Coherence Tomography Images using Deep Learning,11314,,,"Kepp Timo,Sudkamp Helge,von der Burchard Claus,Schenke Hendrik,Koch Peter,Huettmann Gereon,Roider Johann,Heinrich Mattias P.,Handels Heinz","Kepp T,Sudkamp H,von der Burchard C,Schenke H,Koch P,Huttmann G,Roider J,Heinrich MP,Handels H",Kepp T,10.1117/12.2551324,University of Lubeck,"The treatment of age-related macular degeneration (AMD) requires continuous eye exams using optical coherence tomography (OCT). The need for treatment is determined by the presence or change of disease-specific OCT-based biomarkers. Therefore, the monitoring frequency has a significant influence on the success of AMD therapy. However, the monitoring frequency of current treatment schemes is not individually adapted to the patient and therefore often insufficient. While a higher monitoring frequency would have a positive effect on the success of treatment, in practice it can only be achieved with a home monitoring solution. One of the key requirements of a home monitoring OCT system is a computer-aided diagnosis to automatically detect and quantify pathological changes using specific OCT-based biomarkers. In this paper, for the first time, retinal scans of a novel self-examination low-cost full-field OCT (SELF-OCT) are segmented using a deep learning-based approach. A convolutional neural network (CNN) is utilized to segment the total retina as well as pigment epithelial detachments (PED). It is shown that the CNN-based approach can segment the retina with high accuracy, whereas the segmentation of the PED proves to be challenging. In addition, a convolutional denoising autoencoder (CDAE) refines the CNN prediction, which has previously learned retinal shape information. It is shown that the CDAE refinement can correct segmentation errors caused by artifacts in the OCT image.","AMD,Low-Cost OCT,Home Monitoring,Segmentation,Deep Learning,Shape Refinement",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,http://arxiv.org/pdf/2001.08480,
76,Weakly-supervised US breast tumor characterization and localization with a box convolution network,11314,,,"Kim Chanho,Kim Won Hwa,Kim Hye Jung,Kim Jaeil","Kim C,Kim WH,Kim HJ,Kim J",Kim J,10.1117/12.2549203,Kyungpook National University,"In US breast tumor diagnosis, machine learning approaches for the malignancy classification and the mass localization have been attracting many researchers to improve the diagnostic sensitivity and specificity while reducing the image interpretation time. Recently, fully-supervised deep learning methods showed their promising results in those tasks. However, the full supervision for the localization requires human efforts and time to annotate ground truth regions. In this paper, we present a weakly-supervised deep network which can localize breast masses in US images from only diagnostic labels (i.e., malignant and benign). Specifically, we exploit a flexible convolution method, which learns the size and offset of the convolution kernel, in the classification network to detect more relevant regions of breast masses against their various size and shape. Experimental results show that the proposed network outperform conventional CNN models, such as VGG-16 and VGG-16 with dilated convolution. The proposed model achieved 89.03% in the binary classification accuracy. To evaluate the localization performance with weakly-supervised manners, we also compared class activation maps for each instance with manual masks of breast mass in terms of the Dice similarity coefficient and localization recall. The experimental results also demonstrate that the deep network with the adjustable convolution layers can clinically relevant features of breast mass and its surrounding area for both benign and malignant cases.","Breast Cancer,Covolutional Neural Networks,Tumor Classification,Tumor Localization,Ultrasound Imaging,Weakly-supervised Learning",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,"ULTRASOUND,LESIONS",,,
77,Survival Prediction of Liver Cancer Patients from CT Images Using Deep Learning and Radiomic Feature-based Regression,11314,,,"Lee Hansang,Hong Helen,Seong Jinsil,Kim Jin Sung,Kim Junmo","Lee H,Hong H,Seong J,Kim JS,Kim J",Hong H,10.1117/12.2551349,Seoul Women's University,"Prediction of survival period for patients with hepatocellular carcinoma (HCC) provides important information for treatment planning such as radiotherapy. However, the task is known to be challenging due to the similarity of tumor imaging characteristics from patients with different survival periods. In this paper, we propose a survival prediction method using deep learning and radiomic features from CT images with support vector machine (SVM) regression. First, to extract the deep features, the convolutional neural network (CNN) is trained for the task of classifying the patients for 24-month survival. Second, the radiomic features including texture and shape are extracted from the patient images. After concatenating the radiomic features and the deep features, the SVM regressor is trained to predict the survival period of the patients. The experiment was performed on the CT scans of 171 HCC patients with 5-fold cross validation. In the experiments, the proposed method showed an accuracy of 86.5%, a root-mean-squared-error (RMSE) of 11.6, and a Spearman rank coefficient of 0.11. In comparisons with the deep feature-only- and radiomic feature-only regression results, the proposed method showed improved accuracy and RMSE than both, but lower rank coefficient than the radiomic feature-only regression. It can be observed that (1) the deep learning of CT images has a promising potential for predicting the survival period of HCC patients, and (2) the radiomic feature analysis provides useful information to strengthen the power of deep learning-based survival prediction.","Computed tomography,hepatocellular carcinoma,survival prediction,deep learning,radiomics",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,HEPATOCELLULAR-CARCINOMA,,,
78,Automatic detection of brain metastases using 3D mask R-CNN for stereotactic radiosurgery,11314,,,"Lei Yang,Tian Zhen,Kahn Shannon,Curran Walter J.,Liu Tian,Yang Xiaofeng","Lei Y,Tian Z,Kahn S,Curran WJ,Liu T,Yang XF",Yang XF,10.1117/12.2549860,Emory University,"Brain metastases are one of the most common neurologic complications of cancers, occurring in about 30% of all patients with cancer. Moreover, about 40% of brain metastases patients have more than three metastases. Stereotactic radiosurgery (SRS) is a well-established treatment for brain metastases, which requires accurate detection and delineation of the brain metastases. However, manually detecting and locating all the brain metastases can be very time-consuming and labor-intensive, which is a big efficiency bottleneck in this typical one-day outpatient SRS procedure. Developing a fast automatic detection tool of brain metastases is highly desirable, but is very challenging given the large number of brain metastases that a patient can have and the small size that a brain metastasis can be. In this work, we propose to use a 3D Mask R-CNN method to automatically and quickly detect the brain metastases on magnetic resonance (MR) images for SRS. At the training stage, coarse feature maps were extracted from 3D MR image patches using pretrained ResNet. Then, a region proposal network (RPN) was used to predict the locations and sizes of the rough candidate tumor ROIs from the coarse feature maps. By using a uniformed fully convolution network (FCN), the metastases within ROI was segmented. The segmentation loss, classification loss (metastases or non-metastases), as well as ROI location and size regression loss were used to supervise the proposed networks. For a new query patient, candidate ROIs and predicted probability maps within ROIs were obtained from our trained model. By aggregating ROIs and the tumor probability maps and performing a consolidation via weighted cluster scoring, the final ROIs of the brain metastases was obtained. We have tested our method on 20 patients' brain contrast T1-weighted MR images, and achieved 86.5%+/- 3.2% sensitivity and 89.7%+/- 4.8% specificity. For each patient, it took our trained model a few seconds to detect the brain metastases on the 3D MR images. The results of our preliminary study have demonstrated its efficacy and clinical feasibility. This auto-detection method could be a useful tool to significantly improve the efficiency of SRS treatment planning and hence ultimately improve the clinical outcome.","Multi-organ-at-risk segmentation,magnetic resonance imaging,3D Faster R-CNN",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,SEGMENTATION,,,
79,Automatic Kellgren-Lawrence grade estimation driven deep learning algorithms,11314,,,"Li Nianyi,Swiecicki Albert,Said Nicholas,O'Donnell Jonathan,Jiranek William A.,Mazurowski Maciej A.","Li NY,Swiecicki A,Said N,O'Donnell J,Jiranek WA,Mazurowski MA",Li NY,10.1117/12.2551392,Duke University,"Knee osteoarthritis (OA) is a prevalent and disabling degenerative joint disease. Objectively identifying knee OA severity is challenging given significant inter-reader variability due to human interpretation factors. The Kellgren-Lawrence (KL) grading system is a commonly used scale to quantitatively characterize the severity of knee OA in knee radiographs. It is important to reliably identify severe knee OA since total knee arthroplasty (TKA) can provide significant improvement in patient quality of life for patients with severe knee OA. In this study, we demonstrate a deep learning approach to automatically assessing KL grades. Our approach uses faster R-CNN object detection network to identify the knee region and deep convolutional neural network for classification. We used a dataset of 7962 knee radiographs for each posteroanterior (PA) and lateral (LAT) views, to develop and evaluate our approach. Images with their corresponding KL grades were obtained from the Multicenter Osteoarthritis Study (MOST) dataset. Our network showed multi-class classification accuracy of 69.15 % when the assessment was made based on PA views and accuracy of 56.68 % when LAT views were used. The developed network may play a significant role in surgical decision-making regarding knee replacement surgery.","deep learning,osteoarthritis,Kellgren-Lawrence,knee radiographs,Multicenter Osteoarthritis Study",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,OSTEOARTHRITIS,,,
80,Architectural distortion detection approach guided by mammary gland spatial pattern in digital breast tomosynthesis,11314,,,"Li Yue,Xie Zheng,He Zilong,Ma Xiangyuan,Guo Yanhui,Chen Weiguo,Lu Yao","Li Y,Xie Z,He ZL,Ma XY,Guo YH,Chen WG,Lu Y",Lu Y,10.1117/12.2549143,Sun Yat Sen University,"Architectural distortion (AD) is one of the most important potentially ominous signs of breast cancer. As a 3D imaging, digital breast tomosynthesis (DBT) is an accurate tool to detect AD. We developed a deep learning approach for AD detection guided by mammary gland spatial pattern (MGSP) in DBT. The approach consists of two stages: 2D detection and 3D aggregation. In 2D detection, prior MGSP information is obtained first. It includes 1) magnitude image and orientation field map produced from Gabor filters and 2) mammary gland convergence map. Second, Faster-RCNN detection network is employed. Region proposal network extracts features and determines locations of AD candidates and the soft classifier is used for reducing false positives. In 3D aggregation, a region fusion strategy is designed to fuse 2D candidates into 3D candidates. For evaluation, 265 DBT volumes (138 with ADs and 127 without any lesion) were collected from 68 patients. Free response receiver operating characteristic curve was obtained and the mean true positive fraction (MTPF) was used as the figure-of-merit of model performance. Compared with a baseline model based on convergence measure, the six-fold cross validation results showed that our proposed approach achieved MTPF of 0.50 +/- 0.04, while the baseline achieved 0.37 +/- 0.03. The improvement of our approach was statistically significant (p << 0.001).","Architectural distortion,digital breast tomosynthesis,computer aided detection,mammary gland spatial pattern,deep learning",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
81,Performance Investigation of Deep Learning vs. Classifier for Polyp Differentiation via Texture Features,11314,,,"Liang David,Wang David,Wei Alice,Choi Yeseul,Zhang Shu,Pomeroy Marc J.,Pickhardt Perry J.","Liang D,Wang D,Wei A,Choi Y,Zhang S,Pomeroy MJ,Pickhardt PJ",Zhang S,10.1117/12.2550007,State University of New York (SUNY) System,"Computer-aided diagnosis (CADx) of polyps is essential for advancing computed tomography colonography (CTC) with diagnostic capability. In this paper, we present a study of investigating the performance between deep learning and Random Forest (RF) classifier for polyp differentiation in CTC. First, we conducted feature extraction via an extended Haralick model (eHM) to build a total of 30 texture features. The gray level co-occurrence matrix (GLCM) is generated to encode 3D CT image information into a 2D matrix as input to the convolutional neural network (CNN). Then, we split the polyp classification into two state-of-the-art frameworks: the eHM texture features/RF and the GLCM texture matrices/CNN. We evaluated their performances by the merit of area under the curve of receiver operating characteristic using 1,278 polyps (confirmed by pathology). Results demonstrated that by balancing the data, both CNN model and RF classifier can learn or analyze features effectively, and achieve high performance. RF classifier in general outperformed CNN model with a gain of 6.4% (balanced datasets) and 5.4% (unbalanced datasets), showing its effective in feature extraction and analysis for polyp differentiation. However, the performance of CNN got improved through the addition of new data with a gain of 3.6% (balanced datasets) and 3.4% (unbalanced datasets), whereas RF classifier showed no gain when we enlarged datasets. This demonstrated that CNN model have the potential to improve the classification task performance when dealing with larger dataset This study provided valuable information on how to design experiments to improve CADx of polyps.","colon polyp,computer aided diagnosis,CT colonography,deep learning,machine learning,texture features",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,"COMPUTER-AIDED,DIAGNOSIS,CT,COLON",,,
82,EDICNet: An end-to-end detection and interpretable malignancy classification network for pulmonary nodules in computed tomography,11314,,,"Lin Yannan,Wei Leihao,Han Simon X.,Aberle Denise R.,Hsu William","Lin YN,Wei LH,Han SX,Aberle DR,Hsu W",Lin YN,10.1117/12.2551220,University of California System,"We present an interpretable end-to-end computer-aided detection and diagnosis tool for pulmonary nodules on computed tomography (CT) using deep learning-based methods. The proposed network consists of a nodule detector and a nodule malignancy classifier. We used RetinaNet to train a nodule detector using 7,607 slices containing 4,234 nodule annotations and validated it using 2,323 slices containing 1,454 nodule annotations drawn from the LIDC-IDRI dataset The average precision for the nodule class in the validation set reached 0.24 at an intersection over union (IoU) of 0.5. The trained nodule detector was externally validated using a UCLA dataset We then used a hierarchical semantic convolutional neural network (HSCNN) to classify whether a nodule was benign or malignant and generate semantic (radiologist-interpretable) features (e.g., mean diameter, consistency, margin), training the model on 149 cases with diagnostic CTs collected from the same UCLA dataset A total of 149 nodule -centered patches from the UCLA dataset were used to train the HSCNN. Using 5 -fold cross validation and data augmentation, the mean AUC and mean accuracy in the validation set for predicting nodule malignancy achieved 0.89 and 0.74, respectively. Meanwhile, the mean accuracy for predicting nodule mean diameter, consistency, and margin were 0.59, 0.74, and 0.75, respectively. We have developed an initial end-to -end pipeline that automatically detects nodules > 5 mm on CT studies and labels identified nodules with radiologist-interpreted features automatically.","computer-aided diagnosis,computed tomography,pulmonary nodule detection,pulmonary nodule classification,deep learning",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,"FALSE-POSITIVE,REDUCTION,LUNG-CANCER,IMAGES",,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7325481,
83,Robust Hepatic Vessels Segmentation Model Based on Noisy Dataset,11314,,,"Liu Li,Tian Jiang,Zhong Cheng,Shi Zhongchao,Xu Feiyu","Liu L,Tian J,Zhong C,Shi ZC,Xu FY",Liu L,10.1117/12.2551252,Legend Holdings,"Automatic hepatic vessel segmentation from computed tomography (CT) images is essential in computer-assisted liver surgery. However, because of the error-prone and time-consuming manual annotation, it is impractical to obtain the fully correct training labels of hepatic vein (HV) and highly branched portal vein (PV), which largely restricts the development of deep learning methods on hepatic vessel segmentation. To reduce the noise label interference, this paper builds a robust hepatic vessel segmentation model via analyzing the probability distribution relationship between noisy annotation labels and unobserved correct ones, and apply it to deep neural networks (DNNs). Meanwhile, for inferior vena cava (IVC) close to liver and PV in extrahepatic area, segmentation methods are also represented to enhance the completeness of hepatic vessel structure. Experiments, which are conducted on a public hepatic vessel dataset with noise interference, indicates that our model can decrease misclassified regions and increase the vessel recognition probability, simultaneously.","Hepatic Vessel Segmentation,Noisy Labels,Probability Distribution",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
84,Siamese neural networks for the classification of high-dimensional radiomic features,11314,,,"Mahajan Abhishaike,Dormer James,Li Qinmei,Chen Deji,Zhang Zhenfeng,Fei Baowei","Mahajan A,Dormer J,Li QM,Chen DJ,Zhang ZF,Fei BW",Fei BW,10.1117/12.2549389,University of Texas System,"This study demonstrates that a variant of a Siamese neural network architecture is more effective at classifying high-dimensional radiomic features (extracted from T2 MRI images) than traditional models, such as a Support Vector Machine or Discriminant Analysis. Ninety-nine female patients, between the ages of 20 and 48, were imaged with T2 MRI. Using biopsy pathology, the patients were separated into two groups: those with breast cancer (N=55) and those with GLM (N=44). Lesions were segmented by a trained radiologist and the ROIs were used for radiomic feature extraction. The radiomic features include 536 published features from Aerts et al., along with 20 features recurrent quantification analysis features. A Student T-Test was used to select features found to be statistically significant between the two patient groups. These features were then used to train a Siamese neural network. The label given to test features was the label of whichever class the test features with the highest percentile similarity within the training group. Within the two highest-dimensional feature sets, the Siamese network produced an AUC of 0.853 and 0.894, respectively. This is compared to best non-Siamese model, Discriminant Analysis, which produced an AUC of 0.823 and 0.836 for the two respective feature sets. However, when it came to the lower-dimensional recurrent features and the top-20 most significant features from Aerts et al., the Siamese network performed on-par or worse than the competing models. The proposed Siamese neural network architecture can outperform competing other models in high-dimensional, low-sample size spaces with regards to tabular data.","Neural Network,Siamese Network,Breast Cancer,Mastitis,Radiomics,Disease Classification,MRI,Machine Learning",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7288755,
85,Decision fusion of 3D convolutional neural networks to triage patients with suspected prostate cancer using volumetric biparametric MRI,11314,,,"Mehta Pritesh,Antonelli Michela,Ahmed Hashim,Emberton Mark,Punwani Shonit,Ourselin Sebastien","Mehta P,Antonelli M,Ahmed H,Emberton M,Punwani S,Ourselin S",Mehta P,10.1117/12.2547682,University of London,"In this work, we present a computer-aided diagnosis system that uses deep learning and decision fusion to classify patients into one of three classes: ""Likely Prostate Cancer"", ""Equivocal"" and ""Likely not Prostate Cancer"". We impose the group ""Equivocal"" to reduce misclassifications by allowing for uncertainty, akin to prostate imaging reporting systems used by radiologists. We trained 3D convolutional neural networks to perform two binary patient-level classification tasks: classification of patients with/without prostate cancer and classification of patients with/without clinically significant prostate cancer. Networks were trained separately using volumetric T2-weighted images and apparent diffusion coefficient maps for both tasks. The probabilistic outputs of the resulting four trained networks were combined using majority voting followed by the max operator to classify patients into one of the three classes mentioned above. All networks were trained using patient-level labels only, which is a key advantage of our system since voxel-level tumour annotation is often unavailable due to the time and effort required of a radiologist. Our system was evaluated by retrospective analysis on a previously collected trial dataset. At a higher sensitivity setting, our system achieved 0.97 sensitivity and 0.31 specificity compared to an experienced radiologist who achieved 0.99 sensitivity and 0.12 specificity. At a lower sensitivity setting, our system achieved 0.78 sensitivity and 0.77 specificity compared to 0.76 sensitivity and 0.77 specificity for the experienced radiologist. We envision our system acting as a second reader in pre-biopsy screening applications.","prostate cancer,multiparametric MRI,biparametric MRI,computer-aided diagnosis,convolutional neural network,decision fusion",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,"PICTURE,SYSTEM",,https://discovery.ucl.ac.uk/10101213/1/1131433.pdf,
86,Survey of Image Denoising Methods for Medical Image Classification,11314,,,"Michael Peter F.,Yoon Hong-Jun","Michael PF,Yoon HJ",Yoon HJ,10.1117/12.2549695,United States Department of Energy (DOE),"Medical imaging devices, such as X-ray machines, inherently produce images that suffer from visual noise. Our objectives were to (i.) determine the effect of image denoising on a medical image classification task, and (ii.) determine if there exists a correlation between image denoising performance and medical image classification performance. We performed the medical image classification task on chest X-rays using the DenseNet-121 convolutional neural network (CNN) and used the peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) metrics as the image denoising performance measures. We first found that different denoising methods can make a statistically significant difference in classification performance for select labels. We also found that denoising methods affect fine-tuned models more than randomly-initialized models and that fine-tuned models have significantly higher and more uniform performance than randomly-initialized models. Lastly, we found that there is no significant correlation between PSNR and SSIM values and classification performance for our task.","image denoising,image classification,X-ray image denoising,machine learning,deep learning",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,https://www.osti.gov/biblio/1648905,
87,MRI Image Harmonization using Cycle-Consistent Generative Adversarial Network,11314,,,"Modanwal Gourav,Vellal Adithya,Buda Mateusz,Mazurowski Maciej A.","Modanwal G,Vellal A,Buda M,Mazurowski MA",Mazurowski MA,10.1117/12.2551301,Duke University,"Dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) has proven to be a useful modality for evaluating breast abnormalities found in mammography and performing early disease detection in high-risk patients. However, radiological images generated by various vendors of MRI scanners (e.g., GE Healthcare & Siemens) vary greatly in terms of intensity and other image characteristics such as noise distribution. This is a challenge both for the evaluation of images by radiologists and for the computational analysis of images using radiomics or deep learning. For example, an algorithm trained on a set of images acquired by one MRI scanner may perform poorly on a dataset produced by a different scanner. Therefore, there is an urgent need for image harmonization. Traditional image to image translation algorithms can be used to solve this problem, but they require paired data (i.e., the same object imaged using different scanners). In this study, we utilize a deep learning algorithm that uses unpaired data to solve this problem through a bi-directional translation between MRI images. The proposed method is based on a cycle-consistent adversarial network (CycleGAN) that uses two generator-discriminator pairs. The original CycleGAN struggles in preserving the structure (i.e., breast tissue characteristics and shape) during the translation. To overcome this, we modified the discriminator architecture and forced the penalization based on the structure at the scale of smaller patches. This allows the network to focus more on features pertaining to breast tissue. The results demonstrate that the output images are visually realistic, preserve the structure and harmonize intensity across images from different scanners.","MRI,Intensity Harmonization,Medical Image Translation,Deep Learning,CycleGAN",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
88,Machine Learning-powered Prediction of Recurrence in Patients with Non-small Cell Lung Cancer Using Quantitative Clinical and Radiomic Biomarkers,11314,,,"Moon Sehwa,Choi Dahim,Lee Ji-Yeon,Kim Myoung Hee,Hong Helen,Kim Bong-Seog,Choi Jang-Hwan","Moon S,Choi D,Lee JY,Kim MH,Hong H,Kim BS,Choi JH",Moon S,10.1117/12.2549962,Ewha Womans University,"Lung cancer is a fatal disease, non-small cell lung cancer (NSCLC) being the most prevalent type. One of the main purposes of researching NSCLC is identifying patients at high risk for recurrence after surgical resection so that specific and suitable treatments can be found for them. The classification of cancer by anatomic disease extent, that is, by tumor-size (T stage) and nodal-involvement (N stage), is the most widely accepted determinant of appropriate treatment and prognosis among practicing clinicians. However, TN stage-based risk prediction can be inaccurate, as there is moderate observer variability when reporting the size of the lesion. Here, we propose a lung cancer recurrence prediction model using principal component analysis (PCA) and machine learning (ML) techniques and considering radiomic features and clinical data, including the TN stage. After being filtered by a statistical model, the principal components, including T and N-stage data and the handcrafted radiomic features from CT images, were applied to various ML models (i.e., random forests, support vector machines, naive Bayesian classifiers, and both boosting). We conducted this study, not only on recurrence, but also recurrence within two years of surgical resection, since more than 80% of recurrence occurs within this time frame. In both cases, the experimental results showed that combining radiomic features and clinical data improves the prediction of lung-cancer recurrence over that of models that only use TN stage data in terms of the 5-fold cross-validation accuracy mean, the receiver operating characteristic (ROC), the area under the ROC curve (AUC), and Kaplan-Meier curves. Finally, this model has been embedded in a website and is being prepared for the Ministry of Food and Drug Safety (MFDS) medical device registration and approval in South Korea.","non-small cell lung cancer,recurrence risk,machine learning,radiomics,hand-crafted features",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
89,Quality controlled segmentation to aid disease detection,11314,,,"Moradi Mehdi,Wong Ken C. L.,Karargyris Alexandros,Syeda-Mahmood Tanveer","Moradi M,Wong KCL,Karargyris A,Syeda-Mahmood T",Moradi M,10.1117/12.2549426,International Business Machines (IBM),"Basic deep learning classifiers used for medical images often produce global labels. NAThile annotation for localized disease detection might be costly, the knowledge of prevalence of conditions in different anatomical areas can help improve the accuracy by limiting the classifier to relevant, areas. However, this improvement provided by context knowledge, is usually offset by the errors of the segmentation map used to isolate the area of interest. This paper proposes a framework for disease classification consisting of a segmentation network, a segmentation quality assessment network, and two separate classifiers on whole image and relevant segmented area. The quality assessment network controls the impact of the two disease classifiers on the final outcome, utilizing the masked image Only When segmentation is acceptable. We show that in a very large dataset of chest X-ray i TH8geS, this framework produces a 2% increase the area under ROC curve for classification compared to a baseline.",,Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
90,Comparative performance of 3D machine-learning and deep-learning models in the detection of small polyps in dual-energy CT colonography,11314,,,"Nappi Janne J.,Uemura Tomoki,Kim Se Hyung,Kim Hyoungseop,Yoshida Hiroyuki","Nappi JJ,Uemura T,Kim SH,Kim H,Yoshida H",Nappi JJ; Yoshida H,10.1117/12.2549793,Harvard University,"Colorectal cancer is the second leading cause of cancer deaths worldwide. Computed tomographic colonography (CTC) can detect large colorectal polyps and cancers at a high sensitivity, whereas it can miss some of the smaller but still clinically significant 6 - 9 mm polyps. Dual-energy CTC (DE-CTC) can be used to provide more detailed information about scanned materials than does conventional single-energy CTC. We compared the classification performance of a 3D convolutional neural network (DenseNet) with those of four traditional 3D machine-learning models (AdaBoost, support vector machine, random forest, Bayesian neural network) and their cascade and ensemble classifier variants in the detection of small polyps in DE-CTC. Twenty patients with colonoscopy-confirmed polyps were examined by DE-CTC with a reduced one-day bowel preparation. The traditional machine-learning models were designed to identify polyps based on native radiomic dual-energy features of the DE-CTC image volumes. The performance of the machine-learning models was evaluated by use of the leave-one-patient-out method. The DenseNet was trained with a large independent external dataset of single-energy CTC cases and tested on blended image volumes of the DE-CTC cases. Although the DenseNet yielded the highest detection accuracy for typical polyps, AdaBoost and its cascade classifier variant yielded the highest overall polyp detection performance.","Machine learning,Deep learning,CT colonography,Colon and other gastrointestinal tract,Dual-energy CT,Convolutional neural network,Detection, characterization, diagnosis,Virtual colonoscopy",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,"MISS,RATE,PRINCIPLES,REDUCTION,CAD",,,
91,Prediction of MCI to AD Risk of Conversion Survival Models: qMRI vs CSF Measures and Cognitive Assessments,11314,,,"Orozco-Sanchez Jorge,Tamez-Pena Jose","Orozco-Sanchez J,Tamez-Pena J",Orozco-Sanchez J,10.1117/12.2549301,Tecnologico de Monterrey,"Several studies have found that different quantitative MRI (qMRI) measurements are associated with the presence of Alzheimer's disease. Cognitive Assessment scores, Apolipoprotein E4 (ApoE4) and cerebrospinal fluid (CSF) biomarkers are important factors associated with the risk of conversion from MCI to AD. Despite this awareness, the relationship of the qMRI measurements with the conversion rate and their effect in multivariate survival models that combine Radiomics, CSF, ApoE4 and Cognitive assessment is not known. The objective of this work was to evaluate the importance of each data source using several machine learning(ML) approaches that build Cox Survival models that combine cognitive assessments, CSF, ApoE4 and qMRI features. 321 features from 442 subjects from the ADNI study that converted from the MCI status to AD were used. ML methods were explored in a Cross-validation framework. Test results indicated that cognitive assessments plus qMRI data produce Cox survival models that are 92% concordant with the conversion time from MCI to AD, while CSF biomarkers did not have a mayor contribution on the final survival Model.","Radiomics,Survival,Cox Model,Machine Learning,Cross-validation",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,"DIAGNOSED,ALZHEIMERS-DISEASE,CEREBROSPINAL-FLUID,ATROPHY,RATES,BRAIN,ATROPHY,IMPAIRMENT,MRI,BIOMARKERS,VOLUME,PET,PROGRESSION",,,
92,Explainable AI for medical imaging: deep-learning CNN ensemble for classification of estrogen receptor status from breast MRI,11314,,,"Papanastasopoulos Zachary,Samala Ravi K.,Chan Heang-Ping,Hadjiiski Lubomir,Paramagul Chintana,Helvie Mark A.,Neal Colleen H.","Papanastasopoulos Z,Samala RK,Chan HP,Hadjiiski L,Paramagul C,Helvie MA,Neal CH",Papanastasopoulos Z,10.1117/12.2549298,University of Michigan System,"Deep-learning convolutional neural networks (DCNNs) are the most commonly used approach in medical image analysis tasks at present; however, they have largely been used as blackbox predictors, lacking explanation for the underlying reasons. Explainable artificial intelligence (XAI) is an emerging subfield of AI seeking to understand how models make their decisions. In this work, we applied XAI visualization to gain an insight into the features learned by a DCNN trained to classify estrogen receptor status (ER+ vs ER-) based on dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) of the breast. Our data set contained 1395 ER+ regions-of-interest (ROIs) and 729 ER- ROIs from 148 patients, each with a pre-contrast scan and a minimum of two post-contrast scans. We developed a novel transfer-trained dual-domain DCNN architecture derived from the AlexNet model trained on ImageNet data that received the spatial (across the volume) and dynamic (across the acquisition sequence) components of each DCE-MRI ROI as input. The network's performance was evaluated with the area under the receiver operating characteristic curve (AUC) from leave-one-case-out cross validation. To visualize the DCNN learning, we applied XAI techniques, including the Integrated Gradients attribution method and the SmoothGrad noise reduction algorithm, to the ROIs from the training set. We observed that our DCNN learned relevant features from the spatial and dynamic domains, but there were differences in the contributing features from the two domains We also visualized DCNN learning from irrelevant features resulting from pre-processing artifacts. These observations motivate new approaches to pre-processing our data and training our DCNN.","explainable artificial intelligence,deep learning,medical imaging,interpretable AI,breast cancer,magnetic resonance imaging,estrogen receptor,transfer learning",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
93,Multi-Resolution CNN for Brain Vessel Segmentation from Cerebrovascular Images of Intracranial Aneurysm: A Comparison of U-Net and DeepMedic,11314,,,"Patel Tatsat R.,Paliwal Nikhil,Jaiswal Prakhar,Waqas Muhammad,Mokin Maxim,Siddiqui Adnan H.,Meng Hui,Rai Rahul,Tutino Vincent","Patel TR,Paliwal N,Jaiswal P,Waqas M,Mokin M,Siddiqui AH,Meng H,Rai R,Tutino V",Patel TR,10.1117/12.2549761,State University of New York (SUNY) System,"Background: Vascular segmentation of cerebral vascular imaging is tedious and manual, hindering translation of image based computational tools for neurovascular disease (such as intracranial aneurysm) management. Current cerebrovascular segmentation techniques use classic model-based algorithms, but such algorithms are incapable of distinguishing vasculature from artifacts. Deep Learning, specifically the widely accepted U-Net architecture, could be an effective alternative to conventional approaches for cerebrovascular segmentation, but has been shown to perform poorly in segmentation of smaller yet critical vessels.
Methods: In this study, we present a methodology using a specialized convolutional neural network (CNN) architecture-DeepMedic-which uses multi-resolution inputs to enhance the field of view of the architecture, thereby enhancing the accuracy of segmentation of smaller vessels. To show the capability of this architecture, we collected and segmented a total of 100 digital subtraction angiography (DSA) images of cerebral vessels for training, internal validation, and testing (n=80, n=10, and n=10, respectively).
Results: The DeepMedic architecture yielded high performance with a Connectivity-Area-Length (CAL) of 0.84 +/- 0.07 and a dice similarity coefficient (DSC) of 0.94 +/- 0.02 in the independent testing cohort. This was better than U-Net optimized for the patch-size and %-overlap in predictions, which performed with a CAL of 0.79 +/- 0.06 and a DSC of 0.92 +/- 0.02. Notably, our work demonstrated that DeepMedic (CAL: 0.45 +/- 0.12) outperformed U-Net (CAL: 0.59 +/- 0.11) for segmentation of smaller vessels.
Conclusions: Our work showed DeepMedic performs better than the current state-of-the-art method for cerebrovascular segmentation. We hope this study begins to bring a high fidelity deep-learning based approach closer to clinical translation.",,Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
94,Deep Learning with Context Encoding for Semantic Brain Tumor Segmentation and Patient Survival Prediction,11314,,,"Pei Linmin,Vidyaratne Lasitha,Rahman Md Monibor,Iftekharuddin Khan M.","Pei LM,Vidyaratne L,Rahman MM,Iftekharuddin KM",Pei LM,10.1117/12.2550693,Old Dominion University,"One of the most challenging problems encountered in deep learning-based brain tumor segmentation models is the misclassification of tumor tissue classes due to the inherent imbalance in the class representation. Consequently, strong regularization methods are typically considered when training large-scale deep learning models for brain tumor segmentation to overcome undue bias towards representative tissue types. However, these regularization methods tend to be computationally exhaustive, and may not guarantee the learning of features representing all tumor tissue types that exist in the input MRI examples. Recent work in context encoding with deep CNN models have shown promise for semantic segmentation of natural scenes, with particular improvements in small object segmentation due to improved representative feature learning. Accordingly, we propose a novel, efficient 3DCNN based deep learning framework with context encoding for semantic brain tumor segmentation using multimodal magnetic resonance imaging (mMRI). The context encoding module in the proposed model enforces rich, class-dependent feature learning to improve the overall multi-label segmentation performance. We subsequently utilize context augmented features in a machine-learning based survival prediction pipeline to improve the prediction performance. The proposed method is evaluated using the publicly available 2019 Brain Tumor Segmentation (BraTS) and survival prediction challenge dataset. The results show that the proposed method significantly improves the tumor tissue segmentation performance and the overall survival prediction performance.","Brain tumor segmentation,convolutional neural network,context encoding,multimodal magnetic resonance imaging,survival prediction",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,MR-IMAGES,,https://digitalcommons.odu.edu/cgi/viewcontent.cgi?article=1293&context=ece_fac_pubs,
95,An extended-2D CNN for multiclass Alzheimer's Disease diagnosis through Structural MRI,11314,,,"Pereira Mariana,Fantini Irene,Lotufo Roberto,Rittner Leticia","Pereira M,Fantini I,Lotufo R,Rittner L",Pereira M,10.1117/12.2550753,Universidade Estadual de Campinas,"Current techniques trying to predict Alzheimer's disease at an early-stage explore the structural information of T1-weighted MR Images. Among these techniques, deep convolutional neural network (CNN) is the most promising since it has been successfully used in a variety of medical imaging problems. However, the majority of works on Alzheimer's Disease tackle the binary classification problem only, i.e., to distinguish Normal Controls from Alzheimer's Disease patients. Only a few works deal with the multiclass problem, namely, patient classification into one of the three groups: Normal Control (NC), Alzheimer's Disease (AD) or Mild Cognitive Impairment (MCI). In this paper, our primary goal is to tackle the 3-class AD classification problem using T1-weighted MRI and a 2D CNN approach. We used the first two layers of ResNet34 as feature extractor and then trained a classifier using 64 x 64 sized patches from coronal 2D MRI slices. Our extended-2D CNN proposal explores the MRI volumetric information, by using non-consecutive 2D slices as input channels of the CNN, while maintaining the low computational costs associated with a 2D approach. The proposed model, trained and tested on images from ADNI dataset, achieved an accuracy of 68.6% for the multiclass problem, presenting the best performance when compared to state-of-the-art AD classification methods, even the 3D-CNN based ones.","Deep Learning,Convolutional Neural Network,Alzheimer's disease,Magnetic Resonance Image",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,"COGNITIVE,IMPAIRMENT,PREDICTION,CONVERSION,DEMENTIA",,,
96,Investigation of the accuracy of classifying coronary artery disease severity using machine learning with subdomain analysis of Fractional Flow Reserve diagnosis in patients,11314,,,"Podgorsak Alexander R.,Sommer Kelsey,Iyer Dvijay,Wilson Dmichael F.,Rybicki Frank J.,Mitsouras Dimitrios,Sharma Umesh,Kumamaru Kanako K.,Angel Erin,Ionita Ciprian N.","Podgorsak AR,Sommer K,Iyer D,Wilson DF,Rybicki FJ,Mitsouras D,Sharma U,Kumamaru KK,Angel E,Ionita CN",Podgorsak AR,10.1117/12.2549698,State University of New York (SUNY) System,"Coronary artery disease (CAD) is a condition where there is blood-flow reduction in the coronary artery due to plaque build-up. The current standard to diagnose CAD severity is fractional flow reserve (FFR) using the ratio of distal and proximal stenotic pressure measurements. This work investigated the use of a machine-learning classifier of CAD severity. Sixty-four coronary CT angiographies (CCTA) were collected at 70% through the cardiac R-R cycle. Eight straightened curved planar reformations (SCPRs) were reconstructed from each CCTA considering 45 increments around the coronary artery centerline. FFR measurements were considered ground truth to train a convolutional neural network to predict CAD severity based on the 0.80 FFR threshold. Classification accuracy and area under the receiver operating characteristic curve (AUROC) were used to assess the network's predictive capacity. SCPR data were optimized using class-activation maps, and the network was re-trained and assessed in the same manner. Subgroup analysis of the network's performance was carried out considering different coronary artery branches and patient FFR measurements in and out of the FFR grey-zone. Different network input conditions were assessed such as SCPR slice-thickness and SCPR reconstruction using the minimum or average value across the vessel centerline. Network for CAD severity prediction was significantly higher (P<0.05) using thicker SCPR slices. No significant difference was found in network performance using SCPRs from different coronary artery branches, or considering SCPR reconstruction using the minimum or average value. This work indicates that a CNN can predict CAD severity using coronary artery SCPRs.","Coronary artery disease,fractional flow reserve,coronary CT angiography,machine learning,convolutional neural network,straightened curved planar reformation,class-activation maps",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,"ENDOVASCULAR,PROCEDURES,MEDICAL,THERAPY,GRAY,ZONE,ANGIOGRAPHY,PREVENTION,FAME,PCI",,,
97,Comparison of CNN architectures and training strategies for quantitative analysis of idiopathic interstitial pneumonia,11314,,,"Rennotte Simon,Brillet Pierre-Yves,Fetita Catalin","Rennotte S,Brillet PY,Fetita C",Rennotte S,10.1117/12.2548476,IMT - Institut Mines-Telecom,"Fibrosing idiopathic interstitial pneumonia (IIP) is a subclass of interstitial lung diseases manifesting as progressive worsening of lung function. Such degradation is a continuous and irreversible process which requires quantitative follow-up of patients to assess the pathology occurrence and extent in the lung. The development of automated CAD tools for such purpose is oriented today towards machine learning approaches and in particular convolutional neural networks. The difficulty remains in the choice of the network architecture that best fit to the problem, in straight relationship with available databases for training. We follow-up our work on lung texture analysis and investigate different CNN architectures and training strategies in the context of a limited database, with high class imbalance and subjective and partial annotations. We show that increased performances are achieved using an end-to-end architecture versus patch-based, but also that naive implementation in the former case should be avoided. The proposed solution is able to leverage global information in the scan and shows a high improvement in the F1 scores of the predicted classes and visual results of predictions in better accordance with the radiologist expectations.","infiltrative lung diseases,fibrosing idiopathic interstitial pneumonia,lung texture classification,convolutional neural networks,deep learning,locally connected filters,UNet",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,"COMPUTED-TOMOGRAPHY,PULMONARY-FIBROSIS,DIAGNOSIS",,,
98,Generative Synthetic Adversarial Network for Internal Bias Correction and Handling Class Imbalance Problem in Medical Image Diagnosis,11314,,,"Rezaei Mina,Uemura Tomoki,Nappi Janne,Yoshida Hiroyuki,Lippert Christoph,Meinel Christoph","Rezaei M,Uemura T,Nappi J,Yoshida H,Lippert C,Meinel C",Rezaei M,10.1117/12.2551166,"Hasso Plattner Inst, Prof Dr Helmert St 2-3, Berlin, Germany.","Imbalanced training data introduce important challenge into medical image analysis where a majority of the data belongs to a normal class and only few samples belong to abnormal classes. We propose to mitigate the class imbalance problem by introducing two generative adversarial network (GAN) architectures for class minority oversampling. Here, we explore balancing data distribution 1) by generating new sample from unsupervised GAN or 2) synthesize missing image modalities from semi-supervised GAN. We evaluated the effect of the synthetic unsupervised and semi-supervised GAN methods by use of 1,500 MR images for brain disease diagnosis, where the classification performance of a residual network was compared between unbalanced datasets, classic data augmentation, and the proposed new GAN-based methods.The evaluation results showed that the synthesized minority samples generated by GAN improved classification accuracy up to 18% in term of Dice score.","Imbalanced Learning,Synthetic Medical Imaging,GANs,Multi-Class Classification",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
99,A Data-driven Approach for Stratifying Psychotic and Mood Disorders Subjects Using Structural Magnitude Resonance Imaging Data,11314,,,"Rokham Hooman,Falakshahi Haleh,Calhoun Vince D.","Rokham H,Falakshahi H,Calhoun VD",Rokham H,10.1117/12.2549680,University System of Georgia,"Psychotic disorders such as schizophrenia and bipolar disorder are difficult to classify because they share overlapping symptoms. Deriving biomarkers of illness using structural MRI dataset are essential because they may lead to improved diagnosis. Previous studies typically predict the diagnosis labels using supervised classifiers that rely on truly labeled dataset Mislabeled subjects may increase the complexity of the predictive model and may impact its performance. In this work, we address the problem of inaccurate diagnosis labeling of psychotic disorders using a data-driven approach. We performed dimension reduction using PCA on the vectorized images and then k-mean clustering on the components. We evaluate our method on a structural MRI dataset, with over 900 subjects labeled using DSM-IV and biotypes. An ANOVA statistical significance test was performed after clustering based on each labelling approach and after clustering. Subjects were grouped into 5 clusters using our method, and each cluster includes all types of patients. However, we found statistically significant group differences in brain regions across 5 clusters, while for DSM and biotype, there were no significant differences. Our results also show the performance of the predictive model improved significantly using data-driven labels. Our method shows underlying biological changes associated with mental illness may be identified by studying and considering features of the brain imaging data, and annotating brain imaging data using a data-driven approach may eventually lead to improved diagnosis and advanced drug discovery and help patients.","psychosis disorder,mood disorder,data-driven,clustering,structural MRI,bipolar,schizophrenia,schizoaffective",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,"BIPOLAR-SCHIZOPHRENIA,NETWORK,CLASSIFICATION,MRI",,,
100,Weakly Supervised 3D Classification of Chest CT using Aggregated Multi-Resolution Deep Segmentation Features,11314,,,"Saha Anindo,Tushar Fakrul I.,Faryna Khrystyna,D'Anniballe Vincent M.,Hou Rui,Mazurowski Maciej A.,Rubin Geoffrey D.,Lo Joseph Y.","Saha A,Tushar FI,Faryna K,D'Anniballe VM,Hou R,Mazurowski MA,Rubin GD,Lo JY",Saha A,10.1117/12.2550857,Universitat de Girona,"Weakly supervised disease classification of Cl imaging suffers from poor localization Owing to case-level annotations, where even a positive scan can hold hundreds to thousands of negative slices along multiple planes. Furthermore, although deep learning segmentation and classification models extract distinctly unique combinations of anatomical features from the same target class(es), they are typically seen as two independent processes in a computer-aided diagnosis (CAD) pipeline, with little to no feature reuse. In this research, we propose a medical classifier that leverages the semantic structural concepts learned via multi-resolution segmentation feature maps, to guide weakly supervised 3D classification of chest CT volumes. Additionally, a comparative analysis is drawn across two different types of feature aggregation to explore the vast possibilities surrounding feature fusion. Using a dataset of 1593 scans labeled on a case-level basis via rule-based model, we train a dual-stage convolutional neural network (CNN) to perform organ segmentation and binary classification of four representative diseases (emphysema, pneumonia/atelectasis, mass and nodules) in lungs. The baseline model, with separate stages for segmentation and classification, results in AUC of 0.791. Using identical hyperparameters, the connected architecture using static and dynamic feature aggregation improves performance to AUC of 0.832 and 0.851, respectively. This study advances the field in two key ways. First, case-level report data is used to weakly supervise a 3D CT classifier of multiple, simultaneous diseases for an organ. Second, segmentation and classification models are connected with two different feature aggregation strategies to enhance the classification performance.","CT,convolutional neural network,weak supervision,feature aggregation,3D classification,lungs",Proceedings Paper,"SPIE-INT SOC OPTICAL ENGINEERING, 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA","Engineering,Optics,Radiology, Nuclear Medicine & Medical Imaging",,,,,http://arxiv.org/pdf/2011.00149,
