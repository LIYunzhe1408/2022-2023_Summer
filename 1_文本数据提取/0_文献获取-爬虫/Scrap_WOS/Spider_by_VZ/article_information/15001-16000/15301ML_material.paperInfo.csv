,title,Volume,Issue,Pages,whole__author_name,simply_author_name,reprint author,DOI,reprint address,Abstract,Keywords,Document Type,Publisher,Research Domain,Published Date,impact_factor,Keywords_plus,joural,pdf_link,Download_SuccessOrDefeat
1,ESTIMATING REPRODUCIBLE FUNCTIONAL NETWORKS ASSOCIATED WITH TASK DYNAMICS USING UNSUPERVISED LSTMS,,,1395-1398,"Dvornek Nicha C.,Ventola Pamela,Duncan James S.","Dvornek NC,Ventola P,Duncan JS",Dvornek NC,,Yale University,"We propose a method for estimating more reproducible functional networks that are more strongly associated with dynamic task activity by using recurrent neural networks with long short term memory (LSTMs). The LSTM model is trained in an unsupervised manner to learn to generate the functional magnetic resonance imaging (fMRI) time-series data in regions of interest. The learned functional networks can then be used for further analysis, e.g., correlation analysis to determine functional networks that are strongly associated with an fMRI task paradigm. We test our approach and compare to other methods for decomposing functional networks from fMRI activity on 2 related but separate datasets that employ a biological motion perception task. We demonstrate that the functional networks learned by the LSTM model are more strongly associated with the task activity and dynamics compared to other approaches. Furthermore, the patterns of network association are more closely replicated across subjects within the same dataset as well as across datasets. More reproducible functional networks are essential for better characterizing the neural correlates of a target task.","Functional Networks,Task fMRI,Recurrent Neural Networks,Unsupervised Learning",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,http://arxiv.org/pdf/2105.02869,
2,OPTIMIZE CNN MODEL FOR FMRI SIGNAL CLASSIFICATION VIA ADANET-BASED NEURAL ARCHITECTURE SEARCH,,,1399-1403,"Dai Haixing,Ge Fangfei,Li Qing,Zhang Wei,Liu Tianming","Dai HX,Ge FF,Li Q,Zhang W,Liu TM",Dai HX,,University System of Georgia,"Recent studies showed that convolutional neural network (CNN) models possess remarkable capability of differentiating and characterizing fMRI signals from cortical gyri and sulci. In addition, visualization and analysis of the filters in the learned CNN models suggest that sulcal fMRI signals are more diverse and have higher frequency than gyral signals. However, it is not clear whether the gyral fMRI signals can be further divided into sub-populations, e.g., 3-hinge areas vs 2-hinge areas. It is also unclear whether the CNN models of two classes (gyral vs sulcal) classification can be further optimized for three classes (3-hinge gyral vs 2-hinge gyral vs sulcal) classification. To answer these questions, in this paper, we employed the AdaNet framework to design a neural architecture search (NAS) system for optimizing CNN models for three classes fMRI signal classification. The core idea is that AdaNet adaptively learns both the optimal structure of the CNN network and its weights so that the learnt CNN model can effectively extract discriminative features that maximize the classification accuracies of three classes of 3-hinge gyral, 2-hinge gyral and sulcal fMRI signals. We evaluated our framework on the Autism Brain Imaging Data Exchange ( ABIDE) dataset, and experiments showed that our framework can obtained significantly better results, in terms of both classification accuracy and extracted features.","Neural Architecture Search,Auto ML,CNN,3-hinge,2-hinge",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
3,A NOVEL FRAMEWORK FOR GRADING AUTISM SEVERITY USING TASK-BASED FMRI,,,1404-1407,"Haweel Reem,Dekhil Omar,Shalaby Ahmed,Mahmoud Ali,Ghazal Mohammed,Khalil Ashraf,Keynton Robert,Barnes Gregory,El-Baz Ayman","Haweel R,Dekhil O,Shalaby A,Mahmoud A,Ghazal M,Khalil A,Keynton R,Barnes G,El-Baz A",Haweel R,,University of Louisville,"Autism is a developmental disorder associated with difficulties in communication and social interaction. Currently, the gold standard in autism diagnosis is the autism diagnostic observation schedule (ADOS) interviews that assign a score indicating the level of severity for each individual. However, current researchers investigate developing objective technologies to diagnose autism employing brain image modalities. One of such image modalities is task-based functional MRI which exhibits alterations in functional activity that is believed to be important in explaining autism causative factors. Although autism is defined over a wide spectrum, previous diagnosis approaches only divide subjects into normal or autistic. In this paper, a novel framework for grading the severity level of autistic subjects using task-based fMRI data is presented. A speech experiment is used to obtain local features related to the functional activity of the brain. According to ADOS reports, the adopted dataset of 39 subjects is classified to three groups (13 subjects per group): mild, moderate and severe. Individual analysis with the general linear model (GLM) is used for feature extraction for each 246 brain areas according to the Brainnetome atlas (BNT). Our classification results are obtained by random forest classifier after recursive feature elimination (RFE) with 72% accuracy. Finally, we validate our selected features by applying higher level group analysis to prove how informative they are and to infer the significant statistical differences between groups","Task-based fMRI,ASD,FSL,Machine learning",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
4,MULTI-CONTRAST MR RECONSTRUCTION WITH ENHANCED DENOISING AUTOENCODER PRIOR LEARNING,,,1432-1436,"Liu Xiangshun,Zhang Minghui,Liu Qiegen,Xiao Taohui,Zheng Hairong,Ying Leslie,Wang Shanshan","Liu XS,Zhang MH,Liu QG,Xiao TH,Zheng HR,Ying LL,Wang SS",Wang SS,,Chinese Academy of Sciences,"This paper proposes an enhanced denoising autoencoder prior (EDAEP) learning framework for accurate multi-contrast MR image reconstruction. A multi-model structure with various noise levels is designed to capture features of different scales from different contrast images. Furthermore, a weighted aggregation strategy is proposed to balance the impact of different model outputs, making the performance of the proposed model more robust and stable while facing noise attacks. The model was trained to handle three different sampling patterns and different acceleration factors on two public datasets. Results demonstrate that our proposed method can improve the quality of reconstructed images and outperform the previous state-of-the-art approaches. The code is available at https://github.com/yqx7150.","Multi-contrast MR reconstruction,Autoencoder prior,deep learning",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,"RESONANCE,IMAGE-RECONSTRUCTION",,,
5,BENCHMARKING DEEP NETS MRI RECONSTRUCTION MODELS ON THE FASTMRI PUBLICLY AVAILABLE DATASET,,,1441-1445,"Ramzi Zaccharie,Ciuciu Philippe,Starck Jean-Luc","Ramzi Z,Ciuciu P,Starck JL",Ramzi Z,,CEA,"The MRI reconstruction field lacked a proper data set that allowed for reproducible results on real raw data (i.e. complex-valued), especially when it comes to deep learning (DL) methods as this kind of approaches require much more data than classical Compressed Sensing (CS) reconstruction. This lack is now filled by the fastMRI data set, and it is needed to evaluate recent DL models on this benchmark. Besides, these networks are written in different frameworks and repositories (if publicly available), it is therefore needed to have a common tool, publicly available, allowing a reproducible benchmark of the different methods and ease of building new models. We provide such a tool that allows the benchmark of different reconstruction deep learning models.","MRI reconstruction,Deep learning,fastMRI",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,NETWORKS,,https://hal.inria.fr/hal-02436223/document,
6,DeepMRS: An End-to-End Deep Neural Network for Dementia Disease Detection using MRS Data,,,1459-1463,"Ben Ahmed Olfa,Fezzani Seifeddine,Guillevin Carole,Fezai Lobna,Naudin Mathieu,Gianelli Benoit,Fernandez-Maloigne Christine","Ben Ahmed O,Fezzani S,Guillevin C,Fezai L,Naudin M,Gianelli B,Fernandez-Maloigne C",Ben Ahmed O,,Universite de Poitiers,"Alzheimer's disease (AD) is the most common form of dementia. Neuroimaging data is an integral part of the clinical assessment providing a way for clinicians to detect brain abnormalities for AD diagnosis. Anatomical MRI has been widely used to assess structural brain atrophy for AD detection and prediction. In addition to structural changes, metabolic changes in some brain regions such as the Posterior Cingulate Cortex (PCC) could be a good bio-marker for an early AD detection. Recently, proton Magnetic Resonance Spectroscopy (1H-MRS) have been proved to be effective to reveal a wealth of brain metabolic information. In this paper, we propose an end-to-end deep leaning Network for early AD and Normal Control (NC) subjects classification using 1H-MRS raw data from the PCC area. This work is the first investigation of 1H-MRS data with deep-learning technique for early AD detection. Data of 135 subjects, collected in Poitiers university hospital, are used to learn the proposed DeepMRS network. Our classification of patients with early AD versus NC subjects achieves an AUC of 94; 74%, a sensitivity of 100% and a specificity of 89; 47% demonstrating a promising early dementia detection performance.","Alzheimer's disease,deep learning,MRS,computer-aided diagnosis",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,"MAGNETIC-RESONANCE-SPECTROSCOPY,MILD,COGNITIVE,IMPAIRMENT,ALZHEIMERS-DISEASE,DIAGNOSIS,IMAGES",,,
7,EFFICIENT DETECTION OF EMVI IN RECTAL CANCER VIA RICHER CONTEXT INFORMATION AND FEATURE FUSION,,,1464-1468,"Li Shuai,Zhang Zhengdong,Lu Yun","Li S,Zhang ZD,Lu Y",Zhang ZD,,Beihang University,"It is vital to automatically detect the Extramural Vascular Invasion (EMVI) in rectal cancer before surgery, which facilitates to guide the patient's treatment planning. Nevertheless, there are few studies about EMVI detection through magnetic resonance imaging (MRI). Moreover, since EMVI has three main characteristics: highly-variable appearances, relatively-small sizes and similar shapes with surrounding tissues, current deep learning based methods can not be directly used. In this paper, we propose a novel and efficient EMVI detection framework, which gives rise to three main contributions. Firstly, we introduce a self-attention module to capture dependencies ranging from local to global. Secondly, we design a parallel atrous convolution (PAC) block and a global pyramid pooling (GPP) module to encode richer context information at multiple scales. Thirdly, we fuse the whole-scene and local-region information together to improve the feature representation ability. Experimental results show that our framework can significantly improve the detection accuracy and outperform other state-of-the-art methods.","Rectal cancer,EMVI detection,Richer Context information,Feature fusion,Deep learning",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,INVASION,,,
8,DEEP LEARNING BASED DETECTION OF ACUTE AORTIC SYNDROME IN CONTRAST CT IMAGES,,,1474-1477,"Yellapragada Manikanta Srikar,Xie Yiting,Graf Benedikt,Richmond David,Krishnan Arun,Sitek Arkadiusz","Yellapragada MS,Xie YT,Graf B,Richmond D,Krishnan A,Sitek A",Yellapragada MS,,New York University,"Acute aortic syndrome (AAS) is a group of life threatening conditions of the aorta. We have developed an end-to-end automatic approach to detect AAS in computed tomography (CT) images. Our approach consists of two steps. At first, we extract N cross sections along the segmented aorta centerline for each CT scan. These cross sections are stacked together to form a new volume which is then classified using two different classifiers, a 3D convolutional neural network (3D CNN) and a multiple instance learning (MIL). We trained, validated, and compared two models on 2291 contrast CT volumes. We tested on a set aside cohort of 230 normal and 50 positive CT volumes. Our models detected AAS with an Area under Receiver Operating Characteristic curve (AUC) of 0.965 and 0.985 using 3DCNN and MIL, respectively.","Acute aortic syndrome,dissection,CT imaging,deep learning,radiology reports",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,DISSECTION,,http://arxiv.org/pdf/2004.01648,
9,SEMI-SUPERVISED MULTI-DOMAIN MULTI-TASK TRAINING FOR METASTATIC COLON LYMPH NODE DIAGNOSIS FROM ABDOMINAL CT,,,1478-1481,"Glaser Saskia,Maicas Gabriel,Bedrikovetski Sergei,Sammour Tank,Carneiro Gustavo","Glaser S,Maicas G,Bedrikovetski S,Sammour T,Carneiro G",Carneiro G,,University of Adelaide,"The diagnosis of the presence of metastatic lymph nodes from abdominal computed tomography (CT) scans is an essential task performed by radiologists to guide radiation and chemotherapy treatment. State-of-the-art deep learning classifiers trained for this task usually rely on a training set containing CT volumes and their respective image-level (i.e., global) annotation. However, the lack of annotations for the localisation of the regions of interest (ROIs) containing lymph nodes can limit classification accuracy due to the small size of the relevant ROIs in this problem. The use of lymph node ROIs together with global annotations in a multi-task training process has the potential to improve classification accuracy, but the high cost involved in obtaining the ROI annotation for the same samples that have global annotations is a roadblock for this alternative. We address this limitation by introducing a new training strategy from two data sets: one containing the global annotations, and another (publicly available) containing only the lymph node ROI localisation. We term our new strategy semi-supervised multi-domain multi-task training, where the goal is to improve the diagnosis accuracy on the globally annotated data set by incorporating the ROI annotations from a different domain. Using a private data set containing global annotations and a public data set containing lymph node ROI localisation, we show that our proposed training mechanism improves the area under the ROC curve for the classification task compared to several training method baselines.","semi-supervised,multi-task,multiple domain,ROI annotations,weak annotations,colon cancer,lymph node diagnosis,abdominal CT",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,http://arxiv.org/pdf/1910.10371,
10,COMPUTER AIDED DIAGNOSIS OF CLINICALLY SIGNIFICANT PROSTATE CANCER IN LOW-RISK PATIENTS ON MULTI-PARAMETRIC MR IMAGES USING DEEP LEARNING,,,1482-1485,"Arif Muhammad,Schoots Ivo G.,Castillo Jose M. T.,Roobol Monique J.,Niessen Wiro,Veenland Jifke F.","Arif M,Schoots IG,Castillo TJM,Roobol MJ,Niessen W,Veenland JF",Arif M,,Erasmus University Rotterdam,"The purpose of this study was to develop a quantitative method for detection and segmentation of clinically significant (ISUP grade >= 2) prostate cancer (PCa) in low-risk patient. A consecutive cohort of 356 patients ( active surveillance) was selected and divided in two groups: 1) MRI and targeted-biopsy positive PCa, 2) MRI and standard-biopsy negative PCa. A 3D convolutional neural network was trained in three-fold cross validation with MRI and targeted-biopsy positive patient's data using two mp-MRI sequences (T2-weighted, DWI-b800) and ADC map as input. After training, the model was tested on separate positive and negative patients to evaluate the performance. The model achieved an average area under the curve (AUC) of the receiver operating characteristics is 0.78 (sensitivity = 85%, specificity = 72%). The diagnostic performance of the proposed method in segmenting significant PCa and to conform non-significant PCa in low-risk patients is characterized by a good AUC and negative-predictive-value.","Prostate Cancer,Segmentation,Active Surveillance,Convolutional Neural Network,Multi-parametric MRI",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
11,DEEP LEARNING FEATURES FOR MODELING PERCEPTUAL SIMILARITY IN MICROCALCIFICATION LESION RETRIEVAL,,,1486-1489,"Wang Juan,Lei Liang,Yang Yongyi","Wang J,Lei L,Yang YY",Wang J,,Illinois Institute of Technology,"Retrieving cases with similar image features has been found to be effective for improving the diagnostic accuracy of microcalcification (MC) lesions in mammograms. However, a major challenge in such an image-retrieval approach is how to determine a retrieved lesion image has diagnostically similar features to that of a query case. We investigate the feasibility of modeling perceptually similar MC lesions by using deep learning features extracted from two types of deep neural networks, of which one is a supervised-learning network developed for the task of MC detection and the other is a denoising autoencoder network. In the experiments, the deep learning features were compared against the perceptual similarity scores collected from a reader study on 1,000 MC lesion image pairs. The results indicate that the deep learning features can potentially be more effective for modelling the notion of perceptual similarity of MC lesions than traditional handcrafted texture features.","Clustered microcalcifications,content-based image retrieval,deep learning features",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
12,AF-SEG: AN ANNOTATION-FREE APPROACH FOR IMAGE SEGMENTATION BY SELF-SUPERVISION AND GENERATIVE ADVERSARIAL NETWORK,,,1503-1507,"Yu Fei,Dong Hexin,Zhang Mo,Zhao Jie,Dong Bin,Li Quanzheng,Zhang Li","Yu F,Dong HX,Zhang M,Zhao J,Dong B,Li QZ,Zhang L",Zhang L,,Peking University,"Traditional segmentation methods are annotation-free but usually produce unsatisfactory results. The latest leading deep learning methods improve the results but require expensive and time-consuming pixel-level manual annotations. In this work, we propose a novel method based on self-supervision and generative adversarial network (GAN), which has high performance and requires no manual annotations. First, we perform traditional segmentation methods to obtain coarse segmentation. Then, we use GAN to generate a synthetic image, on which the image foreground is pixel-to-pixel corresponding to the coarse segmentation. Finally, we train the segmentation model with the data pairs of synthetic images and coarse segmentations. We evaluate our method on two types of segmentation tasks, including red blood cell (RBC) segmentation on microscope images and vessel segmentation on digital subtraction angiographies (DSA). The results show that our annotation-free method provides a considerable improvement over the traditional methods and achieves comparable accuracies with fully supervised methods.","Image segmentation,Generative adversarial network,Annotation free,Deep learning",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
13,MITIGATING ADVERSARIAL ATTACKS ON MEDICAL IMAGE UNDERSTANDING SYSTEMS,,,1517-1521,"Paul Rahul,Schabath Matthew,Gillies Robert,Hall Lawrence,Goldgof Dmitry","Paul R,Schabath M,Gillies R,Hall L,Goldgof D",Paul R,,State University System of Florida,"Deep learning systems are now being widely used to analyze lung cancer. However, recent work has shown a deep learning system can be easily fooled by intentionally adding some noise in the image. This is called as Adversarial attack. This paper presents an adversarial attack for malignancy prediction of lung nodules. We found that the adversarial attack can cause significant changes in lung nodule malignancy prediction accuracy. An ensemble-based defense strategy was developed to reduce the effect of an adversarial attack. A multi-initialization based CNN ensemble was utilized. We also explored adding adversarial images in the training set, which eventually reduced the rate of mis-classification and made the CNN models more robust to an adversarial attack. A subset of cases from the National Lung Screening Trial (NLST) dataset were used in our study. Initially, 75.1%, 75.5% and 76% classification accuracy were obtained from the three CNNs on original images (without an adversarial attack). Fast Gradient Sign Method (FGSM) and one-pixel attacks were analyzed. After the FGSM attack, 46.4%, 39.24%, and 39.71% accuracy was obtained from the 3 CNNs. Whereas, after a one pixel attack 72.15%, 73%, and 73% classification accuracy was achieved. FGSM caused much more damaged to CNN prediction. With a multi-initialization based ensemble and including adversarial images in the training set, 82.27% and 81.43% classification accuracy were attained after FGSM and one-pixel attacks respectively.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
14,USING TRANSFER LEARNING AND CLASS ACTIVATION MAPS SUPPORTING DETECTION AND LOCALIZATION OF FEMORAL FRACTURES ON ANTEROPOSTERIOR RADIOGRAPHS,,,1526-1529,"Gupta Vikash,Demirer Mutlu,Bigelow Matthew,Yu Sarah M.,Yu Joseph S.,Prevedello Luciano M.,White Richard D.,Erdal Barbaros S.","Gupta V,Demirer M,Bigelow M,Yu SM,Yu JS,Prevedello LM,White RD,Erdal BS",Gupta V,,Ohio State University,"Acute Proximal Femoral Fractures are a growing health concern among the aging population. These fractures are often associated with significant morbidity and mortality as well as reduced quality of life. Furthermore, with the increasing life expectancy owing to advances in healthcare, the number of proximal femoral fractures may increase by a factor of 2 to 3, since the majority of fractures occur in patients over the age of 65. In this paper, we show that by using transfer learning and leveraging pre-trained models, we can achieve very high accuracy in detecting fractures and that they can be localized utilizing class activation maps.","Musculoskeletal imaging,X-ray,Femoral fractures,Deep Learning,Convolutional Neural Networks,Transfer learning",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,MORTALITY,,,
15,KNOWLEDGE TRANSFER BETWEEN DATASETS FOR LEARNING-BASED TISSUE MICROSTRUCTURE ESTIMATION,,,1530-1533,"Qin Yu,Li Yuxing,Liu Zhiwen,Ye Chuyang","Qin Y,Li YX,Liu ZW,Ye CY",Ye CY,,Beijing Institute of Technology,"Learning-based approaches, especially those based on deep networks, have enabled high-quality estimation of tissue microstructure from low-quality diffusion magnetic resonance imaging (dMRI) scans, which are acquired with a limited number of diffusion gradients and a relatively poor spatial resolution. These learning-based approaches to tissue microstructure estimation require acquisitions of training dMRI scans with high-quality diffusion signals, which are densely sampled in the q-space and have a high spatial resolution. However, the acquisition of training scans may not be available for all datasets. Therefore, we explore knowledge transfer between different dMRI datasets so that learning-based tissue microstructure estimation can be applied for datasets where training scans are not acquired. Specifically, for a target dataset of interest, where only low-quality diffusion signals are acquired without training scans, we exploit the information in a source dMRI dataset acquired with highquality diffusion signals. We interpolate the diffusion signals in the source dataset in the q-space using a dictionary-based signal representation, so that the interpolated signals match the acquisition scheme of the target dataset. Then, the interpolated signals are used together with the high-quality tissue microstructure computed from the source dataset to train deep networks that perform tissue microstructure estimation for the target dataset. Experiments were performed on brain dMRI scans with low-quality diffusion signals, where the benefit of the proposed strategy is demonstrated.","tissue microstructure,deep network,knowledge transfer",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,"HUMAN,CONNECTOME,PROJECT,DEEP,NETWORK,DIFFUSION",,http://arxiv.org/pdf/1910.10930,
16,DEEP LEARNING MODELS TO STUDY THE EARLY STAGES OF PARKINSON'S DISEASE,,,1534-1537,"Ramirez Veronica Munoz,Kmetzsch Virgilio,Forbes Florence,Dojat Michel","Ramirez VM,Kmetzsch V,Forbes F,Dojat M",Ramirez VM,,Communaute Universite Grenoble Alpes,"Current physio-pathological data suggest that Parkinson's Disease (PD) symptoms are related to important alterations in subcortical brain structures. However, structural changes in these small regions remain difficult to detect for neuroradiologists, in particular, at the early stages of the disease (de novo PD patients). The absence of a reliable ground truth at the voxel level prevents the application of traditional supervised deep learning techniques. In this work, we consider instead an anomaly detection approach and show that auto-encoders (AE) could provide an efficient anomaly scoring to discriminate de novo PD patients using quantitative Magnetic Resonance Imaging (MRI) data.","Brain,Anomaly detection,Autoencoder,Diffusion Imaging,MRI",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
17,LIVER SEGMENTATION IN CT WITH MRI DATA: ZERO-SHOT DOMAIN ADAPTATION BY CONTOUR EXTRACTION AND SHAPE PRIORS,,,1538-1542,"Pham D. D.,Dovletov G.,Pauli J.","Pham DD,Dovletov G,Pauli J",Pham DD,,University of Duisburg Essen,"In this work we address the problem of domain adaptation for segmentation tasks with deep convolutional neural networks. We focus on managing the domain shift from MRI to CT volumes on the example of 3D liver segmentation. Domain adaptation between modalities is particularly of practical importance, as different hospital departments usually tend to use different imaging modalities and protocols in their clinical routine. Thus, training a model with source data from one department may not be sufficient for application in another institution. Most adaptation strategies make use of target domain samples and often additionally incorporate the corresponding ground truths from the target domain during the training process. In contrast to these approaches, we investigate the possibility of training our model solely on source domain data sets, i.e. we apply zero-shot domain adaptation. To compensate the missing target domain data, we use prior knowledge about both modalities to steer the model towards more general features during the training process. We particularly make use of fixed Sobel kernels to enhance contour information and apply anatomical priors, learned separately by a convolutional autoencoder. Although we completely discard including the target domain in the training process, our proposed approach improves a vanilla U-Net implementation drastically and yields promising segmentation results.","Zero-shot Domain Adaptation,Segmentation,Shape Priors,Contour Priors,Steered Filters",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,"ABDOMINAL,ORGANS,MULTILEVEL",,,
18,DEEP RANDOM FORESTS FOR SMALL SAMPLE SIZE PREDICTION WITH MEDICAL IMAGING DATA,,,1543-1547,"Katzmann Alexander,Muehlberg Alexander,Suehling Michael,Noerenberg Dominik,Holch Julian Walter,Gross Horst-Michael","Katzmann A,Muehlberg A,Suehling M,Norenberg D,Holch JW,Gross HM",Katzmann A,,Siemens AG,"Deep neural networks represent the state of the art for computer-aided medical imaging assessment, e.g. lesion detection, organ segmentation and disease classification. While for large datasets their superior performance is a clear argument, medical imaging data is often small and highly heterogeneous. In combination with the typical parameter amount in deep neural networks, this often leads to overfitting and results in a low level of generalization performance. We propose a straight-forward combination of random forests and deep neural networks for superior performance on medical imaging datasets with only small data, and provide an extensive evaluation of survival prediction for metastatic colorectal cancer patients using computed tomography imaging data, with our proposed method clearly outperforming other approaches.","Ensemble learning,Random forests,Survival prediction",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,"NEURAL-NETWORKS,CLASSIFICATION,CANCER",,,
19,TRAINING LIVER VESSEL SEGMENTATION DEEP NEURAL NETWORKS ON NOISY LABELS FROM CONTRAST CT IMAGING,,,1552-1555,"Xu Minfeng,Wang Yu,Chi Ying,Hua Xiansheng","Xu MF,Wang Y,Chi Y,Hua XS",Xu MF,,Alibaba Group,"Liver vessel segmentation from contrast CT images is critical in liver surgical planning and navigation. Due to the complex vessel systems, manual segmentation of liver vessel is laborious and error-prone. In this work we propose liver vessel segmentation deep neural networks that are able to perform the segmentation automatically. Dense image labeling incurred by normal deep learning based segmentation makes it impractical for vessel segmentation. On the contrary, our method only requires a few of initial voxel-level labels, and thus tremendously reduces the workload of manual annotating. A small number of liver vessels are firstly annotated and used to train a sparse dictionary and logistic regressor to get the preliminary vessel predictions. Then the proposed deep neural network is trained based on boot-strapping technique where the training target is a convex combination of the model predictions and those preliminary vessel predictions. Followed by the post-processing step consisting of region connection and noisy removal, the liver vessel tree is finally built from model predictions. Experimental results of 3DIRCADb dataset show that our method can effectively extract liver vessel trees from contrast CT images with minimum annotations. The state-of-art results have been achieved on this small dataset with the average dice and sensitivity 0.687 +/- 0.041 and 0.786 +/- 0.105 respectively.","Liver vessel segmentation,noisy label,deep learning,Contrast enhanced CT,dictionary learning",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,3D,,,
20,UNSUPERVISED DOMAIN ADAPTATION FOR CROSS-DEVICE OCT LESION DETECTION VIA LEARNING ADAPTIVE FEATURES,,,1570-1573,"Yang Suhui,Zhou Xia,Wang Jun,Xie Guotong,Lv Chuanfeng,Gao Peng,Lv Bin","Yang SH,Zhou X,Wang J,Xie GT,Lv CF,Gao P,Lv B",Lv B,,"Ping An Technol Shenzhen Co Ltd, Shenzhen, Peoples R China.","Optical coherence tomography (OCT) is widely used in computer-aided medical diagnosis of retinal pathologies. Deep convolutional network has been successfully applied to detect lesions from OCT images. Different OCT imaging devices inevitably cause variation in the distribution between training phase and testing phase, which will lead to extremely reduction on model performance. Most existing unsupervised domain adaptation methods are mainly focused on lesion segmentation, there are few studies on lesion detection tasks especially for OCT images. In this paper, we propose a novel unsupervised domain adaptation framework adaptively learning feature representation to achieve cross-device lesion detection for OCT images. Firstly, we design global and local adversarial discriminators to force the networks to learn device-independent features. Secondly, we develop a non-parameter adaptive feature norm into global adversarial discriminator to stabilize the discrimination in target domain. Finally, we perform the validation experiment on lesion detection task across two OCT devices. The results exhibit that the proposed framework has promising performance compared with other unsupervised domain adaptation approaches.","lesion detection,optical coherence tomography,unsupervised domain adaptation",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
21,DEEP LEARNING FOR HIGH SPEED OPTICAL COHERENCE ELASTOGRAPHY,,,1583-1586,"Neidhardt M.,Bengs M.,Latus S.,Schlueter M.,Saathoff T.,Schlaefer A.","Neidhardt M,Bengs M,Latus S,Schluter M,Saathoff T,Schlaefer A",Neidhardt M,,Hamburg University of Technology,"Mechanical properties of tissue provide valuable information for identifying lesions. One approach to obtain quantitative estimates of elastic properties is shear wave elastography with optical coherence elastography (OCE). However, given the shear wave velocity, it is still difficult to estimate elastic properties. Hence, we propose deep learning to directly predict elastic tissue properties from OCE data. We acquire 2D images with a frame rate of 30 kHz and use convolutional neural networks to predict gelatin concentration, which we use as a surrogate for tissue elasticity. We compare our deep learning approach to predictions from conventional regression models, using the shear wave velocity as a feature. Mean absolut prediction errors for the conventional approaches range from 132 +/- 098 p.p. to 157 +/- 130 p.p. whereas we report an error of 090 +/- 084 p.p. for the convolutional neural network with 3D spatio-temporal input. Our results indicate that deep learning on spatio-temporal data outperforms elastography based on explicit shear wave velocity estimation.","Optical Coherence Elastography,Deep Learning,Convolutional Neural Networks,High-Speed Imaging,Shear Wave Elastography",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,http://arxiv.org/pdf/2004.14024,
22,Reservoir Computing for Jurkat T-cell Segmentation in High Resolution Live Cell Ca2+ Fluorescence Microscopy,,,1587-1591,"Hadaeghi Fatemeh,Diercks Bjoern-Philipp,Wolf Insa M. A.,Werner Rene","Hadaeghi F,Diercks BP,Wolf IMA,Werner R",Hadaeghi F,,University of Hamburg,"The reservoir computing (RC) paradigm is exploited to detect Jurkat T cells and antibody-coated beads in fluorescence microscopy data. Recent progress in imaging of subcellular calcium (Ca2+) signaling offers a high spatial and temporal resolution to characterize early signaling events in T cells. However, data acquisition with dual-wavelength Ca2+ indicators, the photo-bleaching at high acquisition rate, low signal-to-noise ratio, and temporal fluctuations of image intensity entail corporation of post-processing techniques into Ca2+ imaging systems. Besides, although continuous recording enables real-time Ca2+ signal tracking in T cells, reliable automated algorithms must be developed to characterize the cells, and to extract the relevant information for conducting further statistical analyses. Here, we present a robust two-channel segmentation algorithm to detect Jurkat T lymphocytes as well as antibody-coated beads that are utilized to mimic cell-cell interaction and to activate the T cells in microscopy data. Our algorithm uses the reservoir computing framework to learn and recognize the cells - taking the spatiotemporal correlations between pixels into account. A comparison of segmentation accuracy on testing data between our proposed method and the deep learning U-Net model confirms that the developed model provides accurate and computationally cheap solution to the cell segmentation problem.","Ca2+ imaging,Jurkat T-cells,Reservoir computing,Segmentation,Machine learning",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
23,WEAKLY-SUPERVISED PREDICTION OF CELL MIGRATION MODES IN CONFOCAL MICROSCOPY IMAGES USING BAYESIAN DEEP LEARNING,,,1626-1629,"Gupta Anindya,Larsson Veronica,Matuszewski Damian,Stromblad Staffan,Wahlby Carolina","Gupta A,Larsson V,Matuszewski D,Stromblad S,Wahlby C",Gupta A,,Uppsala University,"Cell migration is pivotal for their development, physiology and disease treatment. A single cell on a 2D surface can utilize continuous or discontinuous migration modes. To comprehend the cell migration, an adequate quantification for single cell-based analysis is crucial. An automatized approach could alleviate tedious manual analysis, facilitating large-scale drug screening. Supervised deep learning has shown promising outcomes in computerized microscopy image analysis. However, their implication is limited due to the scarcity of carefully annotated data and uncertain deterministic outputs. We compare three deep learning models to study the problem of learning discriminative morphological representations using weakly annotated data for predicting the cell migration modes. We also estimate Bayesian uncertainty to describe the confidence of the probabilistic predictions. Amongst three compared models, DenseNet yielded the best results with a sensitivity of 87.91% +/- 13.22 at a false negative rate of 1.26% +/- 4.18.","Bayesian deep learning,cell migration,systems microscopy,weakly supervised learning",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,https://doi.org/10.1101/810473,
24,TRANSCRIPTOME-SUPERVISED CLASSIFICATION OF TISSUE MORPHOLOGY USING DEEP LEARNING,,,1630-1633,"Andersson Axel,Partel Gabriele);,Solorzano Leslie);,Wahlby Carolina","Andersson A,Partel G,Solorzano L,Wahlby C",Andersson A,,Uppsala University,"Deep learning has proven to successfully learn variations in tissue and cell morphology. Training of such models typically relies on expensive manual annotations. Here we conjecture that spatially resolved gene expression, e.i., the transcriptome, can be used as an alternative to manual annotations. In particular, we trained five convolutional neural networks with patches of different size extracted from locations defined by spatially resolved gene expression. The network is trained to classify tissue morphology related to two different genes, general tissue, as well as background, on an image of fluorescence stained nuclei in a mouse brain coronal section. Performance is evaluated on an independent tissue section from a different mouse brain, reaching an average Dice score of 0.51. Results may indicate that novel techniques for spatially resolved transcriptomics together with deep learning may provide a unique and unbiased way to find genotype-phenotype relationships.","In situ sequencing,Gene expression,Tissue classification,Deep learning",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
25,A DEEP LEARNING APPROACH TO VIDEO FLUOROSCOPIC SWALLOWING EXAM CLASSIFICATION,,,1647-1650,"Wilhelm Patrick,Reinhardt Joseph M.,Van Daele Douglas","Wilhelm P,Reinhardt JM,Van Daele D",Wilhelm P,,University of Iowa,"Dysphagia, or difficulty swallowing, is a serious health problem that reduces the quality of life of those affected. The standard method to diagnose dysphagia is the x-ray video fluoroscopic swallowing exam (VFSE). In this paper we investigate the use of deep learning networks to classify VFSE as normal or abnormal. The proposed network is based on a long term recurrent convolutional network (LRCN). This network was trained and validated using 1154 VFSE. Using 10-fold cross-validation, the accuracy of classification was 85% and the area under the ROC curve was 0.89. This work shows the promise of using deep learning networks as a screening tool to detect dysphagia in VFSE.","Deep learning,videofluoroscopy,dysphagia",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,DYSPHAGIA,,,
26,REDUCE FALSE-POSITIVE RATE BY ACTIVE LEARNING FOR AUTOMATIC POLYP DETECTION IN COLONOSCOPY VIDEOS,,,1655-1658,"Guo Zhe,Zhang Ruiyao,Li Qin,Liu Xinkai,Nemoto Daiki,Togashi Kazutomo,Niroshana Isuru S. M.,Shi Yuchen,Zhu Xin","Guo Z,Zhang RY,Li Q,Liu XK,Nemoto D,Togashi K,Niroshana SMI,Shi YC,Zhu X",Guo Z,,University of Aizu,"Automatic polyp detection is reported to have a high false-positive rate (FPR) because of various polyp-like structures and artifacts in complex colon environment. An efficient polyp's computer-aided detection (CADe) polyp detection system should have a high sensitivity and a low FPR (high specificity). Convolutional neural networks have been implemented in colonoscopy-based automatic polyp detection and achieved high performance in improving polyp detection rate. However, complex colon environments caused excessive false positives are going to prevent the clinical implementation of CADe systems. To reduce false positive rate, we proposed an automatic polyp detection algorithm, combined with YOLOv3 architecture and active learning. This algorithm was trained with colonoscopy videos/images from 283 subjects. Through testing with 100 short and 9 full colonoscopy videos, the proposed algorithm shown FPR of 2.8% and 1.5 %, respectively, similar sensitivities of expert endoscopists.","Active learning,colonoscopy,computer-aided detection,false-positive rate",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
27,SCREENING FOR BARRETT'S ESOPHAGUS WITH PROBE-BASED CONFOCAL LASER ENDOMICROSCOPY VIDEOS,,,1659-1663,"Pulido J. Vince,Guleria Shan,Ehsan Lubaina,Shah Tilak,Syed Sana,Brown Don E.","Pulido JV,Guleria S,Ehsan L,Shah T,Syed S,Brown DE",Syed S,,University of Virginia,"Histologic diagnosis of Barrett's esophagus and esophageal malignancy via probe-based confocal laser endomicroscopy (pCLE) allows for real-time examination of epithelial architecture and targeted biopsy sampling. Although pCLE demonstrates high specificity, sensitivity remains low. This study employs deep learning architectures in order to improve the accuracy of pCLE in diagnosing esophageal cancer and its precursors. pCLE videos are curated and annotated as belonging to one of the three classes: squamous, Barrett's (intestinal metaplasia without dysplasia), or dysplasia. We introduce two novel video architectures, Attention-Pooling and Multi-Module AttentionPooling deep networks, that outperform other models and demonstrate a high degree of explainability.","Barrett's Esophagus,Video Classification,Probe-based confocal laser endomicroscopy (pCLE)",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,"CAPSULE,ENDOSCOPY,RANDOM,BIOPSIES,MANAGEMENT,DIAGNOSIS",,,
28,PREDICTION OF LANGUAGE IMPAIRMENTS IN CHILDREN USING DEEP RELATIONAL REASONING WITH DWI DATA,,,1680-1684,"Banerjee Soumyanil,Dong Ming,Lee Min-Hee,O'Hara Nolan,Asano Eishi,Jeong Jeong-Won","Banerjee S,Dong M,Lee MH,O'Hara N,Asano E,Jeong JW",Banerjee S,,Wayne State University,"This paper proposes a new deep learning model using relational reasoning with diffusion-weighted imaging (DWI) data. We investigate how effectively and comprehensively DWI tractography-based connectome predicts the impairment of expressive and receptive language ability in individual children with focal epilepsy (FE). The proposed model constitutes a combination of a dilated convolutional neural network (CNN) and a relation network (RN), with the latter being applied to the dependencies of axonal connections across cortical regions in the whole brain. The presented results from 51 FE children demonstrate that the proposed model outperforms other existing state-of-the-art algorithms to predict language abilities without depending on connectome densities, with average improvement of up to 96:2% and 83:8% in expressive and receptive language prediction, respectively.",GYRUS,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,GYRUS,,,
29,DISCOVERING SALIENT ANATOMICAL LANDMARKS BY PREDICTING HUMAN GAZE,,,1711-1714,"Droste R.,Chatelain P.,Drukker L.,Sharma H.,Papageorghiou A. T.,Noble J. A.","Droste R,Chatelain P,Drukker L,Sharma H,Papageorghiou AT,Noble JA",Droste R,,University of Oxford,"Anatomical landmarks are a crucial prerequisite for many medical imaging tasks. Usually, the set of landmarks for a given task is predefined by experts. The landmark locations for a given image are then annotated manually or via machine learning methods trained on manual annotations. In this paper, in contrast, we present a method to automatically discover and localize anatomical landmarks in medical images. Specifically, we consider landmarks that attract the visual attention of humans, which we term visually salient landmarks. We illustrate the method for fetal neurosonographic images. First, full-length clinical fetal ultrasound scans are recorded with live sonographer gaze-tracking. Next, a convolutional neural network (CNN) is trained to predict the gaze point distribution (saliency map) of the sonographers on scan video frames. The CNN is then used to predict saliency maps of unseen fetal neurosonographic images, and the landmarks are extracted as the local maxima of these saliency maps. Finally, the landmarks are matched across images by clustering the landmark CNN features. We show that the discovered landmarks can be used within affine image registration, with average landmark alignment errors between 4.1% and 10.9% of the fetal head long axis length.","Landmark detection,visual saliency,salient landmarks,image registration,ultrasound",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,"DEFORMABLE,REGISTRATION",,http://arxiv.org/pdf/2001.08188,
30,SEMI-SUPERVISED CERVICAL DYSPLASIA CLASSIFICATION WITH LEARNABLE GRAPH CONVOLUTIONAL NETWORK,,,1720-1724,"Ou Yanglan,Xue Yuan,Yuan Ye,Xu Tao,Pisztora Vincent,Li Jia,Huang Xiaolei","Ou YL,Xue Y,Yuan Y,Xu T,Pisztora V,Li J,Huang XL",Ou YL,,Pennsylvania Commonwealth System of Higher Education (PCSHE),"Cervical cancer is the second most prevalent cancer affecting women today. As the early detection of cervical carcinoma relies heavily upon screening and pre-clinical testing, digital cervicography has great potential as a primary or auxiliary screening tool, especially in low-resource regions due to its low cost and easy access. Although an automated cervical dysplasia detection system has been desirable, traditional fully-supervised training of such systems requires large amounts of annotated data which are often labor-intensive to collect. To alleviate the need for much manual annotation, we propose a novel graph convolutional network (GCN) based semi-supervised classification model that can be trained with fewer annotations. In existing GCNs, graphs are constructed with fixed features and can not be updated during the learning process. This limits their ability to exploit new features learned during graph convolution. In this paper, we propose a novel and more flexible GCN model with a feature encoder that adaptively updates the adjacency matrix during learning and demonstrate that this model design leads to improved performance. Our experimental results on a cervical dysplasia classification dataset show that the proposed framework outperforms previous methods under a semi-supervised setting, especially when the labeled samples are scarce.","Semi-supervised learning,Graph convolutional network,Cervical cancer classification",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,http://arxiv.org/pdf/2004.00191,
31,SEPARATION OF METABOLITE AND MACROMOLECULE SIGNALS FOR H-1-MRSI USING LEARNED NONLINEAR MODELS,,,1725-1728,"Li Yahang,Wang Zepeng);,Lam Fan","Li YH,Wang ZP,Lam F",Li YH,,University of Illinois System,"This paper presents a novel method to reconstruct and separate metabolite and macromolecule (MM) signals in H-1 magnetic resonance spectroscopic imaging (MRSI) data using learned nonlinear models. Specifically, deep autoencoder (DAE) networks were constructed and trained to learn the nonlinear low-dimensional manifolds, where the metabolite and MM signals reside individually. A regularized reconstruction formulation is proposed to integrate the learned models with signal encoding model to reconstruct and separate the metabolite and MM components. An efficient algorithm was developed to solve the associated optimization problem. The performance of the proposed method has been evaluated using simulation and experimental H-1-MRSI data. Efficient low-dimensional signal representation of the learned models and improved metabolite/MM separation over the standard parametric fitting based approach have been demonstrated.","proton magnetic resonance spectroscopic imaging,deep learning,deep autoencoder,regularization,ADMM",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,"MAGNETIC-RESONANCE-SPECTROSCOPY,QUANTIFICATION,QUANTITATION",,,
32,A SEMI-SUPERVISED JOINT LEARNING APPROACH TO LEFT VENTRICULAR SEGMENTATION AND MOTION TRACKING IN ECHOCARDIOGRAPHY,,,1734-1737,"Ta Kevinminh,Ahn Shawn S.,Lu Allen,Stendahl John C.,Sinusas Albert J.,Duncan James S.","Ta K,Ahn SS,Lu A,Stendahl JC,Sinusas AJ,Duncan JS",Ta K,,Yale University,"Accurate interpretation and analysis of echocardiography is important in assessing cardiovascular health. However, motion tracking often relies on accurate segmentation of the myocardium, which can be difficult to obtain due to inherent ultrasound properties. In order to address this limitation, we propose a semi-supervised joint learning network that exploits overlapping features in motion tracking and segmentation. The network simultaneously trains two branches: one for motion tracking and one for segmentation. Each branch learns to extract features relevant to their respective tasks and shares them with the other. Learned motion estimations propagate a manually segmented mask through time, which is used to guide future segmentation predictions. Physiological constraints are introduced to enforce realistic cardiac behavior. Experimental results on synthetic and in vivo canine 2D+t echocardiographic sequences outperform some competing methods in both tasks.","Echocardiography,Motion Tracking,Segmentation,Semi-Supervised,Deep Learning",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7526517,
33,CORONARY WALL SEGMENTATION IN CCTA SCANS VIA A HYBRID NET WITH CONTOURS REGULARIZATION,,,1743-1747,"Huang Kaikai,Tejero-de-Pablos Antonio,Yamane Hiroaki,Kurose Yusuke,Iho Junichi,Tokunaga Youji,Horie Makoto,Nishizawa Keisuke,Hayashi Yusaku,Koyama Yasushi","Huang K,Tejero-de-Pablos A,Yamane R,Kurose Y,Iho J,Tokunaga Y,Horie M,Nishizawa K,Hayashi Y,Koyama Y",Huang K,,University of Tokyo,"Providing closed and well-connected boundaries of coronary artery is essential to assist cardiologists in the diagnosis of coronary artery disease (CAD). Recently, several deep learning-based methods have been proposed for boundary detection and segmentation in a medical image. However, when applied to coronary wall detection, they tend to produce disconnected and inaccurate boundaries. In this paper, we propose a novel boundary detection method for coronary arteries that focuses on the continuity and connectivity of the boundaries. In order to model the spatial continuity of consecutive images, our hybrid architecture takes a volume (i.e., a segment of the coronary artery) as input and detects the boundary of the target slice (i.e., the central slice of the segment). Then, to ensure closed boundaries, we propose a contour-constrained weighted Hausdorff distance loss. We evaluate our method on a dataset of 34 patients of coronary CT angiography scans with curved planar reconstruction (CCTA-CPR) of the arteries (i.e., cross-sections). Experiment results show that our method can produce smooth closed boundaries outperforming the state-of-the-art accuracy.","Boundary Detection,Coronary Artery,Hausdorff Distance Loss",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,http://arxiv.org/pdf/2002.12263,
34,SOFT-LABEL GUIDED SEMI-SUPERVISED LEARNING FOR BI-VENTRICLE SEGMENTATION IN CARDIAC CINE MRI,,,1752-1755,"Chang Qi,Yan Zhennan,Lou Yixuan,Axel Leon,Metaxas Dimitris N.","Chang Q,Yan ZN,Lou YX,Axel L,Metaxas DN",Chang Q,,Rutgers State University New Brunswick,"Deep convolutional neural networks have been applied to medical image segmentation tasks successfully in recent years by taking advantage of a large amount of training data with golden standard annotations. However, it is difficult and expensive to obtain good-quality annotations in practice. This work aims to propose a novel semi-supervised learning framework to improve the ventricle segmentation from 2D cine MR images. Our method is efficient and effective by computing soft labels dynamically for the unlabeled data. Specifically, we obtain the soft labels, rather than hard labels, from a teacher model in every learning iteration. The uncertainty of the target label of unlabeled data is intrinsically encoded in the soft label. The soft label can be improved towards the ideal target in training. We use a separate loss to regularize the unlabeled data to produce similar probability distribution as the soft labels in each iteration. Experiments show that our method outperforms a state-of-the-art semi-supervised method.","Semi-supervised segmentation,soft-label",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
35,AUTOMATED LEFT ATRIAL SEGMENTATION FROM MAGNETIC RESONANCE IMAGE SEQUENCES USING DEEP CONVOLUTIONAL NEURAL NETWORK WITH AUTOENCODER,,,1756-1760,"Ghosh Shrimanti,Ray Nilanjan,Boulanger Pierre,Punithakumar Kumaradevan,Noga Michelle","Ghosh S,Ray N,Boulanger P,Punithakumar K,Noga M",Ghosh S,,University of Alberta,"This study presents a novel automated algorithm to segment the left atrium (LA) from 2, 3 and 4-chamber long-axis cardiac cine magnetic resonance image (MRI) sequences using deep convolutional neural network (CNN). The objective of the segmentation process is to delineate the boundary between myocardium and endocardium and exclude the mitral valve so that the results could be used for generating clinical measurements such as strain and strain rate. As such, the segmentation needs to be performed using open contours, a more challenging problem than the pixel-wise semantic segmentation performed by existing machine learning-based approaches such as U-net. The proposed neural net is built based on pre-trained CNN Inception V4 architecture, and it predicts a compressed vector by applying a multi-layer autoencoder, which is then back-projected into the segmentation contour of the LA to perform the delineation using open contours. Quantitative evaluations were performed to compare the performances of the proposed method and the current state-of-the-art U-net method. Both methods were trained using 6195 images acquired from 80 patients and evaluated using 1515 images acquired from 20 patients where the datasets were obtained retrospectively, and ground truth manual segmentation was provided by an expert radiologist. The proposed method yielded an average Dice score of 93.1% and Hausdorff distance of 4.2 mm, whereas the U-net yielded 91.6% and 11.9 mm for Dice score and Hausdorff distance metrics, respectively. The quantitative evaluations demonstrated that the proposed method performed significantly better than U-net in terms of Hausdorff distance in addition to providing open contour-based segmentation for the LA.","Magnetic resonance imaging (MRI),left atrial segmentation,autoencoders,convolutional neural networks (CNN)",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
36,GENERATING CONTROLLABLE ULTRASOUND IMAGES OF THE FETAL HEAD,,,1761-1764,"Lee Lok Hin,Noble J. Alison","Lee LH,Noble JA",Lee LH,,University of Oxford,"Synthesis of anatomically realistic ultrasound images could be potentially valuable in sonographer training and to provide training images for algorithms, but is a challenging technical problem. Generating examples where different image attributes can be controlled may also be useful for tasks such as semi-supervised classification and regression to augment costly human annotation. In this paper, we propose using an information maximizing generative adversarial network with a least-squares loss function to generate new examples of fetal brain ultrasound images from clinically acquired healthy subject twenty-week anatomy scans. The unsupervised network succeeds in disentangling natural clinical variations in anatomical visibility and image acquisition parameters, which allows for user-control in image generation. To evaluate our method, we also introduce an additional synthetic fetal ultrasound specific image quality metric called the Frechet SonoNet Distance (FSD) to quantitatively evaluate synthesis quality. To the best of our knowledge, this is the first work that generates ultrasound images with a generator network trained on clinical acquisitions where governing parameters can be controlled in a visually interpretable manner.","Generative adversarial networks,representation disentanglement,fetal ultrasound",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,https://ora.ox.ac.uk/objects/uuid:49859279-9cca-4105-b9f3-1bd9bbfa92df/download_file?safe_filename=Lee%2520and%2520Noble%2520Generating%2520controllable%2520ultrasound.pdf&type_of_work=Conference+item,
37,LEARNING A SELF-INVERSE NETWORK FOR BIDIRECTIONAL MRI IMAGE SYNTHESIS,,,1765-1769,"Shen A. Zengming,Chen B. Yifan,Zhou C. Kevin S.,Georgescu D. Bogdan,Liu E. Xuqi,Huang F. Thomas S.","Shen AZM,Chen BYF,Zhou CKS,Georgescu DB,Liu EXQ,Huang FTS",Shen AZM,,"Univ Illinios Urbana Champaign, Champaign, IL 61820 USA.","The one-to-one mapping is necessary for MRI image synthesis as MRI images are unique to the patient. State-of-the-art approaches for image synthesis from domain X to domain Y learn a convolutional neural network that meticulously maps between the domains. A different network is typically implemented to map along the opposite direction, from Y to X. In this paper, we explore the possibility of only wielding one network for bi-directional image synthesis. In other words, such an autonomous learning network implements a self-inverse function. A self-inverse network shares several distinct advantages: only one network instead of two, better generalization and more restricted parameter space. Most importantly, a self-inverse function guarantees a one-to-one mapping, a property that cannot be guaranteed by earlier approaches that are not self-inverse. The experiments on MRI T1 and T2 images show that, compared with the baseline approaches that use two separate models for the image synthesis along two directions, our self-inverse network achieves better synthesis results in terms of standard metrics. Finally, our sensitivity analysis confirms the feasibility of learning a one-to-one mapping function for MRI image synthesis.","image synthesis,MRI,self-inverse,mapping",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
38,ZERO-SHOT ADAPTATION TO SIMULATE 3D ULTRASOUND VOLUME BY LEARNING A MULTILINEAR SEPARABLE 2D CONVOLUTIONAL NEURAL NETWORK,,,1780-1783,"Mooga Anand,Sethuraman Ramanathan,Sheet Debdoot","Mooga A,Sethuraman R,Sheet D",Mooga A,,Indian Institute of Technology System (IIT System),"Ultrasound imaging relies on sensing of waves returned after interaction with scattering media present in biological tissues. An acoustic pulse transmitted by a single element transducer dilates along the direction of propagation, and is observed as 1D point spread function (PSF) in A-mode imaging. In 2D B-mode imaging, a 1D array of transducer elements is used and dilation of pulse is also observed along the direction of these elements, manifesting a 2D PSF. In 3D B-mode imaging using a 2D matrix of transducer elements, a 3D PSF is observed. Fast simulation of a 3D B-mode volume by way of convolutional transformer networks to learn the PSF family would require a training dataset of true 3D volumes which are not readily available. Here we start in Stage 0 with a simple physics based simulator in 3D to generate speckles from a tissue echogenicity map. Next in Stage 1, we learn a multilinear separable 2D convolutional neural network using 1D convolutions to model PSF family along direction of ultrasound propagation and orthogonal to it. This is adversarially trained using a visual Turing test on 2D ultrasound images. The PSF being circularly symmetric about an axis parallel to the direction of wave propagation, we simulate full 3D volume, by way of alternating the direction of 1D convolution along 2 axes that are mutually orthogonal to the direction of wave propagation. We validate performance using visual Turing test with experts and distribution similarity measures.","Adversarial learning,separable multilinear convolution,ultrasound simulation,zero-shot learning",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
39,OPEN-SET OCT IMAGE RECOGNITION WITH SYNTHETIC LEARNING,,,1788-1792,"Xiao Yuting,Gao Shenghua,Chai Zhengjie,Zhou Kang,Zhang Tianyang,Zhao Yitian,Cheng Jun,Liu Jiang","Xiao YT,Gao SH,Chai ZJ,Zhou K,Zhang TY,Zhao YT,Cheng J,Liu J",Xiao YT,,ShanghaiTech University,"Due to new eye diseases discovered every year, doctors may encounter some rare or unknown diseases. Similarly, in medical image recognition field, many practical medical classification tasks may encounter the case where some testing samples belong to some rare or unknown classes that have never been observed or included in the training set, which is termed as an open-set problem. As rare diseases samples are difficult to be obtained and included in the training set, it is reasonable to design an algorithm that recognizes both known and unknown diseases. Towards this end, this paper leverages a novel generative adversarial network (GAN) based synthetic learning for open-set retinal optical coherence tomography (OCT) image recognition. Specifically, we first train an auto-encoder GAN and a classifier to reconstruct and classify the observed images, respectively. Then a subspace-constrained synthesis loss is introduced to generate images that locate near the boundaries of the subspace of images corresponding to each observed disease, meanwhile, these images cannot be classified by the pre-trained classifier. In other words, these synthesized images are categorized into an unknown class. In this way, we can generate images belonging to the unknown class, and add them into the original dataset to retrain the classifier for the unknown disease discovery.","Open-set,Generative Adversarial Network,Subspace-constrained Synthesis Loss",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
40,SYNTHESIS AND EDITION OF ULTRASOUND IMAGES VIA SKETCH GUIDED PROGRESSIVE GROWING GANS,,,1793-1797,"Liang Jiamin,Yang Xin,Li Haoming,Wang Yi,Manh The Van,Dou Haoran,Chen Chaoyu,Fang Jinghui,Liang Xiaowen,Mai Zixin","Liang JM,Yang X,Li HM,Wang Y,Van MT,Dou HR,Chen CY,Fang JH,Liang XW,Mai ZX",Ni D,,Shenzhen University,"Ultrasound (US) is widely accepted in clinic for anatomical structure inspection. However, lacking in resources to practice US scan, novices often struggle to learn the operation skills. Also, in the deep learning era, automated US image analysis is limited by the lack of annotated samples. Efficiently synthesizing realistic, editable and high resolution US images can solve the problems. The task is challenging and previous methods can only partially complete it. In this paper, we devise a new framework for US image synthesis. Particularly, we firstly adopt a sketch generative adversarial networks (Sgan) to introduce background sketch upon object mask in a conditioned generative adversarial network. With enriched sketch cues, Sgan can generate realistic US images with editable and fine-grained structure details. Although effective, Sgan is hard to generate high resolution US images. To achieve this, we further implant the Sgan into a progressive growing scheme (PGSgan). By smoothly growing both generator and discriminator, PGSgan can gradually synthesize US images from low to high resolution. By synthesizing ovary and follicle US images, our extensive perceptual evaluation, user study and segmentation results prove the promising efficacy and efficiency of the proposed PGSgan.","Ultrasound,Image synthesis,Conditional GAN,High resolution,Progressive growing",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,MODEL,,http://arxiv.org/pdf/2004.00226,
41,REMOVE APPEARANCE SHIFT FOR ULTRASOUND IMAGE SEGMENTATION VIA FAST AND UNIVERSAL STYLE TRANSFER,,,1824-1828,"Liu Zhendong,Yang Xin,Gao Rui,Liu Shengfeng,Dou Haoran,He Shuangchi,Huang Yuhao,Huang Yankai,Luo Huanjia,Zhang Yuanji","Liu ZD,Yang X,Gao R,Liu SF,Dou HR,He SC,Huang YH,Huang YK,Luo HJ,Zhang YJ",Ni D,,Shenzhen University,"Deep Neural Networks (DNNs) suffer from the performance degradation when image appearance shift occurs, especially in ultrasound (US) image segmentation. In this paper, we propose a novel and intuitive framework to remove the appearance shift, and hence improve the generalization ability of DNNs. Our work has three highlights. First, we follow the spirit of universal style transfer to remove appearance shifts, which was not explored before for US images. Without sacrificing image structure details, it enables the arbitrary style-content transfer. Second, accelerated with Adaptive Instance Normalization block, our framework achieved real-time speed required in the clinical US scanning. Third, an efficient and effective style image selection strategy is proposed to ensure the target-style US image and testing content US image properly match each other. Experiments on two large US datasets demonstrate that our methods are superior to state-of-the-art methods on making DNNs robust against various appearance shifts.","Style transfer,Image segmentation,Ultrasound image,Appearance shift,Generalization ability",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,http://arxiv.org/pdf/2002.05844,
42,REGION PROPOSAL NETWORK WITH GRAPH PRIOR AND IOU-BALANCE LOSS FOR LANDMARK DETECTION IN 3D ULTRASOUND,,,1829-1833,"Chen Chaoyu,Yang Xin,Huang Ruobing,Shi Wenlong,Liu Shengfeng,Lin Mingrong,Huang Yuhao,Yang Yong,Zhang Yuanji,Luo Huanjia","Chen CY,Yang X,Huang RB,Shi WL,Liu SF,Lin MR,Huang YH,Yang Y,Zhang YJ,Luo HJ",Ni D,,Shenzhen University,"3D ultrasound (US) can facilitate detailed prenatal examinations for fetal growth monitoring. To analyze a 3D US volume, it is fundamental to identify anatomical landmarks of the evaluated organs accurately. Typical deep learning methods usually regress the coordinates directly or involve heatmap-matching. However, these methods struggle to deal with volumes with large sizes and the highly-varying positions and orientations of fetuses. In this work, we exploit an object detection framework to detect landmarks in 3D fetal facial US volumes. By regressing multiple parameters of the landmark-centered bounding box (B-box) with a strict criteria, the proposed model is able to pinpoint the exact location of the targeted landmarks. Specifically, the model uses a 3D region proposal network (RPN) to generate 3D candidate regions, followed by several 3D classification branches to select the best candidate. It also adopts an IoU-balance loss to improve communications between branches that benefit the learning process. Furthermore, it leverage a distance-based graph prior to regularize the training and helps to reduce false positive predictions. The performance of the proposed framework is evaluated on a 3D US dataset to detect five key fetal facial landmarks. Results showed the proposed method outperforms some of the state-of-the-art methods in efficacy and efficiency.","3D ultrasound,Landmark detection,Region proposal network,Fetal face,Prior knowledge",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,http://arxiv.org/pdf/2004.00207,
43,SELF-SUPERVISED REPRESENTATION LEARNING FOR ULTRASOUND VIDEO,,,1847-1850,"Jiao Jianbo,Droste Richard,Drukker Lior,Papageorghiou Aris T.,Noble J. Alison","Jiao JB,Droste R,Drukker L,Papageorghiou AT,Noble JA",Jiao JB,,University of Oxford,"Recent advances in deep learning have achieved promising performance for medical image analysis, while in most cases ground-truth annotations from human experts are necessary to train the deep model. In practice, such annotations are expensive to collect and can be scarce for medical imaging applications. Therefore, there is significant interest in learning representations from unlabelled raw data. In this paper, we propose a self-supervised learning approach to learn meaningful and transferable representations from medical imaging video without any type of human annotation. We assume that in order to learn such a representation, the model should identify anatomical structures from the unlabelled data. Therefore we force the model to address anatomy-aware tasks with free supervision from the data itself. Specifically, the model is designed to correct the order of a reshuffled video clip and at the same time predict the geometric transformation applied to the video clip. Experiments on fetal ultrasound video show that the proposed approach can effectively learn meaningful and strong representations, which transfer well to downstream tasks like standard plane detection and saliency prediction.","Self-supervised,representation learning,ultrasound video",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,http://arxiv.org/pdf/2003.00105,
44,A ROBUST NETWORK ARCHITECTURE TO DETECT NORMAL CHEST X-RAY RADIOGRAPHS,,,1851-1855,"Wong Ken C. L.,Moradi Mehdi,Wu Joy,Pillai Anup,Sharma Arjun,Gur Yaniv,Ahmad Hassan,Chowdary Minnekanti Sunil,Chiranjeevi J.,Polaka Kiran Kumar Reddy","Wong KCL,Moradi M,Wu J,Pillai A,Sharma A,Gur Y,Ahmad H,Chowdary MS,Chiranjeevi J,Polaka KKR",Moradi M,,International Business Machines (IBM),"We propose a novel deep neural network architecture for normalcy detection in chest x-ray images. This architecture treats the problem as fine-grained binary classification in which the normal cases are well-defined as a class while leaving all other cases in the broad class of abnormal. It employs several components that allow generalization and prevent overfitting across demographics. The model is trained and validated on a large public dataset of frontal chest X-ray images. It is then tested independently on images from a clinical institution of differing patient demographics using a three radiologist consensus for ground truth labeling. The model provides an area under ROC curve of 0.96 when tested on 1271 images. We can automatically remove nearly a third of disease-free chest X-ray screening images from the workflow, without introducing any false negatives (100% sensitivity to disease) thus raising the potential of expediting radiology workflows in hospitals in future.","Deep neural networks,AI-assisted radiology,Automatic chest X-ray read",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,http://arxiv.org/pdf/2004.06147,
45,IMPROVING LUNG NODULE DETECTION WITH LEARNABLE NON-MAXIMUM SUPPRESSION,,,1861-1865,"Capia Elvis R.,Sousa Azael M.,Falcao Alexandre X.","Capia ER,Sousa AM,Falcao AX",Capia ER,,Universidade Estadual de Campinas,"Current lung nodule detection methods generate several candidate regions per nodule, such that a Non-Maximum Suppression (NMS) algorithm is required to select a single region per nodule while eliminating the redundant ones. GossipNet is a 1D Neural Network (NN) for NMS, which can learn the NMS parameters rather than relying on handcrafted ones. However, GossipNet does not take advantage of image features to learn NMS. We use Faster R-CNN with ResNet18 for candidate region detection and present FeatureNMS - a neural network that provides additional image features to the input of GossipNet, which result from a transformation over the voxel intensities of each candidate region in the CT image. Experiments indicate that FeatureNMS can improve nodule detection in 2.33% and 0.91%, on average, when compared to traditional NMS and the original GossipNet, respectively.","Non-maximum suppression,CT images of the thorax,Lung nodule detection,Deep Neural Networks",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
46,LUNG NODULE MALIGNANCY CLASSIFICATION BASED ON NLSTx DATA,,,1870-1874,"Veasey Benjamin,Farhangi M. Mehdi,Frigui Hichem,Broadhead Justin,Dahle Michael,Pezeshk Aria,Seow Albert,Amini Amir A.","Veasey B,Farhangi MM,Frigui H,Broadhead J,Dahle M,Pezeshk A,Seow A,Amini AA",Veasey B,,University of Louisville,"While several datasets containing CT images of lung nodules exist, they do not contain definitive diagnoses and often rely on radiologists' visual assessment for malignancy rating. This is in spite of the fact that lung cancer is one of the top three most frequently misdiagnosed diseases based on visual assessment. In this paper, we propose a dataset of difficult-to-diagnose lung nodules based on data from the National Lung Screening Trial (NLST), which we refer to as NLSTx. In NLSTx, each malignant nodule has a definitive ground truth label from biopsy. Herein, we also propose a novel deep convolutional neural network (CNN) / recurrent neural network framework that allows for use of pre-trained 2-D convolutional feature extractors, similar to those developed in the ImageNet challenge. Our results show that the proposed framework achieves comparable performance to an equivalent 3-D CNN while requiring half the number of parameters.","Lung Cancer,Nodule Classification,Computer-Aided Diagnosis,Early Detection",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
47,RELATIONAL LEARNING BETWEEN MULTIPLE PULMONARY NODULES VIA DEEP SET ATTENTION TRANSFORMERS,,,1875-1878,"Yang Jiancheng,Deng Haoran,Huang Xiaoyang,Ni Bingbing,Xu Yi","Yang JC,Deng HR,Huang XY,Ni BB,Xu Y",Ni BB,,Shanghai Jiao Tong University,"Diagnosis and treatment of multiple pulmonary nodules are clinically important but challenging. Prior studies on nodule characterization use solitary-nodule approaches on multiple nodular patients, which ignores the relations between nodules. In this study, we propose a multiple instance learning (MIL) approach and empirically prove the benefit to learn the relations between multiple nodules. By treating the multiple nodules from a same patient as a whole, critical relational information between solitary-nodule voxels is extracted. To our knowledge, it is the first study to learn the relations between multiple pulmonary nodules. Inspired by recent advances in natural language processing (NLP) domain, we introduce a self-attention transformer equipped with 3D CNN, named NoduleSAT, to replace typical pooling-based aggregation in multiple instance learning. Extensive experiments on lung nodule false positive reduction on LUNA16 database, and malignancy classification on LIDC-IDRI database, validate the effectiveness of the proposed method.","Relational Learning,Attention,Multiple Pulmonary Nodules,Computer-Aided Diagnosis (CADx)",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,"COMPUTED-TOMOGRAPHY,IMAGES",,http://arxiv.org/pdf/2004.05640,
48,LOCALIZATION OF CRITICAL FINDINGS IN CHEST X-RAY WITHOUT LOCAL ANNOTATIONS USING MULTI-INSTANCE LEARNING,,,1879-1882,"Schwab Evan,Goossen Andre,Deshpande Hrishikesh,Saalbach Axel","Schwab E,Goossen A,Deshpande H,Saalbach A",Schwab E,,Philips,"The automatic detection of critical findings in chest X-rays (CXR), such as pneumothorax, is important for assisting radiologists in their clinical workflow like triaging time-sensitive cases and screening for incidental findings. While deep learning (DL) models has become a promising predictive technology with near-human accuracy, they commonly suffer from a lack of explainability, which is an important aspect for clinical deployment of DL models in the highly regulated healthcare industry. For example, localizing critical findings in an image is useful for explaining the predictions of DL classification algorithms. While there have been a host of joint classification and localization methods for computer vision, the state-of-the-art DL models require locally annotated training data in the form of pixel level labels or bounding box coordinates. In the medical domain, this requires an expensive amount of manual annotation by medical experts for each critical finding. This requirement becomes a major barrier for training models that can rapidly scale to various findings. In this work, we address these shortcomings with an interpretable DL algorithm based on multi-instance learning that jointly classifies and localizes critical findings in CXR without the need for local annotations. We show competitive classification results on three different critical findings (pneumothorax, pneumonia, and pulmonary edema) from three different CXR datasets.","chest x-ray,critical findings,multi-instance learning,weak supervision,localization",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,http://arxiv.org/pdf/2001.08817,
49,Walking Imagery Evaluation Based on Multi-view Features and Stacked Denoising Auto-encoder Network,,,1896-1899,"Liang Enmin,Elazab Ahmed,Liang Shuang,Wang Qiong,Wang Tianfu,Lei Baiying","Liang EM,Elazab A,Liang S,Wang Q,Wang TF,Lei BY",Lei BY,,Shenzhen University,"Brain-computer interfaces (BCIs) based on motor imagery (MI) have played an important role and obtained impressive achievement in exercise rehabilitation. However, most of the previous researches focused on the upper limb, while many disabled patients need the same technology to assist their rehabilitation training. It is more difficult to detect lower limb MI than upper limb because of the deeper and smaller corresponding sensorimotor cortex. To solve this problem, a new paradigm is proposed to perform waking imagery (WI) for subjects in a virtual environment (VE) to further enhance their brain activities. Furthermore, to decode WI efficiently when facing the low reliable and limited data, we propose a stacked denoising auto-encoder (SDAE) network, which is trained on multi-view feature obtained from VE. First, the spatial and frequency based features are extracted and fused from the raw data. Second, we use SDAE network to extract the hidden features from the above features. Third, we fuse the previous features and hidden features to train the Softmax classifier. Experimental results on our self-collected data demonstrate that, SDAE network outperforms other deep learning methods in classifying WI in VE.","Brain-computer interface,walking imagery,stacked denoising auto-encoder,virtual environment",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,"MOTOR,IMAGERY",,,
50,TEMPORALLY ADAPTIVE-DYNAMIC SPARSE NETWORK FOR MODELING DISEASE PROGRESSION,,,1900-1904,"Zhang Jie,Wang Yalin","Zhang J,Wang YL",Zhang J,,Arizona State University,"Alzheimer's disease (AD) is a neurodegenerative disorder with progressive impairment of memory and cognitive functions. Sparse coding (SC) has been demonstrated to be an efficient and effective method for AD diagnosis and prognosis. However, previous SC methods usually focus on the baseline data while ignoring the consistent longitudinal features with strong sparsity pattern along the disease progression. Additionally, SC methods extract sparse features from image patches separately rather than learn with the dictionary atoms across the entire subject. To address these two concerns and comprehensively capture temporal-subject sparse features towards earlier and better discriminability of AD, we propose a novel supervised SC network termed Temporally Adaptive-Dynamic Sparse Network (TADsNet) to uncover the sequential correlation and native subject-level codes from the longitudinal brain images. Our work adaptively updates the sparse codes to impose the temporal regularized correlation and dynamically mine the dictionary atoms to make use of entire subject-level features. Experimental results on ADNI-I cohort validate the superiority of our approach.","RNN,Sparse Coding,Longitudinal",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,"REPRESENTATION,REGRESSION,SELECTION",,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8130893,
51,BAYESIAN SKIP-AUTOENCODERS FOR UNSUPERVISED HYPERINTENSE ANOMALY DETECTION IN HIGH RESOLUTION BRAIN MRI,,,1905-1909,"Baur Christoph,Wiestler Benedikt,Albarqouni Shadi,Navab Nassir","Baur C,Wiestler B,Albarqouni S,Navab N",Baur C,,Technical University of Munich,"Autoencoder-based approaches for Unsupervised Anomaly Detection (UAD) in brain MRI have recently gained a lot of attention and have shown promising performance. However, brain MR images are particularly complex and require large model capacity for learning a proper reconstruction, which existing methods encounter by restricting themselves to downsampled data or anatomical subregions. In this work, we show that models with limited capacity can be trained and used for UAD in full brain MR images at their native resolution by introducing skip-connections, a concept which has already proven beneficial for biomedical image segmentation and image-to-image translation, and a dropout-based mechanism to prevent the model from learning an identity mapping. In an ablative study on two different pathologies we show considerable improvements over State-of-the-Art Autoencoder-based UAD models. The stochastic nature of the model also allows to investigate epistemic uncertainty in our so-called Skip-Autoencoder, which is briefly portrayed.","Anomaly Detection,Skip-Autoencoders,Uncertainty",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
52,LEARNING TO DETECT BRAIN LESIONS FROM NOISY ANNOTATIONS,,,1910-1914,"Karimi Davood,Peters Jurriaan M.,Ouaalam Abdelhakim,Prabhu Sanjay P.,Sahin Mustafa,Krueger Darcy A.,Kolevzon Alexander,Eng Charis,Warfield Simon K.,Gholipour Ali","Karimi D,Peters JM,Ouaalam A,Prabhu SP,Sahin M,Krueger DA,Kolevzon A,Eng C,Warfield SK,Gholipour A",Karimi D,,Harvard University,"Supervised training of deep neural networks in medical imaging applications relies heavily on expert-provided annotations. These annotations, however, are often imperfect, as voxel-by-voxel labeling of structures on 3D images is difficult and laborious. In this paper, we focus on one common type of label imperfection, namely, false negatives. Focusing on brain lesion detection, we propose a method to train a convolutional neural network (CNN) to segment lesions while simultaneously improving the quality of the training labels by identifying false negatives and adding them to the training labels. To identify lesions missed by annotators in the training data, our method makes use of the 1) CNN predictions, 2) prediction uncertainty estimated during training, and 3) prior knowledge about lesion size and features. On a dataset of 165 scans of children with tuberous sclerosis complex from five centers, our method achieved better lesion detection and segmentation accuracy than the baseline CNN trained on the noisy labels, and than several alternative techniques.","brain lesion detection,noisy labels,imperfect labels,deep learning,tuberous sclerosis complex",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
53,A HIGH-POWERED BRAIN AGE PREDICTION MODEL BASED ON CONVOLUTIONAL NEURAL NETWORK,,,1915-1919,"Rao Guangxiang,Li Ang);,Liu Yong);,Liu Bing","Rao GX,Li A,Liu Y,Liu B",Rao GX,,Chinese Academy of Sciences,"Predicting individual chronological age based on neuroimaging data is very promising and important for understanding the trajectory of normal brain development. In this work, we proposed a new model to predict brain age ranging from 12 to 30 years old, based on structural magnetic resonance imaging and a deep learning approach with reduced model complexity and computational cost. We found that this model can predict brain age accurately not only in the training set (N = 1721, mean absolute error is 1.89 in 10-fold cross validation) but in an independent validation set (N = 226, mean absolute error is 1.96), substantially outperforming the previous published models. Given the considerable accuracy and generalizability, it is promising to further deploy our model in the clinic and help to investigate the pathophysiology of neurodevelopmental disorders.","Brain age prediction,structural magnetic resonance image,Convolutional Neural Network,Inception Net,global average pooling",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
54,DEEP NETWORK-BASED FEATURE SELECTION FOR IMAGING GENETICS: APPLICATION TO IDENTIFYING BIOMARKERS FOR PARKINSON'S DISEASE,,,1920-1923,"Kim Mansu,Won Ji Hye,Hong Jisu,Kwon Junmo,Park Hyunjin,Shen Li","Kim M,Won JH,Hong J,Kwon J,Park H,Shen L",Shen L,,University of Pennsylvania,"Imaging genetics is a methodology for discovering associations between imaging and genetic variables. Many studies adopted sparse models such as sparse canonical correlation analysis (SCCA) for imaging genetics. These methods are limited to modeling the linear imaging genetics relationship and cannot capture the non-linear high-level relationship between the explored variables. Deep learning approaches are underexplored in imaging genetics, compared to their great successes in many other biomedical domains such as image segmentation and disease classification. In this work, we proposed a deep learning model to select genetic features that can explain the imaging features well. Our empirical study on simulated and real datasets demonstrated that our method outperformed the widely used SCCA method and was able to select important genetic features in a robust fashion. These promising results indicate our deep learning model has the potential to reveal new biomarkers to improve mechanistic understanding of the studied brain disorders.","Imaging genetics,feature selection,deep learning,Parkinson's disease",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
55,DEEP MULTIMODAL BRAIN NETWORK LEARNING FOR JOINT ANALYSIS OF STRUCTURAL MORPHOMETRY AND FUNCTIONAL CONNECTIVITY,,,1924-1928,"Zhang Wen,Wang Yalin","Zhang W,Wang YL",Zhang W,,Arizona State University,"Learning from the multimodal brain imaging data attracts a large amount of attention in medical image analysis due to the proliferation of multimodal data collection. It is widely accepted that multimodal data can provide complementary information than mining from a single modality. However, unifying the image-based knowledge from the multimodal data is very challenging due to different image signals, resolution, data structure, etc.. In this study, we design a supervised deep model to jointly analyze brain morphometry and functional connectivity on the cortical surface and we name it deep multimodal brain network learning (DMBNL). Two graph-based kernels, i.e., geometry-aware surface kernel (GSK) and topology-aware network kernel (TNK), are proposed for processing the cortical surface morphometry and brain functional network. The vertex features on the cortical surface from GSK is pooled and feed into TNK as its initial regional features. In the end, the graph-level feature is computed for each individual and thus can be applied for classification tasks. We test our model on a large autism imaging dataset. The experimental results prove the effectiveness of our model.","Multimodal fusion,brain cortical surface,functional connectivity,deep learning,graph",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8130829,
56,AUTOMATIC DEPRESSION DETECTION VIA FACIAL EXPRESSIONS USING MULTIPLE INSTANCE LEARNING,,,1933-1936,"Wang Yanfei,Ma Jie,Hao Bibo,Hu Pengwei,Wang Xiaoqian,Mei Jing,Li Shaochun","Wang YF,Ma J,Hao BB,Hu PW,Wang XQ,Mei J,Li SC",Wang YF,,"IBM Res, Beijing, Peoples R China.","Depression affects more than 300 million people around the world and is the leading cause of disability in USA for individuals ages from 15 to 44. The damage of it compares to most common diseases like cancer, diabetes, or heart disease according to the WHO report. However, people with depression symptoms sometimes do not receive proper treatment due to access barriers. In this paper, we propose a method that automatically detects depression using only landmarks of facial expressions, which are easy to collect with less privacy exposure. We deal with the coarse-grained labels i.e. one final label for the long-time series video clips, which is the common cases in applications, through the integration of feature manipulation and multiple instance learning. The effectiveness of our method is compared to other visual based methods, and our method even outperforms multi-modal methods that use multiple modalities.","depression,multiple instance learning,average pooling,visual features,mental health",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
57,SUPERVISED LEARNING FOR SEGMENTING OPEN BOUNDARIES IN MEDICAL IMAGES,,,1945-1948,"Amer Karim,Jacob Athira,Funka-Lea Gareth,El-Zehiry Noha","Amer K,Jacob A,Funka-Lea G,El-Zehiry N",Amer K,,Siemens AG,"Image segmentation is one of the most important building blocks in many medical imaging applications. Often, it is the first step in any artificial intelligence (AI) assisted diagnosis system.
Most convolutional neural network image-to-image segmentation algorithms compute a binary mask segmentation and extract the object boundary as the edge of the binary mask always leading to a closed boundary.
In this paper, we present a novel image-to-image segmentation algorithm that learns open boundaries. The object delineation is directly learnt by training a U-Net like network on a distance map representation of the boundary without any constraints on its shape or topology. We validate the proposed approach on the segmentation of the left atrium in intra-cardiac echocardiography images. For this application, it is important to produce segmentations only where a strong evidence of the anatomy exists.
To our knowledge, this is the first work to train a U-Net on a distance map ground truth representation for open boundary segmentation.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
58,A CONTEXT BASED DEEP LEARNING APPROACH FOR UNBALANCED MEDICAL IMAGE SEGMENTATION,,,1949-1953,"Murugesan Balamurali,Sarveswaran Kaushik,Raghavan Vijaya S.,Shankaranarayana Sharath M.,Ram Keerthi,Sivaprakasam Mohanasankar","Murugesan B,Sarveswaran K,Raghavan SV,Shankaranarayana SM,Ram K,Sivaprakasam M",Murugesan B,,Indian Institute of Technology System (IIT System),"Automated medical image segmentation is an important step in many medical procedures. Recently, deep learning networks have been widely used for various medical image segmentation tasks, with U-Net and generative adversarial nets (GANs) being some of the commonly used ones. Foreground-background class imbalance is a common occurrence in medical images, and U-Net has difficulty in handling class imbalance because of its cross entropy (CE) objective function. Similarly, GAN also suffers from class imbalance because the discriminator looks at the entire image to classify it as real or fake. Since the discriminator is essentially a deep learning classifier, it is incapable of correctly identifying minor changes in small structures. To address these issues, we propose a novel context based CE loss function for U-Net, and a novel architecture Seg-GLGAN. The context based CE is a linear combination of CE obtained over the entire image and its region of interest (ROI). In Seg-GLGAN, we introduce a novel context discriminator to which the entire image and its ROI are fed as input, thus enforcing local context. We conduct extensive experiments using two challenging unbalanced datasets: PROMISE12 and ACDC. We observe that segmentation results obtained from our methods give better segmentation metrics as compared to various baseline methods.","Medical image segmentation,Deep learning,Generative adversarial networks,Class imbalance",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,http://arxiv.org/pdf/2001.02387,
59,SEGMENTATION AND UNCERTAINTY MEASURES OF CARDIAC SUBSTRATES WITHIN OPTICAL COHERENCE TOMOGRAPHY IMAGES VIA CONVOLUTIONAL NEURAL NETWORKS,,,1958-1961,"Huang Ziyi,Gan Yu,Lye Theresa,Theagene Darnel,Chintapalli Spandana,Virdi Simeran,Laine Andrew,Angelini Elsa,Hendon Christine P.","Huang ZY,Gan Y,Lye T,Theagene D,Chintapalli S,Virdi S,Laine A,Angelini E,Hendon CP",Huang ZY,,Columbia University,"Segmentation of human cardiac tissue has a great potential to provide critical clinical guidance for Radiofrequency Ablation (RFA). Uncertainty in cardiac tissue segmentation is high because of the ambiguity of the subtle boundary and intra-/inter-physician variations. In this paper, we proposed a deep learning framework for Optical Coherence Tomography (OCT) cardiac segmentation with uncertainty measurement. Our proposed method employs additional dropout layers to assess the uncertainty of pixel-wise label prediction. In addition, we improve the segmentation performance by using focal loss to put more weights on mis-classified examples. Experimental results show that our method achieves high accuracy on pixel-wise label prediction. The feasibility of our method for uncertainty measurement is also demonstrated with excellent correspondence between uncertain regions within OCT images and heterogeneous regions within corresponding histology images.","Optical coherence tomography,Convolutional neural networks,Cardiac tissue imaging,Semantic segmentation",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,"RETINAL,LAYER,CATHETER",,,
60,SEGMENTATION OF FIVE COMPONENTS IN FOUR CHAMBER VIEW OF FETAL ECHOCARDIOGRAPHY,,,1962-1965,"Yang Tingyang,Han Jiancheng,Zhu Haogang,Li Tiantian,Liu Xiaowei,Gu Xiaoyan,Liu Xiangyu,An Shan,Zhang Yingying,Zhang Ye","Yang TY,Han JC,Zhu HG,Li TT,Liu XW,Gu XY,Liu XY,An S,Zhang YY,Zhang Y",Yang TY,,Beihang University,"It is clinically significant to segment five components in four chamber view of fetal echocardiography, including four chambers and the descending aorta. This study completes the multi-disease segmentation and multi-class semantic segmentation of the five key components. After comparing the performance of DeeplabV3+ and U-net in the segmentation task, we choose the former as it provides accurate segmentation in other six disease groups as well as the normal group. With the data proportion balance strategy, the segmentation performance of the Ebstein's anomaly group is improved significantly in spite of its small proportion. We empirically evaluate this strategy in terms of mean iou (MIOU), cross entropy loss (CE) and dice score (DS). The proportion of the atrial abnormality and ventricular abnormality in the entire data set is increased, so that the model learns more semantics. We simulate multiple scenes with uncertain attitudes of the fetus, which provides rich multi-scene semantic information and enhances the robustness of the model.","Four chamber view,Fetal echocardiography,Multi-class segmentation,Data augmentation",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
61,MEMORY-AUGMENTED ANOMALY GENERATIVE ADVERSARIAL NETWORK FOR RETINAL OCT IMAGES SCREENING,,,1971-1974,"Zhang Chengfen,Wang Yue,Zhao Xinyu,Guo Yan,Xie Guotong,Lv Chuanfeng,Lv Bin","Zhang CF,Wang Y,Zhao XY,Guo Y,Xie GT,Lv CF,Lv B",Zhang CF,,"PingAn Technol Shenzhen Co Ltd, Shenzhen, Peoples R China.","Optical coherence tomography (OCT) plays an important role in retinal disease screening. Traditional classification-based screening methods require complicated annotation works. Due to the difficulty of collecting abnormal samples, some anomaly detection methods have been applied to screen retinal lesions only based on normal samples. However, most existing anomaly detection methods are time consuming and easily misjudging abnormal OCT images with implicit lesions like small drusen. To solve these problems, we propose a memory-augmented anomaly generative adversarial network (MA-GAN) for retinal OCT screening. Within the generator, we establish a memory module to enhance the detail expressing abilities of typical OCT normal patterns. Meanwhile, the discriminator of MAGAN is decomposed orthogonally so that it has the encoding ability simultaneously. As a result, the abnormal image can be screened by the greater difference in the distribution of pixels and features between the original image and its reconstructed image. The model trained with 13000 normal OCT images reaches 0.875 AUC on the test set of 2000 normal images and 1000 anomalous images. And the inference time only takes 35 milliseconds for each image. Compared to other anomaly detection methods, our MA-GAN has the advantages in model accuracy and computation time for retinal OCT screening.","Optical coherence tomography,Generative adversarial networks,Anomaly detection",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
62,FULL FIELD OPTICAL COHERENCE TOMOGRAPHY IMAGE DENOISING USING DEEP LEARNING WITH SPATIAL COMPOUNDING,,,1975-1978,"Chen I-Ling,Ho Tuan-Shu,Lu Chih-Wei","Chen IL,Ho TS,Lu CW",Chen IL,,"Apollo Med Opt, Lane 188,Ruiguang Rd 43, Taipei, Taiwan.","In recent years, deep learning is widely and successfully applied in the medical images which have been established an abundant database in clinical practice. OCT is a relatively new imaging technique and worth in-depth exploration in the deep learning field, however, it is still in an early stage where medical doctors are learning to interpret its images. For shortening the learning curve, this paper used a deep convolutional neural network on a high-resolution full-field OCT system to enhance features in images. By combining with the spatial compounding technique, a noise map prediction method can be employed to discriminate noises from signals and thus increase the image quality. For 100 testing samples, the average of PSNR and SSIM have improved from 20.7 and 0.43 to 26.55 and 0.68 after denoising by the proposed denoising model. Moreover, some important features would be more distinct to support diagnosis in clinical data.","Optical coherence tomography,convolutional neural networks,image denoising,residual learning,spatial compounding",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,"SPECKLE,REDUCTION",,,
63,WEAKLY SUPERVISED VULNERABLE PLAQUES DETECTION BY IVOCT IMAGE,,,1983-1986,"Shi Peiwen,Xin Jingmin);,Zheng Nanning","Shi PW,Xin JM,Zheng NN",Shi PW,,Xi'an Jiaotong University,"Vulnerable plaque is a major factor leading to the onset of acute coronary syndrome (ACS), and accordingly, the detection of vulnerable plaques (VPs) could guide cardiologists to provide appropriate surgical treatments before the occurrence of an event. In general, hundreds of images are acquired for each patient during surgery. Hence a fast and accurate automatic detection algorithm is needed. However, VPs' detection requires extensive annotation of lesion's boundary by an expert practitioner, unlike diagnoses. Therefore in this paper, a multiple instances learning-based method is proposed to locate VPs with the image-level labels only. In the proposed method, the clip proposal module, the feature extraction module, and the detection module are integrated to recognize VPs and detect the lesion area. Finally, experiments are performed on the 2017 IVOCT dataset to examine the task of weakly supervised detection of VPs. Although the bounding box of VPs is not used, the proposed method yields comparable performance with supervised learning methods.","Weakly-supervised deep learning,multiple instance learning,plaque detection,IVOCT",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
64,FUSING METADATA AND DERMOSCOPY IMAGES FOR SKIN DISEASE DIAGNOSIS,,,1996-2000,"Li Weipeng,Zhuang Jiaxin,Wang Ruixuan,Zhang Jianguo,Zheng Wei-Shi","Li WP,Zhuang JX,Wang RX,Zhang JG,Zheng WS",Li WP,,Sun Yat Sen University,"To date, it is still difficult and challenging to automatically classify dermoscopy images. Although the state-of-the-art convolutional networks were applied to solve the classification problem and achieved overall decent prediction results, there is still room for performance improvement, especially for rare disease categories. Considering that human dermatologists often make use of other information (e.g., body locations of skin lesions) to help diagnose, we propose using both dermoscopy images and non-image metadata for intelligent diagnosis of skin diseases. Specifically, the metadata information is innovatively applied to control the importance of different types of visual information during diagnosis. Comprehensive experiments with various deep learning model architectures demonstrated the superior performance of the proposed fusion approach especially for relatively rare diseases. All our codes will be made publicly available(1).","Skin disease classification,metadata,data fusion",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,"CLASSIFICATION,CANCER",,,
65,KAPPA LOSS FOR SKIN LESION SEGMENTATION IN FULLY CONVOLUTIONAL NETWORK,,,2001-2004,"Zhang Jing,Petitjean Caroline,Ainouz Samia","Zhang J,Petitjean C,Ainouz S",Zhang J,,"Normandie Univ, INSA Rouen, UNIROUEN, UNIHAVRE,LITIS, Rouen, France.","Skin melanoma represents a major health issue. Today, diagnosis and follow-up can rely on computer-aided diagnosis tools, to help dermatologists segment and quantitatively describe the image content. In particular, deep convolutional neural networks (CNN) have lately been become the state-of-the-art in automated medical image segmentation. The loss function plays an important role in CNN in the backpropagation process. In this work, we propose a metric-inspired loss function, based on the Kappa index. Unlike the Dice loss, a standard loss used in image segmentation CNN, the Kappa loss takes into account all the pixels in the image, including the true negative - we believe this can improve the accuracy of the evaluation process between prediction and ground truth. We demonstrate the differentiability of the Kappa loss and present some results on six public datasets of skin lesion. Experiments have shown promising results in skin lesion segmentation.","Image segmentation,melanoma,loss function,agreement index,U-Net",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
66,A MULTI-TASK SELF-SUPERVISED LEARNING FRAMEWORK FOR SCOPY IMAGES,,,2005-2009,"Li Yuexiang,Chen Jiawei,Zheng Yefeng","Li YX,Chen JW,Zheng YF",Li YX,,Shenzhen University,"The training of deep learning models requires large amount of training data. However, as the annotations of medical data are difficult to acquire, the quantity of annotated medical images is often not enough to well train the deep learning networks. In this paper, we propose a novel multi-task self-supervised learning framework, namely ColorMe, for the scopy images, which deeply exploits the rich information embedded in raw data and looses the demand of training data. The approach pre-trains neural networks on multiple proxy tasks, i.e., green to red/blue colorization and color distribution estimation, which are defined in terms of the prior-knowledge of scopy images. Compared to the train-from-scratch strategy, fine-tuning from these pre-trained networks leads to a better accuracy on various tasks - cervix type classification and skin lesion segmentation.","Self-supervised learning,Multi-task learning,Colorization",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
67,COMPLEMENTARY NETWORK WITH ADAPTIVE RECEPTIVE FIELDS FOR MELANOMA SEGMENTATION,,,2010-2013,"Guo Xiaoqing,Chen Zhen,Yuan Yixuan","Guo XQ,Chen Z,Yuan YX",Guo XQ,,City University of Hong Kong,"Automatic melanoma segmentation in dermoscopic images is essential in computer-aided diagnosis of skin cancer. Existing methods may suffer from the hole and shrink problems with limited segmentation performance. To tackle these issues, we propose a novel complementary network with adaptive receptive filed learning. Instead of regarding the segmentation task independently, we introduce a foreground network to detect melanoma lesions and a background network to mask non-melanoma regions. Moreover, we propose adaptive atrous convolution (AAC) and knowledge aggregation module (KAM) to fill holes and alleviate the shrink problems. AAC explicitly controls the receptive field at multiple scales and KAM convolves shallow feature maps by dilated convolutions with adaptive receptive fields, which are adjusted according to deep feature maps. In addition, a novel mutual loss is proposed to utilize the dependency between the foreground and background networks, thereby enabling the reciprocally influence within these two networks. Consequently, this mutual training strategy enables the semi-supervised learning and improve the boundary-sensitivity. Training with Skin Imaging Collaboration (ISIC) 2018 skin lesion segmentation dataset, our method achieves a dice co-efficient of 86.4% and shows better performance compared with state-of-the-art melanoma segmentation methods(1).","Melanoma segmentation,adaptive receptive fields,semi-supervised learning",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,http://arxiv.org/pdf/2001.03893,
68,LEVERAGING ADAPTIVE COLOR AUGMENTATION IN CONVOLUTIONAL NEURAL NETWORKS FOR DEEP SKIN LESION SEGMENTATION,,,2014-2017,"Saha Anindo,Prasad Prem,Thabit Abdullah","Saha A,Prasad P,Thabit A",Saha A,,Universitat de Girona,"Fully automatic detection of skin lesions in dermatoscopic images can facilitate early diagnosis and repression of malignant melanoma and non-melanoma skin cancer. Although convolutional neural networks are a powerful solution, they are limited by the illumination spectrum of annotated dermatoscopic screening images, where color is an important discriminative feature. In this paper, we propose an adaptive color augmentation technique to amplify data expression and model performance, while regulating color difference and saturation to minimize the risks of using synthetic data. Through deep visualization, we qualitatively identify and verify the semantic structural features learned by the network for discriminating skin lesions against normal skin tissue. The overall system achieves a Dice Ratio of 0.891 with 0.943 sensitivity and 0.932 specificity on the ISIC 2018 Testing Set for segmentation.","dermatoscopy,melanoma,convolutional neural network,color augmentation,segmentation,lesion",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,http://arxiv.org/pdf/2011.00148,
69,DEEP DISENTANGLED REPRESENTATION LEARNING OF PET IMAGES FOR LYMPHOMA OUTCOME PREDICTION,,,2018-2021,"Guo Yu,Decazes Pierre,Becker Stephanie,Li Hua,Ruan Su","Guo Y,Decazes P,Becker S,Li H,Ruan S",Guo Y,,Tianjin University,"Image feature extraction based on deep disentangled representation learning of PET images is proposed for the prediction of lymphoma treatment response. Our method encodes PET images as spatial representations and modality representations by performing supervised tumor segmentation and image reconstruction. In this way, the whole image features (global features) as well as tumor region features (local features) can be extracted without the labor-intensive tumor segmentation and feature calculation procedure. The learned global and local image features are then joined with several prognostic factors evaluated by physicians based on clinical information, and used as input of a SVM classifier for predicting outcome results of lymphoma patients. In this study, 186 lymphoma patient data were included for training and testing the proposed model. The proposed method was compared with the traditional straightforward feature extraction method. The better prediction results of the proposed method also show its efficiency for prognostic prediction related feature extraction in PET images.","lymphoma,treatment response prediction,PET images,spatial representation,modality representation",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
70,"MuscNet, a Weighted Voting Model of Multi-Source Connectivity Networks to Predict Mild Cognitive Impairment Using Resting-State Functional MRI",8,,174023-174031,"Li Jialiang,Yao Zhaomin,Duan Meiyu,Liu Shuai,Li Fei,Zhu Haiyang,Xia Zhiqiang,Huang Lan,Zhou Fengfeng","Li JL,Yao ZM,Duan MY,Liu S,Li F,Zhu HY,Xia ZQ,Huang L,Zhou FF",Zhou FF,10.1109/ACCESS.2020.3025828,Jilin University,"The neurological disorder mild cognitive impairment (MCI) demonstrates minor impacts on the patient's daily activities and may be ignored as the status of normal aging. But some of the MCI patients may further develop into severe statuses like Alzheimer's disease (AD). The brain functional connectivity network (BFCN) was usually constructed from the resting-state functional magnetic resonance imaging (rs-fMRI) data. This technology has been widely used to detect the neurodegenerative dementia and to reveal the intrinsic mechanism of neural activities. The BFCN edge was usually determined by the pairwise correlation between the brain regions. This study proposed a weighted voting model of multi-source connectivity networks (MuscNet) by integrating multiple BFCNs of different correlation coefficients. Our model was further improved by removing redundant features. The experimental data demonstrated that different BFCNs contributed complementary information to each other and MuscNet outperformed the existing models on detecting MCI patients. The previous study suggested the existence of multiple solutions with similarly good performance for a machine learning problem. The proposed model MuscNet utilized a weighted voting strategy to slightly outperform the existing studies, suggesting an effective way to fuse multiple base models. The reason may need further theoretical investigations about why different base models contribute to each other for the MCI prediction.","Correlation,Correlation coefficient,Brain modeling,Measurement,Dementia,Time series analysis,Mild cognitive impairment,Alzheimer's disease,resting-state functional MRI,brain functional connectivity network,multi-source connectivity network,weighted voting model,MuscNet",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,"ALZHEIMERS-DISEASE,CINGULATE,CORTEX,CEREBELLUM",IEEE ACCESS,https://ieeexplore.ieee.org/ielx7/6287639/8948470/09203784.pdf,
71,IoT architecture for advanced manufacturing technologies,22,,2359-2365,"Nabeel Muhammed M.,Srinivasan M.,Prince Easter,Padmanabhan R.","Nabeel MM,Srinivasan M,Prince E,Padmanabhan R",Nabeel MM,,VIT Chennai,"Real time tracking of the manufacturing parts in an assembly is a current area of focus in Industry 4.0.And group technology is widely used to classify the manufacturing parts in many manufacturing scenarios. Using controllers attached to RFID module and accelerometers, bulk value of data is produced. This data can be processed further to produce valuable information such as, productivity, stage of processing, quality of the part and machine/tool life prediction. The developed architecture continuously checks for RFID cards. On reading a RFID card, the controller identifies apart and starts storing sensor information until a second RFID reader detects the same part, after a process. This process continues for all incoming parts and the information is stored in a database which can be further processed for valuable output. The machined parts are tracked until assembly and are grouped with respect to geometry, machine operation and materials used. Data such as vibration during machine operation is acquired to monitor the quality of the part and to understand the wear of the tool. The system works with a microcontroller, multiple RFID, SD module, Wi-Fi module and uses SPI protocol for operation and is capable of ensuring the quality of the process and the product. The developed architecture for RFID embedded solution for group technology is integrated with data analytics engine and machine learning module, thereby enabling real-time tracking of part quality and productivity. (C) 2019 Elsevier Ltd. All rights reserved.","RFID based Industry system,Vibration analysis,SPI protocol,CNC machining",Proceedings Paper,"ELSEVIER, RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS",Materials Science,,,SYSTEM,MATERIALS TODAY-PROCEEDINGS,,
72,Electroencephalogram-Based Preference Prediction Using Deep Transfer Learning,8,,176818-176829,"Aldayel Mashael S.,Ykhlef Mourad,Al-Nafjan Abeer N.","Aldayel MS,Ykhlef M,Al-Nafjan AN",Al-Nafjan AN,10.1109/ACCESS.2020.3027429,Al-Imam Muhammad Ibn Saud Islamic University,"Transfer learning is an approach in machine learning where a model that was built and trained on one task is re-purposed on a second task. The success of transfer learning in computer vision has motivated its use in neuroscience. Although common in image recognition, the use of transfer learning in EEG classification remains unexplored. Most EEG-based neuroscience studies depend on using traditional machine learning algorithms to answer a question, rather than on improving the algorithms. Developing algorithms for transfer learning for EEG can also assist with problems of low data availability in EEG classification. The primary objective of this study is to investigate EEG-based transfer learning and propose deep transfer learning models to transfer knowledge from emotion recognition to preference recognition to enhance the classification prediction accuracy. To the best of our knowledge, this is the first study demonstrating the effect of applying deep transfer learning between EEG-based emotion recognition and EEG-based preference detection. We propose different approaches for deep transfer learning models to detect preferences from EEG signals using the preprocessed DEAP dataset. Two types of features were extracted from EEG signals, namely the power spectral density and valence. We built three models of deep neural networks: basic without transfer learning, fine-tuning of deep transfer learning, and retraining of deep transfer learning. We compared the performance of deep transfer learning with those of deep neural networks and other conventional classification algorithms such as support vector machine, random forest, and k-nearest neighbor. Although the deep neural network classifiers achieved a high accuracy of greater than 87%, deep transfer learning achieved the highest accuracy result of 93%. The results demonstrate that although the proposed deep transfer learning approaches exhibit higher accuracy than the support vector machine and k-nearest neighbor classifiers, random forest achieves results similar to those of deep transfer learning.","Electroencephalography,Brain modeling,Task analysis,Emotion recognition,Extraterrestrial measurements,Biological neural networks,Cognition,Data mining,brain-computer interfaces,emotion recognition,supervised learning,artificial neural networks,signal processing,consumer behavior",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,"EEG,CLASSIFICATION,RESPONSES,ASYMMETRY,AROUSAL,BRAIN",IEEE ACCESS,https://doi.org/10.1109/access.2020.3027429,
73,An Unsupervised Retinal Vessel Segmentation Using Hessian and Intensity Based Approach,8,,165056-165070,"Alhussein Musaed,Aurangzeb Khursheed,Haider Syed Irtaza","Alhussein M,Aurangzeb K,Haider SI",Aurangzeb K,10.1109/ACCESS.2020.3022943,King Saud University,"The structure of blood vessels play a crucial role in diagnoses of the various vision threatening diseases including Glaucoma and Diabetic Retinopathy (DR). The correct segmentation of retinal blood vessels is a crucial step in the study of retinal fundus images. We proposed a simple unsupervised approach by using a combination of Hessian based approach and intensity transformation approach. We have applied CLAHE for enhancing the contrast of the retinal fundus images. An enhanced version of PSO algorithm is applied for contextual region tuning of CLAHE. Morphological filter and Wiener filter are used to de-noise the enhanced image. The eigenvalues are obtained from the Hessian matrix at two different scales to extract thick and thin vessel enhanced images separately. The intensity transformation approach is separately applied to the enhanced image to maximize the vessel details. Global Otsu thresholding is applied on intensity transformed image and thick vessel enhanced image whereas ISODATA local thresholding is applied on thin vessel enhanced image. Finally, a simple post-processing step based on the region parameters such as area, eccentricity, and solidity is used. The region parameters are obtained for each connected component in input binary images. The threshold values of region parameters are empirically investigated and applied to each of the three binary images to remove the non-vessel components. The thresholded images are combined by applying logical OR operator, which resulted in the final segmented binary image. We assessed our developed framework on the open-access CHASE_DB1 and DRIVE datasets, achieving a sensitivity of 0.7776 and 0.7851, and an accuracy of 0.9505 and 0.9559 respectively. These results outperform several state-of-the-art unsupervised methods. The reduced computational complexity and significantly improved evaluation metrics advocates for its use in the automated diagnostic systems for retinal image analysis.","Machine learning,vessel segmentation,CLAHE,morphology,Wiener filter",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,"BLOOD-VESSELS,EXTRACTION,TECHNIQUES,MATCHED-FILTER,OPTIC,DISC,IMAGES,MODEL",IEEE ACCESS,https://ieeexplore.ieee.org/ielx7/6287639/8948470/09189768.pdf,
74,A Deep Learning Pipeline for Identification of Motor Units in Musculoskeletal Ultrasound,8,,170595-170608,"Ali Hazrat,Umander Johannes,Rohlen Robin,Gronlund Christer","Ali H,Umander J,Rohlen R,Gronlund C",Gronlund C,10.1109/ACCESS.2020.3023495,Umea University,"Skeletal muscles are functionally regulated by populations of so-called motor units (MUs). An MU comprises a bundle of muscle fibers controlled by a neuron from the spinal cord. Current methods to diagnose neuromuscular diseases and monitor rehabilitation, and study sports sciences rely on recording and analyzing the bio-electric activity of the MUs. However, these methods provide information from a limited part of a muscle. Ultrasound imaging provides information from a large part of the muscle. It has recently been shown that ultrafast ultrasound imaging can be used to record and analyze the mechanical response of individual MUs using blind source separation. In this work, we present an alternative method - a deep learning pipeline - to identify active MUs in ultrasound image sequences, including segmentation of their territories and signal estimation of their mechanical responses (twitch train). We train and evaluate the model using simulated data mimicking the complex activation pattern of tens of activated MUs with overlapping territories and partially synchronized activation patterns. Using a slow fusion approach (based on 3D CNNs), we transform the spatiotemporal image sequence data to 2D representations and apply a deep neural network architecture for segmentation. Next, we employ a second deep neural network architecture for signal estimation. The results show that the proposed pipeline can effectively identify individual MUs, estimate their territories, and estimate their twitch train signal at low contraction forces. The framework can retain spatio-temporal consistencies and information of the mechanical response of MU activity even when the ultrasound image sequences are transformed into a 2D representation for compatibility with more traditional computer vision and image processing techniques. The proposed pipeline is potentially useful to identify simultaneously active MUs in whole muscles in ultrasound image sequences of voluntary skeletal muscle contractions at low force levels.","Machine learning,Image segmentation,Image sequences,Ultrasonic imaging,Muscles,Pipelines,Convolution,Motor unit,decomposition,ultrafast ultrasound,medical imaging,deep learning,mechanical response,neural networks,recurrent neural networks",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,"NEURAL-NETWORKS,EMG,MUSCLE,WAVES,IMAGE,NOISE",IEEE ACCESS,http://arxiv.org/pdf/2010.03028,
75,Efficient Lung Nodule Classification Using Transferable Texture Convolutional Neural Network,8,,175859-175870,"Ali Imdad,Muzammil Muhammad,Ul Haq Ihsan,Khaliq Amir A.,Abdullah Suheel","Ali I,Muzammil M,Ul Haq I,Khaliq AA,Abdullah S",Muzammil M,10.1109/ACCESS.2020.3026080,"Int Islamic Univ, Fac Engn & Technol, Islamabad 44000, Islamabad, Pakistan.","Lung nodules are vital indicators for the presence of lung cancer. An early detection enhances the survival rate of the patient by starting treatment at the right time. The detection and classification of malignancy in Computed Tomography (CT) images is a very time-consuming and difficult task for radiologists which lead the researchers to develop algorithms for Computer-Aided Diagnosis (CAD) systems to mitigate this burden. The performance of CAD systems is continuously improving by using various deep learning techniques for screening of lung cancer. In this paper, we proposed transferable texture Convolutional Neural Networks (CNN) to improve the classification performance of pulmonary nodules in CT scans. An Energy Layer (EL) is incorporated in our scheme, which extracts texture features from the convolutional layer. The inclusion of EL reduces the number of learnable parameters of the network, which further reduces the memory requirements and computational complexity. The proposed model has only three convolutional layers and one EL, instead of pooling layer. Overall proposed CNN architecture comprises of nine layers for automatic feature extraction and classification of pulmonary nodule candidates as malignant or benign. Furthermore, the pre-trained model of proposed CNN is also used to handle the smaller dataset classification problem by using transfer learning. This work has been evaluated on publicly available LIDC-IDRI and the LUNGx Challenge database through different evaluation matrices, such as; the accuracy, specificity, error rate and AUC. The proposed model is trained by six-fold cross-validation and achieved an accuracy score of 96.69%+/- 0.72% with only 3.30%+/- 0.72% error rate. Whereas, the measured AUC and recall is 99.11%+/- 0.45% and 97.19%+/- 0.57%, respectively. Moreover, we also tested our proposed technique on the MNIST dataset and achieved state-of-the-art results in terms of accuracy and error rate.","Lung,Feature extraction,Cancer,Databases,Computed tomography,Biomedical imaging,Task analysis,Computed tomography,cancer detection,computer aided diagnosis,image classification,machine learning,transfer learning,lung nodule,CNN,LIDC-IDRI,LUNGx challenge",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,"COMPUTER-AIDED,DIAGNOSIS,SHAPE",IEEE ACCESS,https://doi.org/10.1109/access.2020.3026080,
76,Predictive Modeling of Antibiotic Susceptibility in E. Coli Strains Using the U-Net Network and One-Class Classification,8,,167711-167720,"Ali Nairveen,Kirchhoff Johanna,Onoja Patrick Igoche,Tannert Astrid,Neugebauer Ute,Popp Jurgen,Bocklitz Thomas","Ali N,Kirchhoff J,Onoja PI,Tannert A,Neugebauer U,Popp J,Bocklitz T",Bocklitz T,10.1109/ACCESS.2020.3022829,Friedrich Schiller University of Jena,"The antibiotic resistance of bacterial pathogens has become one of the most serious global health issues due to misusing and overusing of antibiotics. Recently, different technologies were developed to determine bacteria susceptibility towards antibiotics; however, each of these technologies has its advantages and limitations in clinical applications. In this contribution, we aim to assess and automate the detection of bacterial susceptibilities towards three antibiotics; i.e. ciprofloxacin, cefotaxime and piperacillin using a combination of image processing and machine learning algorithms. Therein, microscopic images were collected from different E. coli strains, then the convolutional neural network U-Net was implemented to segment the areas showing bacteria. Subsequently, the encoder part of the trained U-Net was utilized as a feature extractor, and the U-Net bottleneck features were utilized to predict the antibiotic susceptibility of E. coli strains using a one-class support vector machine (OCSVM). This one-class model was always trained on images of untreated controls of each bacterial strain while the image labels of treated bacteria were predicted as control or non-control images. If an image of treated bacteria is predicted as control, we assume that these bacteria resist this antibiotic. In contrast, the sensitive bacteria show different morphology of the control bacteria; therefore, images collected from these treated bacteria are expected to be classified as non-control. Our results showed 83% area under the receiver operating characteristic (ROC) curve when OCSVM models were built using the U-Net bottleneck features of control bacteria images only. Additionally, the mean sensitivities of these one-class models are 91.67% and 86.61% for cefotaxime and piperacillin; respectively. The mean sensitivity for the prediction of ciprofloxacin is only 59.72% as the bacteria morphology was not fully detected by the proposed method.","Antibiotic resistance,E. coli strains,U-Net convolutional neural network,one-class SVM",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,IDENTIFICATION,IEEE ACCESS,https://ieeexplore.ieee.org/ielx7/6287639/8948470/09187776.pdf,
77,A Machine Learning Framework for Pulse Detection During Out-of-Hospital Cardiac Arrest,8,,161031-161041,"Alonso Erik,Irusta Unai,Aramendi Elisabete,Daya Mohamud R.","Alonso E,Irusta U,Aramendi E,Daya MR",Alonso E,10.1109/ACCESS.2020.3021310,University of Basque Country,"The availability of an automatic pulse detection during out-of-hospital cardiac arrest (OHCA) would allow the rapid identification of cardiac arrest and the prompt detection of return of spontaneous circulation. The aim of this study was to develop a reliable pulse detection algorithm using the electrocardiogram (ECG) and thoracic impedance (TI), the signals available in most defibrilators. The dataset used in the study consisted of 1140 ECG and TI segments from 187 OHCA patients, whereof 792 were labelled as pulse-generating rhythm (PR) and 348 as pulseless electrical activity (PEA) by a pool of experts in OHCA. First, an adaptive filtering scheme was used to extract the impedance circulation component and its first derivative from the TI. Then, the wavelet decomposition of the ECG was carried out to obtain the different subband components and the denoised ECG. Pulse/no-pulse (PR/PEA) discrimination features were extracted from those signals and fed into a support vector machine (SVM) classifier that made the pulse/no-pulse decision. A quasi-stratified and patient wise nested cross validation procedure was used to select the best feature subset and to tune the SVM hyperparameters. This procedure was repeated 50 times to estimate the statistical distributions of the performance metrics of the method. The optimal solution consisted in a five feature classifier that yielded a mean (standard deviation) sensitivity, specificity, balanced accuracy and total accuracy of 92.4% (0.7), 93.0% (0.8), 92.7% (0.5) and 92.6% (0.5), respectively. When compared to available methods, our solution presented an improvement in balanced accuracy of at least 2.5 points. A reliable pulse detection algorithm for OHCA using the signals available in defibrillators was acomplished.","Electrocardiography,Feature extraction,Cardiac arrest,Impedance,Support vector machines,Transforms,Machine learning,adaptive filtering,stationary wavelet transform (SWT),support vector machine (SVM),out-of-hospital cardiac arrest (OHCA),thoracic impedance,electrocardiogram (ECG),pulse detection",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,"CARDIOPULMONARY-RESUSCITATION,VENTRICULAR-FIBRILLATION,SPONTANEOUS,CIRCULATION,IMPEDANCE,CARDIOGRAM,SURVIVAL,ELECTROCARDIOGRAM,DEFIBRILLATOR,MANAGEMENT,PERSONNEL,CHECKING",IEEE ACCESS,https://ieeexplore.ieee.org/ielx7/6287639/8948470/09184797.pdf,
78,Design of a Wearable Wireless Multi-Sensor Monitoring System and Application for Activity Recognition Using Deep Learning,8,,169183-169195,"Ascioglu Gokmen,Senol Yavuz","Ascioglu G,Senol Y",Ascioglu G,10.1109/ACCESS.2020.3024003,Dokuz Eylul University,"Human activity monitoring and recognition systems assist experts in evaluating various health problems including obesity, cardiac diseases and, sports injury detection. However, these systems have two challenging points; monitoring activities for outdoor applications and extracting relevant features using hand-crafted techniques from multi-dimensional and large datasets. To address these challenges, we have focused on new dataset generation for activity recognition, a novel design of a sensor-based wireless activity monitoring system, and its application to deep learning neural networks. The designed monitoring system consists of one master and four slave devices, and can collect and record acceleration and gyroscope information. The slave devices were attached on arm, chest, thigh, and shank areas of the human body. Activity data were collected and recorded from sixty healthy people for thirteen activity types including drink from cup and cleaning table. These activities were divided into three activity categories as basic, complex, and all, which is the combination of basic and complex activities. Obtained datasets were fed into deep learning neural networks namely convolutional neural network (CNN), long-short term memory (LSTM) neural networks, and convolutional LSTM (ConvLSTM) neural networks. The performance of each neural network for each category type was separately examined. The results show that ConvLSTM outperforms CNN and LSTM as far as activity recognition is concerned.","Wireless sensor networks,Machine learning,Monitoring,Legged locomotion,Activity recognition,Neural networks,Activity recognition,deep learning,wireless sensor network,wearable sensors,data augmentation",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,"SENSOR,ACCELEROMETER,HEALTH",IEEE ACCESS,https://ieeexplore.ieee.org/ielx7/6287639/8948470/09195815.pdf,
79,Machine Learning-Based Automatic Detection of Central Sleep Apnea Events From a Pressure Sensitive Mat,8,,173428-173439,"Azimi Hilda,Xi Pengcheng,Bouchard Martin,Goubran Rafik,Knoefel Frank","Azimi H,Xi PC,Bouchard M,Goubran R,Knoefel F",Azimi H,10.1109/ACCESS.2020.3025808,University of Ottawa,"Polysomnography (PSG) is the standard test for diagnosing sleep apnea. However, the approach is obtrusive, time-consuming, and with limited access for patients in need of sleep apnea diagnosis. In recent years, there have been many attempts to search for an alternative device or approach that avoids the limitations of PSG. Pressure-sensitive mats (PSM) have proven to be able to detect central sleep apneas (CSA) and be a potential alternative for PSG. In the current study, we combine advanced machine learning approaches with a practical unobtrusive home monitoring device (PSM) to detect CSA events from data collected nocturnally and unattended. Two deep learning methods are implemented for the automatic detection of CSA events: a temporal convolutional network (TCN) and a bidirectional long short-term memory (BiLSTM) network. The deep learning models are compared to a classical machine learning approach (linear support vector machine, SVM) and a simple threshold-based algorithm. Considering the characteristics of each method, we choose strategies, including resampling and weighted cost-functions, to optimize the methods and to perform CSA detection as anomaly detection in an imbalanced data set. We evaluate the performance of all models on a database containing 7 days of data from 9 elderly patients. From the resulting 63 days, data from 7 patients (49 days) are devoted to training for optimizing hyperparameters, and data from 2 patients (14 days) are devoted to testing. Experimental results indicate that the best-performing model achieves an accuracy of 95.1% through training an BiLSTM network. Overall, the implemented deep learning methods achieve better performance than the conventional classification approach (SVM) and the simple threshold-based method, and show good potential for the use of PSM for practical unobtrusive monitoring of CSA.","Sensors,Sleep apnea,Machine learning,Monitoring,Feature extraction,Support vector machines,Data models,Biomedical measurement,data analysis,deep learning,machine learning,patient monitoring,pressure measurement,central sleep apnea detection",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,"NETWORK,CLASSIFIERS,VALIDATION,DIAGNOSIS",IEEE ACCESS,https://ieeexplore.ieee.org/ielx7/6287639/8948470/09203807.pdf,
80,Bridge Segmentation Performance Gap Via Evolving Shape Prior,8,,173961-173973,"Chen Chaoyu,Yang Xin,Dou Haoran,Huang Ruobing,Huang Xiaoqiong,Wang Xu,Duan Chong,Li Shengli,Xue Wufeng,Pheng Ann Heng","Chen CY,Yang X,Dou HR,Huang RB,Huang XQ,Wang X,Duan C,Li SL,Xue WF,Pheng PA",Ni D,10.1109/ACCESS.2020.3026073,Shenzhen University,"Deep neural networks are very compelling for medical image segmentation. However, deep models often suffer from notable performance drops in real clinical settings due to the complex appearance shift in daily scannings. Domain adaptation partially addresses the problem between imaging domains. However, it heavily depends on the expensive re-collection and re-training for domain-specific datasets and thus is not applicable to domain-agnostic images. In this paper, we propose a case adaptation strategy aiming to bridge the segmentation performance gap on domain-agnostic images. Our contribution is three-fold. First, we design a general self-supervised learning framework for case adaptation, which exploits its predictions as supervision to drive the adaptation. Without extra annotations and any burden on model complexity, the framework enables trained deep models at-hand to directly segment domain-agnostic testing images. Second, we propose a novel Evolving Shape Prior (ESP) which recursively introduces strong shape knowledge into networks and evolves with the adaptation procedure to provide adaptive supervision. ESP can stabilize self-supervised learning and guide it to move towards model convergence. Third, we perform extensive experiments on 10 datasets with different levels of difficulty and typical appearance shifts blended, proving our framework is a promising solution in reducing segmentation performance degradation. Through this work, we investigate the feasibility of case adaptation as a general strategy in enhancing the robustness of deep segmentation networks, with comprehensive analyses proving its efficacy and efficiency.","Image segmentation,Adaptation models,Shape,Biomedical imaging,Testing,Ultrasonic imaging,Data models,Deep neural networks,medical image segmentation,case adaptation,self-supervision,evolving shape prior",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,"WHOLE,HEART,SEGMENTATION,PROBABILISTIC,ATLAS,MODEL",IEEE ACCESS,https://doi.org/10.1109/access.2020.3026073,
81,Atrial Fibrillation Identification With PPG Signals Using a Combination of Time-Frequency Analysis and Deep Learning,8,,172692-172706,"Cheng Peng,Chen Zhencheng,Li Quanzhong,Gong Qiong,Zhu Jianming,Liang Yongbo","Cheng P,Chen ZC,Li QZ,Gong Q,Zhu JM,Liang YB",Chen ZC; Liang YB,10.1109/ACCESS.2020.3025374,Guilin University of Electronic Technology,"Atrial fibrillation (AF) is the most common persistent arrhythmia and is likely to cause strokes and damage to heart function in patients. Electrocardiogram (ECG) is the gold standard for detecting AF. However, ECGs have short boards with short monitoring cycles and problems with gathering. It is also difficult to detect a burst AF through ECG. In contrast, photoplethysmography (PPG) is easy to perform and suitable for long-term monitoring. In this study, we propose a method that combines time-frequency analysis with deep learning and identifies AF based on PPG. The advantage of the method is that there is no need for the noise filtering and feature extraction of PPG, and it has a high generalization capability. The data for the experiment came from three publicly accessible databases. The first part of the experimental method uses data augmentation to convert the 10 s PPG segment into a time-frequency chromatograph by means of time-frequency analysis. The second part inputs the chromatograph into a hybrid framework that combines a convolutional neural network (CNN) and long short-term memory (LSTM) for AF/nonAF classification. The experimental results show that the method has a high classification accuracy, sensitivity, specificity, and F1 score, which are equal to 98.21%, 98.00%, 98.07% and 98.13%, respectively. The area under the receiver operating characteristic curve (AUC) is 0.9959. The model we propose not only aids doctors in diagnosing AF but also provides a method for identifying AF through portable wearable devices.","Electrocardiography,Data models,Time-frequency analysis,Monitoring,Databases,Training,Machine learning,Atrial fibrillation,photoplethysmography (PPG),time-frequency analysis,convolutional neural networks (CNN),long short-term memory (LSTM)",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,"RESPIRATORY,RATE",IEEE ACCESS,https://ieeexplore.ieee.org/ielx7/6287639/8948470/09201275.pdf,
82,"Time-Frequency Analysis, Denoising, Compression, Segmentation, and Classification of PCG Signals",8,,160882-160890,"Chowdhury Md Tanzil Hoque,Poudel Khem Narayan,Hu Yating","Chowdhury MTH,Poudel KN,Hu YT",Chowdhury MTH,10.1109/ACCESS.2020.3020806,Middle Tennessee State University,"Phonocardigraphy (PCG) is the graphical representation of heart sounds. The PCG signal contains useful information about the functionality and the condition of the heart. It also provides an early indication of potential cardiac abnormalities. Extracting cardiac information from heart sounds and detecting abnormal heart sounds to diagnose heart diseases using the PCG signal can play a vital role in remote patient monitoring. In this paper, we have combined different signal processing techniques and a deep learning method to denoise, compress, segment, and classify PCG signals effectively and accurately. First, the PCG signal is denoised and compressed by using a multi-resolution analysis based on the Discrete Wavelet Transform (DWT). Then, a segmentation algorithm, based on the Shannon energy envelope and zero-crossing, is applied to segment the PCG signal into four major parts: the first heart sound (S1), the systole interval, the second heart sound (S2), and the diastole interval. Finally, Mel-scaled power spectrogram and Mel-frequency cepstral coefficients (MFCC) are employed to extract informative features from the PCG signal, which are then fed into a classifier to classify each PCG signal into a normal or an abnormal signal by using a deep learning approach. For the classification, a 5-layer feed-forward Deep Neural Network (DNN) model is used, and overall testing accuracy of around 97.10% is achieved. Besides providing valuable information regarding heart condition, this signal processing approach can help cardiologists take appropriate and reliable steps toward diagnosis if any cardiovascular disorder is found in the initial stage.","Phonocardiography,Heart,Feature extraction,Spectrogram,Time-frequency analysis,Discrete wavelet transforms,Mel frequency cepstral coefficient,Classification,deep neural network,denoising,discrete wavelet transform,phonocardiogram,segmentation,Shannon energy envelope,TensorFlow,murmur,zero-crossing",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,"SOUND,CLASSIFICATION,HEART,SOUNDS",IEEE ACCESS,https://ieeexplore.ieee.org/ielx7/6287639/8948470/09183915.pdf,
83,IHDS: Intelligent Harvesting Decision System for Date Fruit Based on Maturity Stage Using Deep Learning and Computer Vision,8,,167985-167997,"Faisal Mohammed,Alsulaiman Mansour,Arafah Mohammed,Mekhtiche Mohamed Amine","Faisal M,Alsulaiman M,Arafah M,Mekhtiche MA",Faisal M,10.1109/ACCESS.2020.3023894,King Saud University,"Date is the main fruit crop of the Kingdom of Saudi Arabia (KSA), approximately covering 72% of the total area under permanent crops. The Food and Agriculture Organization states that date production worldwide was 3,430,883 tons in 1990, which increases yearly, reaching 8,526,218 tons in 2018. Date production in KSA was around 527,881 tons in 1990, approximately reaching 1,302,859 tons in 2018. Harvesting date fruits at an appropriate time according to a specific maturity stage or level is a critical decision that significantly affects profit. In the present study, we proposed an intelligent harvesting decision system (IHDS) based on date fruit maturity level. The proposed decision system used computer vision and deep learning (DL) techniques to detect seven different maturity stages/levels of date fruit (Immature stage 1, Immature stage 2, Pre-Khalal, Khalal, Khalal with Rutab, Pre-Tamar, and Tamar). In the IHDS, we developed six different DL systems, and each one produced different accuracy levels in terms of the seven aforementioned maturity stages. The IHDS used datasets that have been collected by the Center of Smart Robotics Research. The maximum performance metrics of the proposed IHDS were 99.4%, 99.4%, 99.7%, and 99.7% for accuracy, F1 score, sensitivity (recall), and precision, respectively.","Vegetation,Agriculture,Robots,Task analysis,Image color analysis,Machine learning,Maturity detection,deep learning,date fruit classification,neural networks",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,,IEEE ACCESS,https://ieeexplore.ieee.org/ielx7/6287639/8948470/09195414.pdf,
84,An Efficient Retrieval System for Biomedical Images Based on Radial Associated Laguerre Moments,8,,175669-175687,"Hassan Gaber,Hosny Khalid M.,Farouk Rushdy Mohamed,Alzohairy Ahmed Mansour","Hassan G,Hosny KM,Farouk RM,Alzohairy AM",Hassan G,10.1109/ACCESS.2020.3026452,Egyptian Knowledge Bank (EKB),"The ability of any retrieval system to extract features by using its feature descriptor is the primary criterion to measure its efficiency. In this paper a novel technique for feature extraction of biomedical images is presented. The mooted system uses the Radial Associated Laguerre Moments (RALMs) as a feature descriptor to obtain features from two types of medical images: computer tomography (CT) and magnetic resonance images (MRI). RALMs represent one sort of discrete orthogonal moments. RALMs extract the features from images using orthogonal moments to retrieve images from a database. Our approach is extensively assessed with noise-free and noisy images from three different benchmark databases: Emphysema-CT, NEMA CT, and NEMA MRI. The first two databases are used for CT image retrieval, while the third is for MR image retrieval. The proposed approach was tested against the state-of-the art local feature descriptors: Local Binary Pattern (LBP), and local diagonal extrema pattern (LDEP). It was also evaluated against orthogonal Fourier-Mellin moments (OFMMs) as a global descriptor. The comparison shows a significant improvement in favor of the proposed approach in terms of three different performance metrics: ARP, ARR, and F_score. The proposed approach was also compared against the convolutional neural network (CNN) as a deep learning based method over the NEMA-MRI dataset. The RALMs based approach showed a significant improvement when compared against two state-of-the-art medical image retrieval approaches: Histogram of Compressed Scattering Coefficients (HCSCs) and a local bit-plane decoding-based AlexNet descriptor (LBpDAD), the study has done over the TCIA-CT dataset. The proposed approach was also tested with big well-known dataset from the international skin imaging collaboration (ISIC) 2018.","Computed tomography,Magnetic resonance imaging,Feature extraction,Image retrieval,Noise measurement,Medical diagnostic imaging,Image retrieval systems,Laguerre moments,medical imaging",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,"FEATURE,DESCRIPTOR,EXTREMA,PATTERN,TEXTURE,MRI,CLASSIFICATION,COMPUTATION,EXTRACTION,BENEFITS",IEEE ACCESS,https://doi.org/10.1109/access.2020.3026452,
85,Utilizing Knowledge Distillation in Deep Learning for Classification of Chest X-Ray Abnormalities,8,,160749-160761,"Ho Thi Kieu Khanh,Gwak Jeonghwan","Ho TKK,Gwak J",Gwak J,10.1109/ACCESS.2020.3020802,Korea National University of Transportation,"Automatic screening and diagnosis of lung abnormalities from chest X-ray images has been recently drawing attention from the computer vision and medical imaging communities. Previous studies of deep neural networks have predominantly demonstrated the effectiveness of lung disease binary classification procedures. However, large numbers of medical images-which can be labeled with a variety of existing or suspected pathologies-are required to be interpreted and reported upon daily by an individual radiologist; this poses a challenge in maintaining a consistently high diagnosis accuracy. In this paper, we present a competitive study of knowledge distillation (KD) in deep learning for classification of abnormalities in chest X-ray images. This method aims to either distill knowledge from cumbersome teacher models into lightweight student models or to self-train these student models, to generate weakly supervised multi-label lung disease classifications. Our approach was based on multi-task deep learning architectures that, in addition to multi-class classification, supported the visualizations utilized in saliency maps of the pathological regions where an abnormality was located. A self-training KD framework, in which the model learned from itself, was shown to outperform both the well-established baseline training procedure and the normal KD, achieving the AUC improvements of up to 6.39% and 3.89%, respectively. Through application to the publicly available ChestX-ray14 dataset, we demonstrated that our approach efficiently overcame the interdependency of 14 weakly annotated thorax diseases and facilitated the state-of-the-art classification compared with the current deep learning baselines.","Diseases,X-ray imaging,Solid modeling,Machine learning,Lung,Biomedical imaging,Training,ChestX-ray14,deep neural networks,knowledge distillation (KD),multi-class classification,saliency maps,self-training KD",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,,"NODULE,LUNG",IEEE ACCESS,https://ieeexplore.ieee.org/ielx7/6287639/8948470/09183910.pdf,
86,Unified Diagnosis Framework for Automated Nuclear Cataract Grading Based on Smartphone Slit-Lamp Images,8,,174169-174178,"Hu Shenming,Wang Xiaoting,Wu Hong,Luan Xinze,Qi Peng,Lin Yi,He Xiangdong,He Wei","Hu SM,Wang XT,Wu H,Luan XZ,Qi P,Lin Y,He XD,He W",Wang XT,10.1109/ACCESS.2020.3025346,"He Univ, Shenyang 110000, Peoples R China.","Cataract constitutes half of the blindness cases worldwide; hence, detecting and treating cataracts in a timely manner are effective strategies for blindness prevention. Recently, methods of detecting cataracts through deep learning are flourishing; however, the task of improving the grading mechanism is still the priority in the research field. This study evaluates the classification capability of the automated nuclear cataract detection algorithm using ocular images captured by smartphone-based slit-lamp. The task of the algorithm is to automatically detect cataract severity in terms of the photometric appearance of the nuclear region of the crystalline lens of the eyes. The nuclear region of the ocular lens was localized by YOLOv3. Subsequently, the combination of a deep learning network, ShuffleNet, and a support vector machine (SVM) classifier was used to grade cataract severity, evaluating the gray conjugate features of the nuclear region. Using the trained algorithm, 819 anterior ocular images captured by smartphone-based slit-lamp were utilized to evaluate the algorithm's performance. The accuracy was 93.5% with Kappa of 95.4% and F1 of 92.3%. The AUC was 0.9198. The proposed validation method could evaluate a cataract severity in 29 ms and the entire classification process in less than 1s. This study can improve the accuracy of the examination, reduce misdiagnosis rate and the difficulty of the doctor's examination. The addition of scoring system can improve the quality of pictures obtained by non-ophthalmologists. The method is especially suitable for cataract screening in the underdeveloped areas or areas which are in shortage of ophthalmic resources. It can also improve the accessibility of ophthalmic medical treatment.","Cataracts,Classification algorithms,Feature extraction,Lenses,Machine learning,Support vector machines,Images captured by smartphone-based slit-lamp,automated cataract detection,grade cataract,deep learning",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,"IMPAIRMENT,FEATURES",IEEE ACCESS,https://ieeexplore.ieee.org/ielx7/6287639/8948470/09201392.pdf,
87,A Data Augmentation Scheme for Geometric Deep Learning in Personalized Brain-Computer Interfaces,8,,162218-162229,"Kalaganis Fotis P.,Laskaris Nikolaos A.,Chatzilari Elisavet,Nikolopoulos Spiros,Kompatsiaris Ioannis","Kalaganis FP,Laskaris NA,Chatzilari E,Nikolopoulos S,Kompatsiaris I",Kalaganis FP,10.1109/ACCESS.2020.3021580,Aristotle University of Thessaloniki,"Electroencephalography signals inherently deviate from the notion of regular spatial sampling, as they reflect the coordinated action from multiple distributed overlapping cortical networks. Hence, the observed brain dynamics are influenced both by the topology of the sensor array and the underlying functional connectivity. Neural engineers are currently exploiting the advances in the domain of graph signal processing in an attempt to create robust and reliable brain decoding systems. In this direction, Geometric Deep Learning is a highly promising concept for combining the benefits of graph signal processing and deep learning towards revolutionising Brain-Computer Interfaces (BCIs). However, its exploitation has been hindered by its data-demanding character. As a remedy, we propose here a novel data augmentation approach that combines the multiplex network modelling of multichannel signal with a graph variant of the classical Empirical Mode Decomposition (EMD), and which proves to be a strong asset when combined with Graph Convolutional Neural Networks (GCNNs). As our graph-EMD algorithm makes no assumptions with respect to linearity and stationarity, it appears as an appealing solution towards analysing brain signals without artificially imposing regularities in either temporal or spatial domain. Our experimental results indicate that the proposed scheme for data augmentation leads to substantial improvement when it is combined with GCNNs. Using recordings from two distinct BCI applications and comparing against a state-of-the-art augmentation method, we illustrate the benefits from its use. By making it available to BCI community, we hope to further foster the application of geometric deep learning in the field.","Electroencephalography,Machine learning,Signal processing,Brain modeling,Laplace equations,Brain-computer interfaces,Decoding,Brain-computer interfaces,BCI,data augmentation,EEG,geometric deep learning,graph CNN,GCNN,graph signal processing,neural engineering",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,"EMPIRICAL,MODE,DECOMPOSITION,TIME-SERIES,METHODOLOGY,ATTENTION,SIGNALS",IEEE ACCESS,https://ieeexplore.ieee.org/ielx7/6287639/8948470/09186097.pdf,
88,A High-Precision Random Forest-Based Maximum Lyapunov Exponent Prediction Model for Spherical Porous Gas Bearing Systems,8,,168079-168086,"Kuo Ping-Huan,Lee Rong-Mao,Wang Cheng-Chi","Kuo PH,Lee RM,Wang CC",Wang CC,10.1109/ACCESS.2020.3022854,National Chin-Yi University of Technology,"Spherical porous air bearing (SPAB) systems have been extensively used in various mechanical engineering applications. SPABs are promising materials in high-rotational speed, high-precision, and high-stiffness instruments. In SPAB systems, a rotor is supported by gas bearings, which provides higher rotational speed and lower heat generation environment than oil bearings do. Furthermore, SPAB does not cause deformation. Although, the supporting force of gas bearings is less, their stability is better than that of oil films. However, because the pressure distribution in the gas films is nonlinear, they are prone to failure at specific critical speeds, rotor imbalances, or inappropriate operations, which results in nonperiodic or chaotic motion and causes structural fatigue to the system. To understand and control the operating conditions of the SPAB systems during the nonperiodic motion, first, the governing equations of the SPAB system were solved to obtain the dynamic behavior of the rotor center. Then, the performance of the SPAB system were examined under different operating conditions by generating the maximum Lyapunov exponents (MLEs). However, the calculation process of MLE is extremely time consuming and complex. To solve this problem efficiently, a high-precision machine learning (ML)-based MLE prediction model was proposed in this study. The results show that the training process can be finished within few minutes, and the prediction process is able to be completed within few seconds. Meanwhile, the results demonstrate the merit of using the machine learning method for solving the MLE prediction problem and shorten the calculation time significantly. The proposed prediction model achieves excellent prediction outcome and it is more efficient and precise than traditional iteration scheme for the calculation of MLE. The feasibility of the proposed model is validated and the results also are the major contribution of this study.","Rotors,Mathematical model,Predictive models,Bifurcation,Radio frequency,Training,Support vector machines,Machine learning,maximum Lyapunov exponents (MLEs),prediction model,spherical porous air bearing (SPAB)",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,"SUPPORT,VECTOR,MACHINE",IEEE ACCESS,https://ieeexplore.ieee.org/ielx7/6287639/8948470/09189824.pdf,
89,A Model-Based Unsupervised Deep Learning Method for Low-Dose CT Reconstruction,8,,159260-159273,"Liang Kaichao);,Zhang Li);,Yang Hongkai);,Chen Zhiqiang);,Xing Yuxiang","Liang KC,Zhang L,Yang HK,Chen ZQ,Xing YX",Xing YX,10.1109/ACCESS.2020.3020406,Tsinghua University,"Low-dose CT (LDCT) is of great significance due to the concern about the potential radiation risk. With the fast development of deep learning, neural networks have become powerful tools in LDCT enhancement. Current deep neural networks for LDCT reconstruction are often trained with paired LDCT dataset and normal-dose CT (NDCT) dataset. However, high quality NDCT dataset paired with LDCT dataset is expensive to acquire or even not available sometimes in reality. In this work, we proposed an unsupervised model-based deep learning (MBDL) for LDCT reconstruction. The network is trained based on group-wise maximum a posterior (G-MAP) loss function with LDCT dataset only. The MBDL is a general framework. It also allows us to combine with supervised training if a small number of paired NDCT dataset accessible to help optimizing the network parameters, i.e. works in a semi-supervised mode. During inference, LDCT images are reconstructed end-to-end by the trained network. We verified the proposed method with simulated projection data from clinical CT images. The proposed method restrained noise well while restoring anatomical structures and it achieved better results than model-based iterative reconstruction (MBIR) with significantly less computational cost. The performances of MBDL were further enhanced by integrating a small paired NDCT dataset for semi-supervised training. The results suggested that MBDL is an efficient and flexible method for LDCT deep learning based reconstruction in the situations lacking of enough high quality NDCT data.","Image reconstruction,Machine learning,Computed tomography,Training,Estimation,Filtration,Filtering theory,Low-dose CT,deep learning,unsupervised learning,semi-supervised learning",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,"STATISTICAL,IMAGE-RECONSTRUCTION,NETWORK,RESTORATION,REDUCTION,DESCENT",IEEE ACCESS,https://ieeexplore.ieee.org/ielx7/6287639/8948470/09180342.pdf,
90,Automatic Sleep Arousals Detection From Polysomnography Using Multi-Convolution Neural Network and Random Forest,8,,176343-176350,"Liu Yitian,Liu Hongxing,Yang Bufang","Liu YT,Liu HX,Yang BF",Liu HX,10.1109/ACCESS.2020.3026814,Nanjing University,"Sleep arousals is a type of sleep disorder, which refers to the phenomenon of waking up and falling asleep again. Monitoring the number and duration of sleep arousals is a crucial aspect of sleep quality assessment. The detection of sleep arousals caused by apnea is relatively easy, and existing methods have been able to give high quality results. However, sleep arousals caused by non-apnea remains an ongoing challenge, and this is also the subject of PhysioNet Computing in Cardiology Challenge 2018. We proposed a non-apnea sleep arousals automatic detection algorithm based on polysomnography (PSG) data. We took 8 most representative signals selected from the 13 channels of PSG signals as input, conducted preliminary classification through multiple convolutional neural networks, and then sent the initial results to the random forest module for ensemble voting, and obtained the final judgment. We carried out some experiments using the CinC 2018 database. We grouped the original dataset reasonably, and based on each group of data, we trained a corresponding CNN, ensuring the balance of positive and negative samples during the training. Our 4-fold cross validation results for the AUROC and AUPRC were 0.953 and 0.552, which were better than the results of the team which ranked first in the CinC 2018.","Sleep apnea,Forestry,Electroencephalography,Feature extraction,Convolution,Neural networks,Sleep arousals,non-apnea,polysomnography,multiple convolutional neural networks,random forest",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,,IEEE ACCESS,https://doi.org/10.1109/access.2020.3026814,
91,A Brief Survey on Breast Cancer Diagnostic With Deep Learning Schemes Using Multi-Image Modalities,8,,165779-165809,"Mahmood Tariq,Li Jianqiang,Pei Yan,Akhtar Faheem,Imran Azhar,Rehman Khalil Ur","Mahmood T,Li JQ,Pei Y,Akhtar F,Imran A,Rehman KU",Pei Y,10.1109/ACCESS.2020.3021343,University of Aizu,"Patients with breast cancer are prone to serious health-related complications with higher mortality. The primary reason might be a misinterpretation of radiologists in recognizing suspicious lesions due to technical issues in imaging qualities and heterogeneous breast densities which increases the false-(positive and negative) ratio. Early intervention is significant in establishing an up-to-date prognosis process which can successfully mitigate complications of disease with higher recovery. The manual screening of breast abnormalities through traditional machine learning schemes misinterpret the inconsistent feature-extraction process which poses a problem, i.e., patients being called-back for biopsies to eliminates the suspicions. However, several deep learning-based methods have been developed for reliable breast cancer prognosis and classification but very few of them provided a comprehensive overview of lesions segmentation. This research focusses on providing benefits and risks of breast multi-imaging modalities, segmentation schemes, feature extraction, classification of breast abnormalities through state-of-the-art deep learning approaches. This research also explores various well-known databases using ""Breast Cancer"" keyword to present a comprehensive survey on existing diagnostic schemes to open-up new research challenges for radiologists and researchers to intervene as early as possible to develop an efficient and reliable breast cancer prognosis system using prominent deep learning schemes.","Breast cancer,Machine learning,Lesions,Solid modeling,Feature extraction,Breast cancer,computer-aided-diagnosis,deep learning techniques,medical image analysis,lesions classification,segmentation",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,"COMPUTER-AIDED,DIAGNOSIS,AUTOMATIC,MASS,DETECTION,TEXTURE,ANALYSIS,CLASSIFICATION,MAMMOGRAMS,SYSTEM,SEGMENTATION,FEATURES,DENSITY,NETWORK",IEEE ACCESS,https://ieeexplore.ieee.org/ielx7/6287639/8948470/09184879.pdf,
92,A Multi-Critic Reinforcement Learning Method: An Application to Multi-Tank Water Systems,8,,173227-173238,"Martinez-Piazuelo Juan,Ochoa Daniel E.,Quijano Nicanor,Giraldo Luis Felipe","Martinez-Piazuelo J,Ochoa DE,Quijano N,Giraldo LF",Martinez-Piazuelo J,10.1109/ACCESS.2020.3025194,University of the Andes Colombia,"This paper investigates the combination of reinforcement learning and neural networks applied to the data-driven control of dynamical systems. In particular, we propose a multi-critic actor-critic architecture that eases the value function learning task by distributing it into multiple neural networks. We also propose a filtered multi-critic approach that offers further performance improvements as it eases the training process of the control policy. All the studied methods are evaluated with several numerical experiments on multi-tank water systems with nonlinear coupled dynamics, where control is known to be a challenging task. The simulation results show that the proposed multi-critic scheme is able to outperform the standard actor-critic approach in terms of speed and sensitivity of the learning process. Moreover, the results show that the filtered multi-critic strategy outperforms the unfiltered one under these same terms. This document highlights the benefits of the multi-critic methodology on a state of the art reinforcement learning algorithm, the deep deterministic policy gradient, and demonstrates its application to multi-tank water systems relevant for industrial process control.","Neural networks,Learning (artificial intelligence),Task analysis,Training,Control systems,Process control,Sensitivity,Data-driven control,approximate dynamic programming,reinforcement learning,actor-critic methods,deep deterministic policy gradient,water-tank systems",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,,IEEE ACCESS,https://doi.org/10.1109/access.2020.3025194,
93,Anxiety Recognition of College Students Using a Takagi-Sugeno-Kang Fuzzy System Modeling Method and Deep Features,8,,159897-159905,"Meng Xiangmin,Zhang Jie","Meng XM,Zhang J",Zhang J,10.1109/ACCESS.2020.3021092,Nanjing University of Aeronautics & Astronautics,"College students are the most active, most sensitive, and most prone group with respect to various psychological problems in contemporary society. In recent years, with the intensification of social competition, including various pressures such as studies, examinations, economic loss, emotional loss, and employment, the incidence of anxiety, depression and suicide rates has increased. To effectively pay attention to the psychological development of college students and to strengthen mental health education, this research proposes a method to automatically identify the anxiety of college students using a Takagi-Sugeno-Kang (TSK) fuzzy system and deep features. First, preprocess the collected EEG of college students. Secondly, use convolutional neural network (CNN) to extract deep features from the input data. Finally, TSK fuzzy system is used to classify features to obtain the final recognition result. Through experiments on standard data sets and self-made data sets, the experimental results verify the superiority of the anxiety identification method used in this study. The experimental results further demonstrate that the depth features have richer information than traditional features. The noise immunity of TSK fuzzy system makes it show good classification performance and generalization. The recognition results can quickly locate students with anxiety disorders and narrow the scope of investigation for students with psychological problems. The automatic recognition of college students' anxiety can improve the efficiency of schools and teachers in investigating students' psychological problems. This research has very good practical application value.","Emotion recognition,Physiology,Psychology,Feature extraction,Fuzzy systems,Classification algorithms,Skin,College students,anxiety recognition,TSK fuzzy system,deep features",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,"EMOTION,RECOGNITION,GENETIC,EPIDEMIOLOGY,CLASSIFICATION,DEPRESSION,IDENTIFICATION,SEGMENTATION,METAANALYSIS,ASSOCIATION,NETWORKS",IEEE ACCESS,https://ieeexplore.ieee.org/ielx7/6287639/8948470/09184941.pdf,
94,Self-Supervised Learning Based on Spatial Awareness for Medical Image Analysis,8,,162973-162981,"Nguyen Xuan-Bac,Lee Guee Sang,Kim Soo Hyung,Yang Hyung Jeong","Nguyen XB,Lee GS,Kim SH,Yang HJ",Lee GS,10.1109/ACCESS.2020.3021469,Chonnam National University,"Medical image analysis is one of the research fields that had huge benefits from deep learning in recent years. To earn a good performance, the learning model requires large scale data with full annotation. However, it is a big burden to collect a sufficient number of labeled data for the training. Since there are more unlabeled data than labeled ones in most of medical applications, self-supervised learning has been utilized to improve the performance. However, most of current methods for self-supervised learning try to understand only semantic features of the data, but have not fully utilized properties inherent in medical images. Specifically, in CT or MR images, the spatial or structural information contained in the dataset has not been fully considered. In this paper, we propose a novel method for self-supervised learning in medical image analysis that can exploit both semantic and spatial features at the same time. The proposed method is experimented in the problems of organ segmentation, intracranial hemorrhage detection and the results show the effectiveness of the method.","Task analysis,Biomedical imaging,Semantics,Training,Image segmentation,Computed tomography,Feature extraction,Self-supervised learning,spatial awareness,medical image analysis,deep learning,convolutional neural network",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,,IEEE ACCESS,https://ieeexplore.ieee.org/ielx7/6287639/8948470/09186121.pdf,
95,Survival Marker miRNA-Mediated Subpathways of Breast Invasive Carcinoma Derived From Activity Profile,8,,160947-160956,"Ning Ziyu,Yu Shuang,Zhao Yanqiao,Wang Yang,Fan Qi,Song Tianming,Yu Xiaoyang","Ning ZY,Yu S,Zhao YQ,Wang Y,Fan Q,Song TM,Yu XY",Yu XY,10.1109/ACCESS.2020.3018148,Harbin University of Science & Technology,"We not only collected 1102 samples form 3 different breast invasive carcinoma (BRCA) datasets, which included a TCGA dataset and 2 GEO datasets, but also collected 23 other cancer datasets, one of which included more than 100 samples. Using these datasets, we topologically inferred miRNA-mediated subpathway activity profiles, which integrated gene expression profiles, prior gene interaction information and target relations between miRNAs and genes, and topological information. Then we constructed the Global Directed Pathway Network (GDPN) with genes as nodes, and from 3 BRCA datasets and other 23 cancer datasets identified a set of miRNA-mediated subpathways that are survival-related risk markers. The results showed large activity values correlated with poor prognosis, such as hsa-miR-107 and hsa-miR-142-3p of BRCA datasets. We assessed the stability and robustness of the miRNA-mediated subpathway survival markers with 2 GEO datasets and 23 external independent datasets. The results showed that the proposed method can significantly reduce noise from sequencing errors and samples heterogeneity by integrating pathway topological information, and can break down the boundary of pathways and provide a new measure for detecting survival-related markers. The top miRNA-mediated subpathways are more reliable in stratifying high risk group and low risk group and selecting therapeutic strategies.","miRNA-mediated subpathway,survival marker,prognosis,topological information,machine learning",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,"COLORECTAL-CANCER,PATHWAY,EXPRESSION,MICRORNAS,PROGNOSIS,TARGETS,MIR-125A-5P,PROGRESSION,METASTASIS,PREDICTION",IEEE ACCESS,https://ieeexplore.ieee.org/ielx7/6287639/8948470/09172063.pdf,
96,Stroke Screening Feature Selection for Arm Weakness Using a Mobile Application,8,,170898-170914,"Phienphanich Phongphan,Tankongchamruskul Nattakit,Akarathanawat Wasan,Chutinet Aurauma,Nimnual Rossukon,Tantibundhit Charturong,Suwanwela Nijasri Charnnarong","Phienphanich P,Tankongchamruskul N,Akarathanawat W,Chutinet A,Nimnual R,Tantibundhit C,Suwanwela NC",Tantibundhit C,10.1109/ACCESS.2020.3024292,Thammasat University,"This work studies the features of a proposed automated stroke self-screening application that utilizes the gyroscope and accelerometer devices in smartphones to determine the possible onset of a stroke by assessing arm muscle weakness. The application requires users to perform two arm movements to evaluate arm weakness and pronation: Curl-up and Raise-up. For the purpose of the study, 68 subjects, consisting of 36 stroke patients with symptoms of arm weakness and 32 healthy subjects, consented to participate. A total of 78 handcrafted features were proposed, 26 of which were extracted from Curl-up and Raise-up for each arm. Then, the differences between corresponding features for each arm were calculated. These features were then tested on 63 combinations of three classical feature selection methods, three feature sets (i.e., Curl-up-only features, Raise-up-only features, and both-exercises combined features) and seven well-known classification methods. The results from ten runs of 10-fold cross-validation showed that Curl-up-only features achieved an average sensitivity of 83.3%, significantly higher than those of the Raise-up-only features or both-exercises features. From all possible combinations, the random forest classification based on information gain feature selection from Curl-up-only features achieved the most efficient results for arm-weakness-stroke screening. It achieved an average sensitivity of 94.8%, an average specificity of 75.2%, an average accuracy of 84.1%, and an average area under the receiver operating characteristic curve of 85.0%. Our work proposes a novel accessible method to screen symptoms of arm weakness that may indicate the onset of a stroke using a single mobile device. In the future, we can combine this method with other methods of evaluating facial drooping and slurred speech to create a complete Face, Arm, Speech, Time (FAST) assessment application.","Feature extraction,Accelerometers,Gyroscopes,Mobile handsets,Muscles,Tools,Biomedical monitoring,Arm weakness,handcrafted features,stroke screening,FAST,curl-up,raise-up",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,"FUSION,SCALE",IEEE ACCESS,https://ieeexplore.ieee.org/ielx7/6287639/8948470/09197717.pdf,
97,Sensor Location Analysis and Minimal Deployment for Fall Detection System,8,,166678-166691,"Ponce Hiram,Martinez-Villasenor Lourdes,Nunez-Martinez Jose","Ponce H,Martinez-Villasenor L,Nunez-Martinez J",Ponce H,10.1109/ACCESS.2020.3022971,Universidad Panamericana - Ciudad de Mexico,"Human falls are considered as an important health problem worldwide. Fall detection systems can alert when a fall occurs reducing the time in which a person obtains medical attention. In this regard, there are different approaches to design fall detection systems, such as wearable sensors, ambient sensors, vision devices, and more recently multimodal approaches. However, these systems depend on the types of devices selected for data acquisition, the location in which these devices are placed, and how fall detection is done. Previously, we have created a multimodal dataset namely UP-Fall Detection and we developed a fall detection system. But the latter cannot be applied on realistic conditions due to a lack of proper selection of minimal sensors. In this work, we propose a methodological analysis to determine the minimal number of sensors required for developing an accurate fall detection system, using the UP-Fall Detection dataset. Specifically, we analyze five wearable sensors and two camera viewpoints separately. After that, we combine them in a feature level to evaluate and select the most suitable single or combined sources of information. From this analysis we found that a wearable sensor at the waist and a lateral viewpoint from a camera exhibits 98.72% of accuracy (intra-subject). At the end, we present a case study on the usage of the analysis results to deploy a minimal-sensor based fall detection system which finally reports 87.56% of accuracy (inter-subject).","Cameras,Wearable sensors,Machine learning,Feature extraction,Accelerometers,Multimodal sensors,Activity recognition,Ambient assisted living,fall detection,health monitoring system,human activity recognition,machine learning,sensor fusion",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,,IEEE ACCESS,https://ieeexplore.ieee.org/ielx7/6287639/8948470/09194014.pdf,
98,Automatic Segmentation Using a Hybrid Dense Network Integrated With an 3D-Atrous Spatial Pyramid Pooling Module for Computed Tomography (CT) Imaging,8,,169794-169803,"Qayyum Abdul,Ahmad Iftikhar,Mumtaz Wajid,Alassafi Madini O.,Alghamdi Rayed,Mazher Moona","Qayyum A,Ahmad I,Mumtaz W,Alassafi MO,Alghamdi R,Mazher M",Qayyum A,10.1109/ACCESS.2020.3024277,Universite de Bourgogne,"Computed tomography (CT) with a contrast-enhanced imaging technique is extensively proposed for the assessment and segmentation of multiple organs, especially organs at risk. It is an important factor involved in the decision making in clinical applications. Automatic segmentation and extraction of abdominal organs, such as thoracic organs at risk, from CT images are challenging tasks due to the low contrast of pixel values surrounding other organs. Various deep learning models based on 2D and 3D convolutional neural networks have been proposed for the segmentation of medical images because of their automatic feature extraction capability based on large labeled datasets. In this paper, we proposed a 3D-atrous spatial pyramid pooling (ASPP) module integrated with a proposed 3D DensNet encoder-decoder network for volumetric segmentation to segment abdominal organs from CT. The proposed network used a 3D-ASPP block to capture spatial information in multiscale input feature maps from the decoder side. We also proposed a 3D-ASPP block with a 3D DensNet network for automatically processing 3D medical volumetric images. The proposed hybrid network was named 3D-ASPPDN for volumetric segmentation via CT medical imaging. We tested our proposed approach on a public dataset, Thoracic Organs at Risk (SegTHOR) 2019. The proposed solution showed excellent performance in comparison with other existing state-of-the-art DL methods. The proposed method achieved Dice scores 97.89% on the SegTHOR dataset. Results presented that 3D-ASPPDN exhibited enhanced performance in volumetric biomedical segmentation. The proposed model could be used for volumetric segmentation in clinical applications to diagnose problems in multi class organs.","Image segmentation,Three-dimensional displays,Computed tomography,Solid modeling,Computational modeling,Biological systems,Biomedical imaging,3D volumetric segmentation,3D deep learning models,3D-atrous spatial pyramid pooling (ASPP),SegTHOR",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,"CONVOLUTIONAL,NEURAL-NETWORKS,MRI,CNN",IEEE ACCESS,https://ieeexplore.ieee.org/ielx7/6287639/8948470/09197680.pdf,
99,DL-CRC: Deep Learning-Based Chest Radiograph Classification for COVID-19 Detection: A Novel Approach,8,,171575-171589,"Sakib Sadman,Tazrin Tahrat,Fouda Mostafa M.,Fadlullah Zubair Md.,Guizani Mohsen","Sakib S,Tazrin T,Fouda MM,Fadlullah ZM,Guizani M",Sakib S,10.1109/ACCESS.2020.3025010,Lakehead University,"With the exponentially growing COVID-19 (coronavirus disease 2019) pandemic, clinicians continue to seek accurate and rapid diagnosis methods in addition to virus and antibody testing modalities. Because radiographs such as X-rays and computed tomography (CT) scans are cost-effective and widely available at public health facilities, hospital emergency rooms (ERs), and even at rural clinics, they could be used for rapid detection of possible COVID-19-induced lung infections. Therefore, toward automating the COVID-19 detection, in this paper, we propose a viable and efficient deep learning-based chest radiograph classification (DL-CRC) framework to distinguish the COVID-19 cases with high accuracy from other abnormal (e.g., pneumonia) and normal cases. A unique dataset is prepared from four publicly available sources containing the posteroanterior (PA) chest view of X-ray data for COVID-19, pneumonia, and normal cases. Our proposed DL-CRC framework leverages a data augmentation of radiograph images (DARI) algorithm for the COVID-19 data by adaptively employing the generative adversarial network (GAN) and generic data augmentation methods to generate synthetic COVID-19 infected chest X-ray images to train a robust model. The training data consisting of actual and synthetic chest X-ray images are fed into our customized convolutional neural network (CNN) model in DL-CRC, which achieves COVID-19 detection accuracy of 93.94% compared to 54.55% for the scenario without data augmentation (i.e., when only a few actual COVID-19 chest X-ray image samples are available in the original dataset). Furthermore, we justify our customized CNN model by extensively comparing it with widely adopted CNN architectures in the literature, namely ResNet, Inception-ResNet v2, and DenseNet that represent depth-based, multi-path-based, and hybrid CNN paradigms. The encouragingly high classification accuracy of our proposal implies that it can efficiently automate COVID-19 detection from radiograph images to provide a fast and reliable evidence of COVID-19 infection in the lung that can complement existing COVID-19 diagnostics modalities.","Radiography,Lung,X-rays,Diseases,Computed tomography,Data models,Hospitals,COVID-19,convolutional neural network (CNN),deep learning,generative adversarial network (GAN),pneumonia",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,,IEEE ACCESS,https://doi.org/10.1109/access.2020.3025010,
100,Recognition of Different Types of Leukocytes Using YOLOv2 and Optimized Bag-of-Features,8,,167448-167459,"Sharif Muhammad,Amin Javaria,Siddiqa Ayesha,Khan Habib Ullah,Arshad Malik Muhammad Sheraz,Anjum Muhammad Almas,Kadry Seifedine","Sharif M,Amin J,Siddiqa A,Khan HU,Malik MSA,Anjum MA,Kadry S",Sharif M,10.1109/ACCESS.2020.3021660,COMSATS University Islamabad (CUI),"White blood cells (WBCs) protect human body against different types of infections including fungal, parasitic, viral, and bacterial. The detection of abnormal regions in WBCs is a difficult task. Therefore a method is proposed for the localization of WBCs based on YOLOv2-Nucleus-Cytoplasm, which contains darkNet-19 as a basenetwork of the YOLOv2 model. In this model features are extracted from LeakyReLU-18 of darkNet-19 and supplied as an input to the YOLOv2 model. The YOLOv2-Nucleus-Cytoplasm model localizes and classifies the WBCs with maximum score labels. It also localize the WBCs into the blast and non-blast cells. After localization, the bag-of-features are extracted and optimized by using particle swarm optimization(PSO). The improved feature vector is fed to classifiers i.e., optimized naive Bayes (O-NB) & optimized discriminant analysis (O-DA) for WBCs classification. The experiments are performed on LISC, ALL-IDB1, and ALL-IDB2 datasets.","Feature extraction,Cells (biology),Computational modeling,White blood cells,Image segmentation,Machine learning,Cytoplasm,leukocytes,darkNet-19,recognition,white blood cells,YOLOv2",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering,Telecommunications",,3.671,"WHITE,BLOOD-CELLS,SEGMENTATION,CLASSIFICATION,SMEAR,IDENTIFICATION,COUNT",IEEE ACCESS,https://ieeexplore.ieee.org/ielx7/6287639/8948470/09186086.pdf,
