,title,Volume,Issue,Pages,whole__author_name,simply_author_name,reprint author,DOI,reprint address,Abstract,Keywords,Document Type,Publisher,Research Domain,Published Date,impact_factor,Keywords_plus,joural,pdf_link,Download_SuccessOrDefeat
1,SPECTRAL GRAPH TRANSFORMER NETWORKS FOR BRAIN SURFACE PARCELLATION,,,372-376,"He Ran,Gopinath Karthik,Desrosiers Christian,Lombaert Herve","He R,Gopinath K,Desrosiers C,Lombaert H",He R,,University of Quebec,"The analysis of the brain surface modeled as a graph mesh is a challenging task. Conventional deep learning approaches often rely on data lying in the Euclidean space. As an extension to irregular graphs, convolution operations are defined in the Fourier or spectral domain. This spectral domain is obtained by decomposing the graph Laplacian, which captures relevant shape information. However, the spectral decomposition across different brain graphs causes inconsistencies between the eigenvectors of individual spectral domains, causing the graph learning algorithm to fail. Current spectral graph convolution methods handle this variance by separately aligning the eigenvectors to a reference brain in a slow iterative step. This paper presents a novel approach for learning the transformation matrix required for aligning brain meshes using a direct data-driven approach. Our alignment and graph processing method provides a fast analysis of brain surfaces. The novel Spectral Graph Transformer (SGT) network proposed in this paper uses very few randomly sub-sampled nodes in the spectral domain to learn the alignment matrix for multiple brain surfaces. We validate the use of this SGT network along with a graph convolution network to perform cortical parcellation. Our method on 101 manually-labeled brain surfaces shows improved parcellation performance over a no-alignment strategy, gaining a significant speed (1400 fold) over traditional iterative alignment approaches.","Spectral transformer network,Cortical parcellation,Graph Convolution Network",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,http://arxiv.org/pdf/1911.10118,
2,STIMULUS SPEECH DECODING FROM HUMAN CORTEX WITH GENERATIVE ADVERSARIAL NETWORK TRANSFER LEARNING,,,390-394,"Wang Ran,Chen Xupeng,Khalilian-Gourtani Amirhossein,Chen Zhaoxi,Yu Leyao,Flinker Adeen,Wang Yao","Wang R,Chen XP,Khalilian-Gourtani A,Chen ZX,Yu LY,Flinker A,Wang Y",Wang R,,New York University,"Decoding auditory stimulus from neural activity can enable neuroprosthetics and direct communication with the brain. Some recent studies have shown successful speech decoding from intracranial recording using deep learning models. However, scarcity of training data leads to low quality speech reconstruction which prevents a complete brain-computer-interface (BCI) application. In this work, we propose a transfer learning approach with a pre-trained GAN to disentangle representation and generation layers for decoding. We first pre-train a generator to produce spectrograms from a representation space using a large corpus of natural speech data. With a small amount of paired data containing the stimulus speech and corresponding ECoG signals, we then transfer it to a bigger network with an encoder attached before, which maps the neural signal to the representation space. To further improve the network generalization ability, we introduce a Gaussian prior distribution regularizer on the latent representation during the transfer phase. With at most 150 training samples for each tested subject, we achieve a state-of-the-art decoding performance. By visualizing the attention mask embedded in the encoder, we observe brain dynamics that are consistent with findings from previous studies investigating dynamics in the superior temporal gyrus (STG), pre-central gyrus (motor) and inferior frontal gyrus (IFG). Our findings demonstrate a high reconstruction accuracy using deep learning networks together with the potential to elucidate interactions across different brain regions during a cognitive task.","speech decoding,generative adversarial networks (GAN),transfer learning,electrocorticographic (ECoG),superior temporal gyrus (STG)",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
3,SIAMESE VERIFICATION FRAMEWORK FOR AUTISM IDENTIFICATION DURING INFANCY USING CORTICAL PATH SIGNATURE FEATURES,,,395-398,"Zhang Xin,Ding Xinyao,Wu Zhengwang,Xia Jing,Ni Hao,Xu Xiangmin,Liao Lufan,Wang Li,Li Gang","Zhang X,Ding XY,Wu ZW,Xia J,Ni H,Xu XM,Liao LF,Wang L,Li G",Zhang X,,South China University of Technology,"Autism spectrum disorder (ASD) is a complex neurodevelopmental disability, which is lack of biologic diagnostic markers. Therefore, exploring the ASD Identification directly from brain imaging data has been an important topic. In this work, we propose the Siamese verification model to identify ASD using 6 and 12 months cortical features. Rather than directly classifying a testing subject is ASD or not, we determine whether it has the same or different label with the reference subject who has been successfully diagnosed. Then, based on the comparison to all the reference subjects, we can predict the label of the testing subject. The advantage of modeling the classification problem as a verification framework is that it can greatly enlarge the training data size and enable us to train a more accurate and reliable model in an end-to-end manner. In addition, to further improve the classification performance, we introduce the path signature (PS) features, which can capture the dynamic longitudinal information of the brain development for the ASD Identification. Experiments showed that our proposed method reaches the best result, i.e., 87% accuracy, 83% sensitivity and 90% specificity comparing to the state-of-the-art methods.","Autism,Cortical Features,Verification Model,Path Signature",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,"SPECTRUM,DISORDER,NETWORK",,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8375290,
4,SEX DIFFERENCES IN THE BRAIN: DIVERGENT RESULTS FROM TRADITIONAL MACHINE LEARNING AND CONVOLUTIONAL NETWORKS,,,408-411,"Brueggeman Leo,Thomas Taylor R.,Koomar Tanner,Hoskins Brady,Michaelson Jacob J.","Brueggeman L,Thomas TR,Koomar T,Hoskins B,Michaelson JJ",Brueggeman L,,University of Iowa,"Neuroimaging research has begun adopting deep learning to model structural differences in the brain. This is a break from previous approaches that rely largely on anatomical volumetric or thickness-based features. Currently, most studies employ either convolutional deep learning based models or traditional machine learning models that use volumetric features. Because of this split, it is unclear which approach yields better predictive performance, or whether the two approaches will lead to different neuroanatomical conclusions, potentially even when applied to the same dataset. To address these questions, we present the largest single study of sex differences in the brain using 21,390 UK Biobank T1-weighted brain MRIs, which we analyzed through both traditional volumetric and 3D convolutional neural network models. Overall, we find that 3D-CNNs outperformed traditional machine learning models, with sex classification area under the ROC curve of 0.849 and 0.683, respectively. When performing sex classification using only single regions of the brain, we observed better performance from 3D-CNNs in all regions tested, indicating sex differences in the brain likely represent both structural and volumetric changes. In addition, we find little consensus in terms of brain region prioritization between the two approaches. In summary, we find that 3D-CNNs show exceptional sex classification performance, extract additional relevant structural information from brain regions beyond volume, and possibly because of this, prioritize sex differences in neuroanatomical regions differently than volume-based approaches.","convolutional neural networks,brain MRI,sex differences",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
5,MULTI-BRANCH DEFORMABLE CONVOLUTIONAL NEURAL NETWORK WITH LABEL DISTRIBUTION LEARNING FOR FETAL BRAIN AGE PREDICTION,,,424-427,"Liao Lufan,Zhang Xin,Zhao Fenqiang,Lou Jingjiao,Wang Li,Xu Xiangmin,Zhang He,Li Gang","Liao LF,Zhang X,Zhao FQ,Lou JJ,Wang L,Xu XM,Zhang H,Li G",Zhang X,,South China University of Technology,"MRI-based fetal brain age prediction is crucial for fetal brain development analysis and early diagnosis of congenital anomalies. The locations and directions of fetal brain are randomly variable and disturbed by adjacent organs, thus imposing great challenges to the fetal brain age prediction. To address this problem, we propose an effective framework based on a deformable convolutional neural network for fetal brain age prediction. Considering the fact of insufficient data, we introduce label distribution learning (LDL), which is able to deal with the small sample problem. We integrate the LDL information into our end-to-end network. Moreover, to fully utilize the complementary multi-view data of fetal brain MRI stacks, a multi-branch CNN is proposed to aggregate multi-view information. We evaluate our method on a fetal brain MRI dataset with 289 subjects and achieve promising age prediction performance.","Fetal brain age prediction,deformable convolution,label distribution learning,multi-branch CNN",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
6,Leveraging Self-supervised Denoising for Image Segmentation,,,428-432,"Prakash Mangal,Buchholz Tim-Oliver,Lalit Manan,Tomancak Pavel,Jug Florian,Krull Alexander","Prakash M,Buchholz TO,Lalit M,Tomancak P,Jug F,Krull A",Prakash M,,"Ctr Syst Biol Dresden CSBD, Dresden, Germany.","Deep learning (DL) has arguably emerged as the method of choice for the detection and segmentation of biological structures in microscopy images. However, DL typically needs copious amounts of annotated training data that is for biomedical projects typically not available and excessively expensive to generate. Additionally, tasks become harder in the presence of noise, requiring even more high-quality training data. Hence, we propose to use denoising networks to improve the performance of other DL-based image segmentation methods. More specifically, we present ideas on how state-of-the-art self-supervised CARE networks can improve cell/nuclei segmentation in microscopy data. Using two state-of-the-art baseline methods, U-Net and StarDist, we show that our ideas consistently improve the quality of resulting segmentations, especially when only limited training data for noisy micrographs are available.","segmentation,denoising,deep learning",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,http://arxiv.org/pdf/1911.12239,
7,TOWARDS FULLY AUTOMATIC 2D US TO 3D CT/MR REGISTRATION: A NOVEL SEGMENTATION-BASED STRATEGY,,,433-437,"Wei Wei,Rak Marko,Alpers Julian,Hansen Christian","Wei W,Rak M,Alpers J,Hansen C",Wei W,,Otto von Guericke University,"2D-US to 3D-CT/MR registration is a crucial module during minimally invasive ultrasound-guided liver tumor ablations. Many modern registration methods still require manual or semi-automatic slice pose initialization due to insufficient robustness of automatic methods. The state-of-the-art regression networks do not work well for liver 2D US to 3D CT/MR registration because of the tremendous inter-patient variability of the liver anatomy. To address this unsolved problem, we propose a deep learning network pipeline which - instead of a regression - starts with a classification network to recognize the coarse ultrasound transducer pose followed by a segmentation network to detect the target plane of the US image in the CT/MR volume. The rigid registration result is derived using plane regression. In contrast to the state-of-the-art regression networks, we do not estimate registration parameters from multi-modal images directly, but rather focus on segmenting the target slice plane in the volume. The experiments reveal that this novel registration strategy can identify the initial slice phase in a 3D volume more reliably than the standard regression-based techniques. The proposed method was evaluated with 1035 US images from 52 patients. We achieved angle and distance errors of 12.7 +/- 6.2 degrees and 4.9 +/- 3.1 mm, clearly outperforming state-of-the-art regression strategy which results in 37.0 +/- 15.6 degrees angle error and 19.0 +/- 11.6 mm distance error.","US,CT/MR,Registration",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,ULTRASOUND,,,
8,DEEP LEARNING FRAMEWORK FOR EPITHELIUM DENSITY ESTIMATION IN PROSTATE MULTI-PARAMETRIC MAGNETIC RESONANCE IMAGING,,,438-441,"Minh Nguyen Nhat To,Sankineni Sandeep,Xu Sheng,Turkbey Baris,Choyke Peter L.,Pinto Peter A.,Moreno Vanessa,Merino Maria,Wood Bradford J.,Kwak Jin Tae","Minh NNT,Sankineni S,Xu S,Turkbey B,Choyke PL,Pinto PA,Moreno V,Merino M,Wood BJ,Kwak JT",Minh NNT,,Sejong University,"Multi-parametric magnetic resonance imaging (mpMRI) permits non-invasive visualization and localization of clinically important cancers in the prostate. However, it cannot fully describe tumor heterogeneity and microstructures that are crucial for cancer management and treatment. Herein, we develop a deep learning framework that could predict epithelium density of the prostate in mpMRI. A deep convolutional neural network is built to estimate epithelium density per voxel-basis. Equipped with an advanced design of the neural network and loss function, the proposed method obtained a SSIM of 0.744 and a MAE of 6.448% in a cross-validation. It also outperformed the competing network. The results are promising as a potential tool to analyze tissue characteristics of the prostate in mpMRI.","Prostate,Histopathology,MRI,Deep neural network",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,ERROR,,,
9,ATTENTIONANATOMY: A UNIFIED FRAMEWORK FOR WHOLE-BODY ORGANS AT RISK SEGMENTATION USING MULTIPLE PARTIALLY ANNOTATED DATASETS,,,451-455,"Sun Shanlin,Liu Yang,Bai Narisu,Tang Hao,Chen Xuming,Huang Qian,Liu Yong,Xie Xiaohui","Sun SL,Liu Y,Bai N,Tang H,Chen XM,Huang Q,Liu Y,Xie XH",Sun SL,,"DeepVoxel Inc, Irvine, CA 92612 USA.","Organs-at-risk (OAR) delineation in computed tomography (CT) is an important step in Radiation Therapy (RT) planning. Recently, deep learning based methods for OAR delineation have been proposed and applied in clinical practice for separate regions of the human body (head and neck, thorax, and abdomen). However, there are few researches regarding the end-to-end whole-body OARs delineation because the existing datasets are mostly partially or incompletely annotated for such task. In this paper, our proposed end-to-end convolutional neural network model, called AttentionAnatomy, can be jointly trained with three partially annotated datasets, segmenting OARs from whole body. Our main contributions are: 1) an attention module implicitly guided by body region label to modulate the segmentation branch output; 2) a prediction re-calibration operation, exploiting prior information of the input images, to handle partial-annotation(HPA) problem; 3) a new hybrid loss function combining batch Dice loss and spatially balanced focal loss to alleviate the organ size imbalance problem. Experimental results of our proposed framework presented significant improvements in both Sorensen-Dice coefficient (DSC) and 95% Hausdorff distance compared to the baseline model.","whole body,automated anatomy segmentation,partial annotations,deep learning",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,"NECK,CT,IMAGES,HEAD",,http://arxiv.org/pdf/2001.04446,
10,A SPATIALLY CONSTRAINED DEEP CONVOLUTIONAL NEURAL NETWORK FOR NERVE FIBER SEGMENTATION IN CORNEAL CONFOCAL MICROSCOPIC IMAGES USING INACCURATE ANNOTATIONS,,,456-460,"Zhang Ning,Francis Susan,Malik Rayaz A.,Chen Xin","Zhang N,Francis S,Malik RA,Chen X",Zhang N,,University of British Columbia,"Semantic image segmentation is one of the most important tasks in medical image analysis. Most state-of-the-art deep learning methods require a large number of accurately annotated examples for model training. However, accurate annotation is difficult to obtain especially in medical applications. In this paper, we propose a spatially constrained deep convolutional neural network (DCNN) to achieve smooth and robust image segmentation using inaccurately annotated labels for training. In our proposed method, image segmentation is formulated as a graph optimization problem that is solved by a DCNN model learning process. The cost function to be optimized consists of a unary term that is calculated by cross entropy measurement and a pairwise term that is based on enforcing a local label consistency. The proposed method has been evaluated based on corneal confocal microscopic (CCM) images for nerve fiber segmentation, where accurate annotations are extremely difficult to be obtained. Based on both the quantitative result of a synthetic dataset and qualitative assessment of a real dataset, the proposed method has achieved superior performance in producing high quality segmentation results even with inaccurate labels for training.","Medical image segmentation,convolutional neural network,conditional random field",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,http://arxiv.org/pdf/2004.09443,
11,Histopathologic Cancer Detection by Dense-Attention Network with Incorporation of Prior Knowledge,,,466-470,"Liu Mingyuan,Yu Yang,Liao Qingcheng,Zhang Jicong","Liu MY,Yu Y,Liao QC,Zhang JC",Zhang JC,,Beihang University,"To identify the cancerous region in histology Whole-Slide Images (WSI), the common approach is to apply a patch-level classifier. Appending surrounding tissues could improve the accuracy of patch-wise classification and maintain consistency of WSI. However, the rule that surrounding tissues play a supporting role rather than a decisive one is difficult to be learned directly by a Convolutional Neural Networks (CNN). In this paper, we propose Dense-Attention Network (DAN) for cancerous patch classification, where the attention mechanism is further developed to incorporate prior knowledge about the surrounding tissue. Moreover, the effectiveness of Data Augmentation in Inference stage (DAI) is further validated. The proposed method is evaluated on the PatchCamelyon dataset, where images with tumor tissues in the center are labeled positive, and those in the outer regions do not influence the label. Compared with other competitive deep-learning methods the proposed method has achieved better performance in terms of AUC.","Cancer detection,DenseNet,Attention Mechanism,Prior Knowledge Incorporation",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
12,LEARNING WITH LESS DATA VIA WEAKLY LABELED PATCH CLASSIFICATION IN DIGITAL PATHOLOGY,,,471-475,"Teh Eu Wern,Taylor Graham W.","Teh EW,Taylor GW",Teh EW,,University of Guelph,"In Digital Pathology (DP), labeled data is generally very scarce due to the requirement that medical experts provide annotations. We address this issue by learning transferable features from weakly labeled data, which are collected from various parts of the body and are organized by non-medical experts. In this paper, we show that features learned from such weakly labeled datasets are indeed transferable and allow us to achieve highly competitive patch classification results on the colorectal cancer (CRC) dataset [1] and the PatchCamelyon (PCam) dataset [2] while using an order of magnitude less labeled data.","Metric Learning,Few-shot Learning,Transfer Learning",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,http://arxiv.org/pdf/1911.12425,
13,WEAKLY-SUPERVISED DEEP STAIN DECOMPOSITION FOR MULTIPLEX IHC IMAGES,,,481-485,"Abousamra Shahira,Fassler Danielle,Hou Le,Zhang Yuwei,Gupta Rajarsi,Kurc Tahsin,Escobar-Hoyos Luisa F.,Samaras Dimitris,Knudson Beatrice,Shroyer Kenneth","Abousamra S,Fassler D,Hou L,Zhang YW,Gupta R,Kurc T,Escobar-Hoyos LF,Samaras D,Knudson B,Shroyer K",Abousamra S,,State University of New York (SUNY) System,"Multiplex immunohistochemistry (mIHC) is an innovative and cost-effective method that simultaneously labels multiple biomarkers in the same tissue section. Current platforms support labeling six or more cell types with different colored stains that can be visualized with brightfield light microscopy. However, analyzing and interpreting multi-colored images comprised of thousands of cells is a challenging task for both pathologists and current image analysis methods. We propose a novel deep learning based method that predicts the concentration of different stains at every pixel of a whole slide image (WSI). Our method incorporates weak annotations as training data: manually placed dots labelling different cell types based on color. We compare our method with other approaches and observe favorable performance on mIHC images.","Color decomposition,machine learning,microscopic images",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
14,MITOSIS DETECTION UNDER LIMITED ANNOTATION: A JOINT LEARNING APPROACH,,,486-489,"Pati Pushpak,Foncubierta-Rodriguez Antonio,Goksel Orcun,Gabrani Maria","Pati P,Foncubierta-Rodriguez A,Goksel O,Gabrani M",Pati P,,International Business Machines (IBM),"Mitotic counting is a vital prognostic marker of tumor proliferation in breast cancer. Deep learning-based mitotic detection is on par with pathologists, but it requires large labeled data for training. We propose a deep classification framework for enhancing mitosis detection by leveraging class label information, via softmax loss, and spatial distribution information among samples, via distance metric learning. We also investigate strategies towards steadily providing informative samples to boost the learning. The efficacy of the proposed framework is established through evaluation on ICPR 2012 and AMIDA 2013 mitotic data. Our framework significantly improves the detection with small training data and achieves on par or superior performance compared to state-of-the-art methods for using the entire training data.","Mitosis detection,Deep metric learning,Representation learning,Joint learning,Triplet sampling,Few annotations",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,BREAST-CANCER,,http://arxiv.org/pdf/2006.09772,
15,SIGNET RING CELLS DETECTION IN HISTOLOGY IMAGES WITH SIMILARITY LEARNING,,,490-494,"Sun Yibao,Huang Xingru,Molina Edgar Giussepi Lopez,Dong Le,Zhang Qianni","Sun YB,Huang XR,Molina EGL,Dong L,Zhang QN",Sun YB,,University of London,"The detection of signet ring cells in histology images is of great value in clinical practice. However, several reasons such as appearance variations and lack of well-labelled data make it a challenging task. Considering the intrinsic characteristics of signet ring cell images, a dedicated similarity learning network is designed in this paper to help the discovery of distinctive feature representations for ring cells. Specifically, we adapt the region proposal network and add an embedding layer to enable similarity learning for training the model. Experimental results show that similarity learning can strengthen the performance of the state-of-the-art and makes our approach competent for the task of signet ring cell detection.","Signet Ring Cell,Object Detection,Similarity Learning,Deep Learning,Computational Pathology",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,NUCLEI,,,
16,Weakly Supervised Multi-Task Learning for Cell Detection and Segmentation,,,513-516,"Chamanzar Alireza,Nie Yao","Chamanzar A,Nie Y",Chamanzar A,,"Roche Tissue Diagnost, Digital Pathol, Santa Clara, CA 95050 USA.","Cell detection and segmentation is fundamental for all downstream analysis of digital pathology images. However, obtaining the pixel-level ground truth for single cell segmentation is extremely labor intensive. To overcome this challenge, we developed an end-to-end deep learning algorithm to perform both single cell detection and segmentation using only point labels. This is achieved through the combination of different task orientated point label encoding methods and a multi-task scheduler for training. We apply and validate our algorithm on PMS2 stained colon rectal cancer and tonsil tissue images. Compared to the state-of-the-art, our algorithm shows significant improvement in cell detection and segmentation without increasing the annotation efforts.","Weakly supervised learning,Cell instance segmentation,Cell detection,Immunohistochemistry,Multi-task learning",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,http://arxiv.org/pdf/1910.12326,
17,PATIENT-SPECIFIC FINETUNING OF DEEP LEARNING MODELS FOR ADAPTIVE RADIOTHERAPY IN PROSTATE CT,,,577-580,"Elmandy Mohamed S.,Ahuja Tanuj,van der Heide U. A.,Staring Marius","Elmandy MS,Ahuja T,van der Heide UA,Staring M",Elmandy MS,,Leiden University,"Contouring of the target volume and Organs-At-Risk (OARs) is a crucial step in radiotherapy treatment planning. In an adaptive radiotherapy setting, updated contours need to be generated based on daily imaging. In this work, we leverage personalized anatomical knowledge accumulated over the treatment sessions, to improve the segmentation accuracy of a pre-trained Convolution Neural Network (CNN), for a specific patient. We investigate a transfer learning approach, finetuning the baseline CNN model to a specific patient, based on imaging acquired in earlier treatment fractions. The baseline CNN model is trained on a prostate CT dataset from one hospital of 379 patients. This model is then fine-tuned and tested on an independent dataset of another hospital of 18 patients, each having 7 to 10 daily CT scans. For the prostate, seminal vesicles, bladder and rectum, the model fine-tuned on each specific patient achieved a Mean Surface Distance (MSD) of 1.64 +/- 0.43 mm, 2.38 +/- 2.76 mm, 2.30 +/- 0.96 mm, and 1.24 +/- 0.89 mm, respectively, which was significantly better than the baseline model. The proposed personalized model adaptation is therefore very promising for clinical implementation in the context of adaptive radiotherapy of prostate cancer.","transfer learning,segmentation,prostate cancer,adaptive radiotherapy,organs at risk",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,http://arxiv.org/pdf/2002.06927,
18,METAL ARTIFACT REDUCTION AND INTRA COCHLEAR ANATOMY SEGMENTATION IN CT IMAGES OF THE EAR WITH A MULTI-RESOLUTION MULTI-TASK 3D NETWORK,,,596-599,"Wang Jianing,Noble Jack H.,Dawant Benoit M.","Wang JN,Noble JH,Dawant BM",Wang JN,,Vanderbilt University,"Segmenting the intra-cochlear anatomy structures (ICAs) in post-implantation CT (Post-CT) images of the cochlear implant ( CI) recipients is challenging due to the strong artifacts produced by the metallic CI electrodes. We propose a multi-resolution multi-task deep network which synthesizes an artifact-free image and segments the ICAs in the Post-CT images simultaneously. The output size of the synthesis branch is 1/64 of that of the segmentation branch. This reduces and the memory usage for training, while generating segmentation labels at a high resolution. In this preliminary study, we use the segmentation results of an automatic method as the ground truth to provide supervision to train our model, and we achieve a median Dice index value of 0.792. Our experiments also confirm the usefulness of the multi-task learning.","multi-task learning,segmentation,synthesis,metal artifact reduction,cochlear implant",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
19,COMBINING MULTIMODAL INFORMATION FOR METAL ARTEFACT REDUCTION: AN UNSUPERVISED DEEP LEARNING FRAMEWORK,,,600-604,"Ranzini Marta B. M.,Groothuis Irme,Klaser Kerstin,Cardoso M. Jorge,Henckel Johann,Ourselin Sebastien,Hart Alister,Modat Marc","Ranzini MBM,Groothuis I,Klaser K,Cardoso MJ,Henckel J,Ourselin S,Hart A,Modat M",Ranzini MBM,,University of London,"Metal artefact reduction (MAR) techniques aim at removing metal-induced noise from clinical images. In Computed Tomography (CT), supervised deep learning approaches have been shown effective but limited in generalisability, as they mostly rely on synthetic data. In Magnetic Resonance Imaging (MRI) instead, no method has yet been introduced to correct the susceptibility artefact, still present even in MARspecific acquisitions. In this work, we hypothesise that a multimodal approach to MAR would improve both CT and MRI. Given their different artefact appearance, their complementary information can compensate for the corrupted signal in either modality. We thus propose an unsupervised deep learning method for multimodal MAR. We introduce the use of Locally Normalised Cross Correlation as a loss term to encourage the fusion of multimodal information. Experiments show that our approach favours a smoother correction in the CT, while promoting signal recovery in the MRI.","Metal Artefact Reduction,CT,MR,Deep Learning,Unsupervised Learning",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,CT,,https://discovery.ucl.ac.uk/10102817/1/ISBI20_0238_FI.pdf,
20,A PHYSICS-MOTIVATED DNN FOR X-RAY CT SCATTER CORRECTION,,,609-613,"Iskender Berk,Bresler Yoram","Iskender B,Bresler Y",Iskender B,,University of Illinois System,"The scattering of photons by the imaged object in X-ray computed tomography (CT) produces degradations of the reconstructions in the form of streaks, cupping, shading artifacts and decreased contrast. We describe a new physics-motivated deep-learning-based method to estimate scatter and correct for it in the acquired projection measurements. The method incorporates both an initial reconstruction and the scatter-corrupted measurements using a specific deep neural network architecture and a cost function tailored to the problem. Numerical experiments show significant improvement over a recent projection-based deep neural network method.","Compton X-ray scatter,computed tomography (CT),CNN,deep learning,monte carlo",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
21,MULTI-CYCLE-CONSISTENT ADVERSARIAL NETWORKS FOR CT IMAGE DENOISING,,,614-618,"Liu Jinglan,Ding Yukun,Xiong Jinjun,Jia Qianjun,Huang Meiping,Zhuang Jian,Xie Bike,Liu Chun-Chen,Shi Yiyu","Liu JL,Ding YK,Xiong JJ,Jia QJ,Huang M,Zhuang J,Xie B,Liu CC,Shi Y",Liu JL,,University of Notre Dame,"CT image denoising can be treated as an image-to-image translation task where the goal is to learn the transform between a source domain X (noisy images) and a target domain Y (clean images). Recently, cycle-consistent adversarial denoising network (CCADN) has achieved state-of-the-art results by enforcing cycle-consistent loss without the need of paired training data. Our detailed analysis of CCADN raises a number of interesting questions. For example, if the noise is large leading to significant difference between domain X and domain Y, can we bridge X and Y with an intermediate domain Z such that both the denoising process between X and Z and that between Z and Y are easier to learn? As such intermediate domains lead to multiple cycles, how do we best enforce cycle-consistency? Driven by these questions, we propose a multi-cycle-consistent adversarial network (MCCAN) that builds intermediate domains and enforces both local and global cycle-consistency. The global cycle-consistency couples all generators together to model the whole denoising process, while the local cycle-consistency imposes effective supervision on the process between adjacent domains. Experiments show that both local and global cycle-consistency are important for the success of MCCAN, which outperforms the state-of-the-art.","Machine learning,Image enhancement/restoration (noise and artifact reduction),Computed tomography (CT),Multi-cycle-consistency",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,"LOW-DOSE,CT",,http://arxiv.org/pdf/2002.12130,
22,A COMPLETION NETWORK FOR RECONSTRUCTION FROM COMPRESSED ACQUISITION,,,619-623,"Ducros N.,Mur A. Lorente,Peyrin F.","Ducros N,Mur AL,Peyrin F",Ducros N,,Institut National des Sciences Appliquees de Lyon - INSA Lyon,"We consider here the problem of reconstructing an image from a few linear measurements. This problem has many biomedical applications, such as computerized tomography, magnetic resonance imaging and optical microscopy.
While this problem has long been solved by compressed sensing methods, these are now outperformed by deep-learning approaches. However, understanding why a given network architecture works well is still an open question.
In this study, we proposed to interpret the reconstruction problem as a Bayesian completion problem where the missing measurements are estimated from those acquired. From this point of view, a network emerges that includes a fully connected layer that provides the best linear completion scheme. This network has a lot fewer parameters to learn than direct networks, and it trains more rapidly than image-domain networks that correct pseudo inverse solutions. Although, this study focuses on computational optics, it might provide some insight for inverse problems that have similar formulations.","Image reconstruction,deep learning,Bayesian completion,fully connected layer",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,https://hal.archives-ouvertes.fr/hal-02342766v2/document,
23,UNSUPERVISED CONE-BEAM ARTIFACT REMOVAL USING CYCLEGAN AND SPECTRAL BLENDING FOR ADAPTIVE RADIOTHERAPY,,,638-641,"Park Sangjoon,Ye Jong Chul","Park S,Ye JC",Park S,,Korea Advanced Institute of Science & Technology (KAIST),"Cone-beam computed tomography (CBCT) used in radiotherapy (RT) has the advantage of being taken daily, but is difficult to use for purposes other than patient setup because of the poor image quality compared to fan-beam computed tomography (CT). Even though several methods have been proposed including the deformable image registration method to improve the quality of CBCT, the outcomes have not yet been satisfactory. Recently, deep learning has shown to produce high-quality results for various image-to-image translation tasks, suggesting the possibility of being an effective tool for converting CBCT into CT. In the field of RT, however, it may not always be possible to obtain paired datasets which consist of exactly matching CBCT and CT images. This study aimed to develop a novel, unsupervised deep-learning algorithm, which requires only unpaired CBCT and fan-beam CT images to remove the cone-beam artifact and thereby improve the quality of CBCT. Specifically, two cycle consistency generative adversarial networks (CycleGAN) were trained in the sagittal and coronal directions, and the generated results along those directions were then combined using spectral blending technique. To evaluate our methods, we applied it to American Association of Physicists in Medicine dataset. The experimental results show that our method outperforms the existing CylceGAN-based method both qualitatively and quantitatively.","Computed Tomography,Cone-beam artifact,Unsupervised learning,CycleGAN,Spectral blending",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,HEAD,,,
24,CONE-ANGLE ARTIFACT REMOVAL USING DIFFERENTIATED BACKPROJECTION DOMAIN DEEP LEARNING,,,642-645,"Kim Junyoung,Han Yoseob,Ye Jong Chul","Kim J,Han Y,Ye JC",Kim J,,Korea Advanced Institute of Science & Technology (KAIST),"For circular trajectory conebeam CT, Feldkamp, Davis, and Kress (FDK) algorithm is widely used for its reconstruction. However, the existence of cone-angle artifacts is fatal for the quality when using this algorithm. There are several model-based iterative reconstruction methods for the cone-angle artifacts removal, but these algorithms usually require repeated applications of computational expensive forward and backward. In this paper, we propose a novel deep learning approach for cone-angle artifact removal on differentiated back-projection domain, which performs a data-driven inversion of an ill-posed deconvolution problem related to the Hilbert transform. The reconstruction results along the coronal and sagittal directions are then combined by a spectral blending technique to minimize the spectral leakage. Experimental results show that our method provides superior performance to the existing","Computed Tomography,Cone-beam artifact,Deep Learning,Spectral blending",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,"CONVOLUTIONAL,NEURAL-NETWORK,IMAGE-RECONSTRUCTION,BEAM,RECONSTRUCTION,COMPUTED-TOMOGRAPHY,PI-LINES,CT",,,
25,IMAGE-DOMAIN MATERIAL DECOMPOSITION USING AN ITERATIVE NEURAL NETWORK FOR DUAL-ENERGY CT,,,651-655,"Li Zhipeng,Chun Il Yong,Long Yong","Li ZP,Chun IY,Long Y",Long Y,,Shanghai Jiao Tong University,"Image-domain material decomposition is susceptible to noise and artifacts in dual-energy CT (DECT) attenuation images. To obtain high quality material images from DECT, data-driven methods are attracting widespread attention. Iterative neural network (INN) approaches achieved high image reconstruction quality and low generalization error in several inverse imaging problems. BCD-Net is an INN of which architecture is constructed by generalizing a block coordinate descent (BCD) algorithm that solves model-based image reconstruction using learned convolutional regularizers. We propose a new INN architecture for DECT material decomposition by replacing a model-based image reconstruction module of BCD-Net with a model-based image decomposition (MBID) module. Experiments with the extended cardiactorso (XCAT) phantom and patient data show that the proposed method greatly improves image decomposition quality compared to a conventional MBID method using an edge-preserving hyperbola regularizer and a state-of-the-art learned MBID method that uses different pre-learned sparsifying transforms for different materials.","Dual-energy CT,Iterative neural network",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,"MULTIMATERIAL,DECOMPOSITION",,,
26,DIGITAL BREAST TOMOSYNTHESIS RECONSTRUCTION WITH DEEP NEURAL NETWORK FOR IMPROVED CONTRAST AND IN-DEPTH RESOLUTION,,,656-659,"Wu Dufan,Kim Kyungsang,Li Quanzheng","Wu DF,Kim K,Li QZ",Wu DF,,Harvard University,"Digital breast tomosynthesis (DBT) provides 3D reconstruction which reduces the superposition and overlapping of breast tissues compared to mammography, leading to increased sensitivity and specificity. However, due to the limited angular sampling, DBT images are still accompanied with severe artifacts and limited in-depth resolution. In this paper, we proposed a deep learning-based DBT reconstruction method to mitigate the limited angular artifacts and improve in-depth resolution. An unroll-type neural network was used with decoupled training for each unroll to reduce training-time computational cost. A novel region of interest loss on inserted microcalcifications was further proposed to improve the spatial resolution and contrast of the microcalcifications. The network was trained and tested on 176 realistic breast phantoms, and improved in-plane contrast (3.17 versus 0.43, p < 0.01) and in-depth resolution (1.19 mm versus 4.96 mm, p < 0.01) was demonstrated by the proposed method compare to iterative reconstruction.","Breast tomosynthesis,reconstruction,deep learning,limited angle",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
27,LOW-SHOT LEARNING OF AUTOMATIC DENTAL PLAQUE SEGMENTATION BASED ON LOCAL-TO-GLOBAL FEATURE FUSION,,,664-668,"Li Shuai,Pang Zhennan,Song Wenfeng,Guo Yuting,You Wenzhe,Hao Aimin,Qin Hong","Li S,Pang ZN,Song WF,Guo YT,You WZ,Hao AM,Qin H",Li S,,Beihang University,"The early detection of dental plaque could prevent periodontal diseases and dental caries, however, it is difficult to recognize it without the use of medical dyeing reagent due to the low contrast between dental plaque and teeth. To combat this problem, this paper introduces a novel low-shot learning method of the intelligent dental plaque segmentation directly using oral endoscope images. The key contribution is to conduct low-shot learning at the super-pixel level and integrate the super-pixels' global and local features towards better segmentation results. Our rationale is that, super-pixel based CNN feature focuses on the statistical distribution of plaques' color, heat kernel signature (HKS) aims to capture the local-to-global structure relationship in the nearby regions centering around plaque area, and circle-LBP feature depicts the local texture pattern on the plaque area. The experimental results confirm that our method outperforms the state-of-the-art methods based on small scale training datasets, and the user study demonstrates our method is more accurate than conventional manual results delineated by experienced dentists.","Dental Plaque,Low-shot Learning,Feature Fusion,Automatic Segmentation,Oral Endoscope Image",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
28,TOOTH SEGMENTATION AND LABELING FROM DIGITAL DENTAL CASTS,,,669-673,"Sun Diya,Pei Yuru,Song Guangying,Guo Yuke,Ma Gengyu,Xu Tianmin,Zha Hongbin","Sun DY,Pei YR,Song GY,Guo YK,Ma GY,Xu TM,Zha HB",Pei YR,,Peking University,"This paper presents an approach to automatic and accurate segmentation and identification of individual teeth from digital dental casts via deep graph convolutional neural networks. Instead of performing the teeth-gingiva and inter-tooth segmentation in two separate phases, the proposed method enables the simultaneous segmentation and identification of the gingiva and teeth. We perform the vertex-wise feature learning via the feature steered graph convolutional neural network (FeaStNet) [1] that dynamically updates the mapping between convolutional filters and local patches from digital dental casts. The proposed framework handles the tightly intertwined segmentation and labeling tasks with a novel constraint on crown shape distribution and concave contours to remove ambiguous labeling of neighboring teeth. We further enforce the smooth segmentation using the pairwise relationship in local patches to penalize rough and inaccurate region boundaries and regularize the vertex-wise labeling in the training process. The qualitative and quantitative evaluations on the digital dental casts obtained in the clinical orthodontics demonstrate that the proposed method achieves efficient and accurate tooth segmentation and produces performance improvements to the state-of-the-art.","Teeth and gingiva segmentation,digital dental cast,FeaStNet,crown shape distribution",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
29,A DEEP LEARNING-FACILITATED RADIOMICS SOLUTION FOR THE PREDICTION OF LUNG LESION SHRINKAGE IN NON-SMALL CELL LUNG CANCER TRIALS,,,678-682,"Chen Antong,Saouaf Jennifer,Zhou Bo,Crawford Randolph,Yuan Jianda,Ma Junshui,Baumgartner Richard,Wang Shubing,Goldmacher Gregory","Chen AT,Saouaf J,Zhou B,Crawford R,Yuan JD,Ma JS,Baumgartner R,Wang SB,Goldmacher G",Chen AT,,Merck & Company,"Herein we propose a deep learning-based approach for the prediction of lung lesion response based on radiomic features extracted from clinical CT scans of patients in non-small cell lung cancer trials. The approach starts with the classification of lung lesions from the set of primary and metastatic lesions at various anatomic locations. Focusing on the lung lesions, we perform automatic segmentation to extract their 3D volumes. Radiomic features are then extracted from the lesion on the pre-treatment scan and the first follow-up scan to predict which lesions will shrink at least 30% in diameter during treatment (either Pembrolizumab or combinations of chemotherapy and Pembrolizumab), which is defined as a partial response by the Response Evaluation Criteria In Solid Tumors (RECIST) guidelines. A 5-fold cross validation on the training set led to an AUC of 0.84 +/- 0.03, and the prediction on the testing dataset reached AUC of 0.73 +/- 0.02 for the outcome of 30% diameter shrinkage.","radiomics,convolutional neural networks,segmentation,quantitative vessel tortuosity",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,http://arxiv.org/pdf/2003.02943,
30,A GENERALIZABLE FRAMEWORK FOR DOMAIN-SPECIFIC NONRIGID REGISTRATION: APPLICATION TO CARDIAC ULTRASOUND,,,696-699,"Peoples Jacob,Ellis Randy","Peoples J,Ellis R",Peoples J,,Queens University - Canada,"Many applications of nonrigid point set registration could benefit from a domain-specific model of allowed deformations. We observe that registration methods using mixture models optimize a differentiable log-likelihood function and are thus amenable to gradient-based optimization. In theory, this allows optimization of any transformations that are expressed as arbitrarily nested differentiable functions. In practice such optimization problems are readily handled with modern machine learning tools. We demonstrate, in experiments on synthetic data generated from a model of the left cardiac ventricle, that complex nested transformations can be robustly optimized using this approach. As a realistic application, we also use the method to propagate the model through an entire cardiac ultrasound sequence. We conclude that this approach, which works with both points and oriented points, provides an easily generalizable framework in which complex, application-specific transformation models may be constructed and optimized.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
31,IMPROVING INTERPRETABILITY OF 2-D ULTRASOUND OF THE LUMBAR SPINE,,,717-721,"Porto Lucas,Rohling Robert","Porto L,Rohling R",Porto L,,University of British Columbia,"Ultrasound-guided anesthesia uses a safe, portable imaging modality to provide visual feedback during the needle injection. Widespread adoption of ultrasound-guided anesthesia has been primarily limited by a lack of access to advanced ultrasound technology and a lack of ultrasound training for anesthesiologists. We sought to address these limitations by introducing a method that aids the interpretability of cross-sectional ultrasound from conventional (2D) machines. We propose a constrained registration of a 3D active shape model constructed from computerized tomography (CT) scans of the lumbar spine to a specific set of targets automatically extracted from 2D B-mode ultrasound images with machine learning models. The registration results in an overlay of the entire bone cross-section of the lumbar spine onto the ultrasound image. Our proposed registration achieved a mean squared error of 1.4 +/- 0.3 mm on a set of 43 ultrasound images, which is smaller than the key anatomical features, suggesting that the overlay is suitable for interpretation.","Regional anesthesia,image registration,active shape models,ultrasound",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,GUIDANCE,,,
32,LEARNING OPTIMAL SHAPE REPRESENTATIONS FOR MULTI-MODAL IMAGE REGISTRATION,,,722-725,"Grossiord Eloise,Risser Laurent,Kanoun Salim,Ken Soleakhena,Malgouyres Francois","Grossiord E,Risser L,Kanoun S,Ken S,Malgouyres F",Grossiord E,,Universite de Toulouse,"In this work, we present a new strategy for the multimodal registration of atypical structures with boundaries that are difficult to define in medical imaging (e.g. lymph nodes). Instead of using a standard Mutual Information (MI) similarity metric, we propose to use the combination of MI with the Modality Independent Neighbourhood Descriptors (MIND) that can help enhancing the organs of interest from their adjacent structures. Our key contribution is then to learn the MIND parameters which optimally represent specific registered structures. As we register atypical organs, NeuralNetwork approaches requiring large databases of annotated training data cannot be used. We rather strongly constrain our learning problem using the MIND formalism, so that the optimal representation of images depends on a limited amount of parameters. In our results, pure MI-based registration is compared with MI-MIND registration on 3D synthetic images and CT/MR images, leading to improved structure overlaps by using MI-MIND. To our knowledge, this is the first time that MIND-MI is evaluated and appears as relevant for multi-modal registration.","Multimodal registration,MIND representation,simulated annealing,MRI,CT",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,"MUTUAL,INFORMATION,MAXIMIZATION,ENTROPY",,,
33,A NOVEL APPROACH TO VERTEBRAL COMPRESSION FRACTURE DETECTION USING IMITATION LEARNING AND PATCH BASED CONVOLUTIONAL NEURAL NETWORK,,,726-730,"Iyer Sankaran,Sowniya Arcot,Blair Alan,White Christopher,Dawes Laughlin,Moses Daniel","Iyer S,Sowniya A,Blair A,White C,Dawes L,Moses D",Iyer S,,University of New South Wales Sydney,"Compression Fractures in vertebrae often go undetected clinically due to various reasons. If left untreated, they can lead to severe secondary fractures due to osteoporosis. We present here a novel fully automated approach to the detection of Vertebral Compression Fractures (VCF). The method involves 3D localisation of thoracic and lumbar spine regions using Deep reinforcement Learning and Imitation Learning. The localised region is then split into 2D sagittal slices around the coronal centre. Each slice is further divided into patches, on which a Convolutional Neural Network (CNN) is trained to detect compression fractures. Experiments for localisation achieved an average Jaccard Index/Dice Coefficient score of 74/85% for 144 CT chest images and 77/86% for 132 CT abdomen images. VCF Detection was performed on another 127 chest images and after localisation, resulted in an average fivefold cross validation accuracy of 80%, sensitivity of 79.87% and specificity of 80.73%.","3D Localisation,VCF detection,Deep Reinforcement Learning,Imitation Learning,Convolutional Neural Networks",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
34,Attention-based CNN for KL Grade Classification: Data from the Osteoarthritis Initiative,,,731-735,"Zhang Bofei,Tan Jimin,Cho Kyunghyun,Chang Gregory,Deniz Cem M.","Zhang BF,Tan JM,Cho KY,Chang G,Deniz CM",Zhang BF,,New York University,"Knee osteoarthritis (OA) is a chronic degenerative disorder of joints and it is the most common reason leading to total knee joint replacement. Diagnosis of OA involves subjective judgment on symptoms, medical history, and radiographic readings using Kellgren-Lawrence grade (KL-grade). Deep learning-based methods such as Convolution Neural Networks (CNN) have recently been applied to automatically diagnose radiographic knee OA. In this study, we applied Residual Neural Network (ResNet) to first detect knee joint from radiographs and later combine ResNet with Convolutional Block Attention Module (CBAM) to make a prediction of the KL-grade automatically. The proposed model achieved a multi-class average accuracy of 74.81%, mean squared error of 0.36, and quadratic Kappa score of 0.88, which demonstrates a significant improvement over the published results. The attention maps were analyzed to provide insights on the decision process of the proposed model(1).","deep learning,osteoarthritis,knee,radiography,convolutional neural networks",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
35,TOWARDS SHAPE-BASED KNEE OSTEOARTHRITIS CLASSIFICATION USING GRAPH CONVOLUTIONAL NETWORKS,,,750-753,von Tycowicz Christoph,von Tycowicz C,von Tycowicz C,,Zuse Institute Berlin,We present a transductive learning approach for morphometric osteophyte grading based on geometric deep learning. We formulate the grading task as semi-supervised node classification problem on a graph embedded in shape space. To account for the high-dimensionality and non-Euclidean structure of shape space we employ a combination of an intrinsic dimension reduction together with a graph convolutional neural network. We demonstrate the performance of our derived classifier in comparisons to an alternative extrinsic approach.,"Geometric deep learning,computer-aided diagnosis,shape analysis,non-Euclidean statistics",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,PROGRESSION,,http://arxiv.org/pdf/1910.06119,
36,LEARNING PROBABILISTIC FUSION OF MULTILABEL LESION CONTOURS,,,767-771,"Cohen Gal,Greenspan Hayit,Goldberger Jacob","Cohen G,Greenspan H,Goldberger J",Cohen G,,Tel Aviv University,"Supervised machine learning algorithms, especially in the medical domain, are affected by considerable ambiguity in expert markings, primarily in proximity to lesion contours. In this study we address the case where the experts opinion for those ambiguous areas is considered as a distribution over the possible values. We propose a novel method that modifies the experts' distributional opinion at ambiguous areas by fusing their markings based on their sensitivity and specificity. The algorithm can be applied at the end of any label fusion algorithm that can handle soft values. The algorithm was applied to obtain consensus from soft Multiple Sclerosis (MS) segmentation masks. Soft MS segmentations are constructed from manual binary delineations by including lesion surrounding voxels in the segmentation mask with a reduced confidence weight. The method was evaluated on the MICCAI 2016 challenge dataset, and outperformed previous methods.","multiple sclerosis,segmentation,multiple annotators,soft labels",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
37,CLASSIFICATION OF LUNG NODULES IN CT VOLUMES USING THE LUNG-RADSTM GUIDELINES WITH UNCERTAINTY PARAMETERIZATION,,,791-794,"Ferreira Carlos A.,Aresta Guilherme,Pedrosa Joao,Rebelo Joao,Negrao Eduardo,Cunha Antonio,Ramos Isabel,Campilho Aurelio","Ferreira CA,Aresta G,Pedrosa J,Rebelo J,Negrao E,Cunha A,Ramos I,Campilho A",Ferreira CA,,"Inst Syst & Comp Engn Technol & Sci INESC TEC, Porto, Portugal.","Currently, lung cancer is the most lethal in the world. In order to make screening and follow-up a little more systematic, guidelines have been proposed. Therefore, this study aimed to create a diagnostic support approach by providing a patient label based on the LUNG-RADSTM guidelines. The only input required by the system is the nodule centroid to take the region of interest for the input of the classification system. With this in mind, two deep learning networks were evaluated: a Wide Residual Network and a DenseNet. Taking into account the annotation uncertainty we proposed to use sample weights that are introduced in the loss function, allowing nodules with a high agreement in the annotation process to take a greater impact on the training error than its counterpart. The best result was achieved with the Wide Residual Network with sample weights achieving a nodule-wise LUNG-RADSTM labelling accuracy of 0.735 +/- 0.003.","LUNG-RADS (TM),uncertainty,nodule classification,malignancy,LIDC-IDRI",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
38,DEEP FEATURE DISENTANGLEMENT LEARNING FOR BONE SUPPRESSION IN CHEST RADIOGRAPHS,,,795-798,"Lin Chunze,Tang Ruixiang,Lin Darryl D.,Liu Langechuan,Lu Jiwen,Chen Yunqiang,Gao Dashan,Zhou Jie","Lin CZ,Tang RX,Lin DD,Liu LGC,Lu JW,Chen YQ,Gao DS,Zhou J",Lin CZ,,Tsinghua University,"Suppression of bony structures in chest radiographs is essential for many computer-aided diagnosis tasks. In this paper, we propose a Disentanglement AutoEncoder (DAE) for bone suppression. As the projection of 3D structures of bones and soft tissues overlap in 2D radiographs, their features are interwoven and need to be disentangled for effective bone suppression. Our DAE progressively separates the features of soft-tissues from that of the bony structure during the encoder phase and reconstructs the soft-tissue image based on the disentangled features of soft-tissue. Bone segmentation can be performed concurrently using the separated bony features through a separate multi-task branch. By training the model with multi-task supervision, we explicitly encourage the autoencoder to pay more attention to the locations of bones in order to avoid loss of soft-tissue information. The proposed method is shown to be effective in suppressing bone structures from chest radiographs with very little visual artifacts.","Bone suppression,feature disentanglement,auto-encoders,deep learning",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
39,AUTOMATIC BOUNDING BOX ANNOTATION OF CHEST X-RAY DATA FOR LOCALIZATION OF ABNORMALITIES,,,799-803,"Wu Joy,Gur Yaniv,Karargyris Alexandros,Bin Syed Ali,Moradi Mehdi,Syeda-Mahmood Tanveer","Wu J,Gur Y,Karargyris A,Bin Syed A,Moradi M,Syeda-Mahmood T",Wu J,,International Business Machines (IBM),"Automatic detection of findings and their locations in chest x-ray studies is an important research area for AI application in healthcare. Whereas for finding classification tasks image-level labeling suffices, additional annotation in the form of bounding boxes is required for detection of findings locations. However, the process of locally marking findings on chest x-ray images is both time consuming and costly as it needs to be performed by radiologists. To address this problem, weakly supervised approaches have been employed to depict finding locations by looking at attention maps produced by convolution networks trained for findings classification. However, these approaches have not shown much promise so far and raised concerns whether the networks are actually focusing on the right abnormality regions. With this in mind, in this paper we propose an automatic approach for labeling chest x-ray images for findings and locations by leveraging radiology reports. Our labeling approach is anatomically standardized to the upper, middle, and lower lung zones for the left and right lungs, and is composed of two stages. In the first stage, we use a lungs segmentation UNet model and an atlas of normal patients to mark the six lung zones on the image with standardized bounding boxes. In the second stage, the associated radiology report is used to label each lung zone as positive or negative for finding, resulting in a set of six labeled bounding boxes per image. Using this approach, we automatically annotated a dataset of 13911 CXR images in a matter of hours, with an average annotation recall of 0.881 and precision of 0.896 when evaluated on 300 dual validated images. Finally, we used this ""silver"" bounding boxes dataset to train an opacity detection model using a RetinaNet architecture, and obtained localization results on par with the state-of-the-art.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
40,AIRWAYNET-SE: A SIMPLE-YET-EFFECTIVE APPROACH TO IMPROVE AIRWAY SEGMENTATION USING CONTEXT SCALE FUSION,,,809-813,"Qin Yulei,Gu Yun,Zheng Hao,Chen Mingjian,Yang Jie,Zhu Yue-Min","Qin YL,Gu Y,Zheng H,Chen MJ,Yang J,Zhu YM",Gu Y; Yang J,,Shanghai Jiao Tong University,"Accurate segmentation of airways from chest CT scans is crucial for pulmonary disease diagnosis and surgical navigation. However, the intra-class variety of airways and their intrinsic tree-like structure pose challenges to the development of automatic segmentation methods. Previous work that exploits convolutional neural networks (CNNs) does not take context scales into consideration, leading to performance degradation on peripheral bronchiole. We propose the two-step AirwayNet-SE, a Simple-yet-Effective CNNs-based approach to improve airway segmentation. The first step is to adopt connectivity modeling to transform the binary segmentation task into 26-connectivity prediction task, facilitating the model's comprehension of airway anatomy. The second step is to predict connectivity with a two-stage CNNs-based approach. In the first stage, a Deep-yet-Narrow Network (DNN) and a Shallow-yet-Wide Network (SWN) are respectively utilized to learn features with large-scale and small-scale context knowledge. These two features are fused in the second stage to predict each voxel's probability of being airway and its connectivity relationship between neighbors. We trained our model on 50 CT scans from public datasets and tested on another 20 scans. Compared with state-of-the-art airway segmentation methods, the robustness and superiority of the AirwayNet-SE confirmed the effectiveness of large-scale and small-scale context fusion. In addition, we released our manual airway annotations of 60 CT scans from public datasets for supervised airway segmentation study.","Airway segmentation,context scale,voxel connectivity,convolutional neural networks",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
41,DEEP LEARNING FOR TIME AVERAGED WALL SHEAR STRESS PREDICTION IN LEFT MAIN CORONARY BIFURCATIONS,,,818-821,"Gharleghi Ramtin,Samarasinghe Gihan,Sowmya Arcot,Beier Susann","Gharleghi R,Samarasinghe G,Sowmya A,Beier S",Gharleghi R,,University of New South Wales Sydney,"Analysing blood flow in coronary arteries has often been suggested in aiding the prediction of cardiovascular disease (CVD) risk. Blood flow induced hemodynamic indices can function as predictive measures in this pursuit and a fast method to calculate these may allow patient specific treatment considerations for improved clinical outcomes in the future. In vivo measurements of these metrics are not practical and thus computational fluid dynamic simulations (CFD) are widely used to investigate blood flow conditions, but require costly computation time for large scale studies such as patient specific considerations in patients screened for CVD. This paper proposes a deep learning approach to estimating the well established hemodynamic risk indicator time average wall shear stress (TAWSS) based on the vessel geometry. The model predicts TAWSS with good accuracy, achieving cross validation results of average Mean Absolute error of 0.0407 Pa and standard deviation of 0.002 Pa on a 127 patient CT angiography dataset, while being several orders of magnitude faster than computational simulations, using the vessel radii, angles between bifurcation (branching) vessels, curvature and other geometrical features. This bypasses costly computational simulations and allows large scale population studies as required for meaningful CVD risk prediction.","Machine learning,Vessels,Computed tomography (CT)",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,FLOW,,,
42,Machine Learning and Graph Based Approach to Automatic Right Atrial Segmentation from Magnetic Resonance Imaging,,,826-829,"Regehr Matthew,Volk Andrew,Noga Michelle,Punithakumar Kumaradevan","Regehr M,Volk A,Noga M,Punithakumar K",Regehr M,,University of Alberta,"Manual delineation of the right atrium throughout the cardiac cycle is tedious and time-consuming, yet promising for early detection of right heart dysfunction. In this study, we developed a fully automated approach to right atrial segmentation in 4-chamber long-axis magnetic resonance image (MRI) cine sequences by applying a U-Net based neural network approach followed by a contour reconstruction and refinement algorithm. In contrast to U-Net, the proposed approach performs segmentation using open contours. This allows for exclusion of the tricuspid valve region from the atrial segmentation, an essential aspect in the analysis of atrial wall motion. The MR images were retrospectively acquired from 242 cine sequences which were manually segmented by an expert radiologist to produce the ground truth data. The neural network was trained over 600 epochs under six different hyperparameter configurations on 202 randomly selected sequences to recognize a dilated region surrounding the right atrial contour. A graph algorithm is then applied to the binary labels predicted by the trained model to accurately reconstruct the corresponding contours. Finally, the contours are refined by combining a nonrigid registration algorithm which tracks the deformation of the heart and a Gaussian process regression. Evaluation of the proposed method on the remaining 40 MR image sequences excluding a single outlier sequence yielded promising Sorensen-Dice coefficients and Hausdorff distances of 95.2% and 4.64 mm respectively before refinement and 94.9% and 4.38 mm afterward.","Right Atrial Segmentation,Magnetic Resonance Imaging,Convolutional Neural Networks,Graph Algorithms,Machine Learning",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,"DEFORMATION,TETRALOGY,SIZE",,,
43,Accelerated Phase Contrast Magnetic Resonance Imaging via Deep Learning,,,834-838,"Nath Ruponti,Callahan Sean,Singam Narayana,Stoddard Marcus,Amini Amir A.","Nath R,Callahan S,Singam N,Stoddard M,Amini AA",Nath R,,University of Louisville,"In this paper, we propose a framework for accelerated reconstruction of 2D phase contrast magnetic resonance images from undersampled k-space domain by using deep learning methods. Undersampling in k-space violates Nyquist Sampling and creates artifacts in the image domain. In the proposed method, we consider the reconstruction problem as a de-aliasing problem in complex spatial domain. To test the proposed method, from fully sampled k-space data undersampling in k-space was performed in the phase-encode direction based on a probability density function which ensures maximum rate of sampling in low frequency regions. For the deep convolutional neural network (CNN) we chose the U-net architecture. The proposed CNN was trained and tested on 4D flow MRI data in 14 subjects with aortic stenosis. The reconstructed complex two channel image showed that the U-net is able to unaliase the undersampled flow images with resulting magnitude and phase difference images showing good agreement with the fully sampled magnitude and phase images. We show that the proposed method outperforms 2D compressed sensing approach of spatial total variation regularization method. Flow waveforms derived from reconstructed images closely follow flow waveforms derived from the original data. Moreover, the method is computationally fast. Each 2D magnitude and phase image is reconstructed within a second using a single GPU.","Aortic Stenosis,Phase-Contrast MRI,Deep Learning,Compressed Sensing,4D Flow MRI",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,MRI,,,
44,A ONE-SHOT LEARNING FRAMEWORK FOR ASSESSMENT OF FIBRILLAR COLLAGEN FROM SECOND HARMONIC GENERATION IMAGES OF AN INFARCTED MYOCARDIUM,,,839-843,"Liu Qun,Mukhopadhyay Supratik,Rodriguez Maria Ximena Bastidas,Fu Xing,Sahu Sushant,Burk David,Gartia Manas","Liu Q,Mukhopadhyay S,Rodriguez MXB,Fu X,Sahu S,Burk D,Gartia M",Liu Q,,Louisiana State University System,"Myocardial infarction (MI) is a scientific term that refers to heart attack. In this study, we infer highly relevant second harmonic generation (SHG) cues from collagen fibers exhibiting highly non-centrosymmetric assembly together with two-photon excited cellular autofluorescence in infarcted mouse heart to quantitatively probe fibrosis, especially targeted at an early stage after MI. We present a robust one-shot machine learning algorithm that enables determination of 2D assembly of collagen with high spatial resolution along with its structural arrangement in heart tissues post-MI with spectral specificity and sensitivity. Detection, evaluation, and precise quantification of fibrosis extent at early stage would guide one to develop treatment therapies that may prevent further progression and determine heart transplant needs for patient survival.","Myocardial Infarction,Fibrosis,Second Harmonic Generation Microscopy",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,http://arxiv.org/pdf/2001.08395,
45,BONE STRUCTURES EXTRACTION AND ENHANCEMENT IN CHEST RADIOGRAPHS VIA CNN TRAINED ON SYNTHETIC DATA,,,858-861,"Gozes Ophir,Greenspan Hayit","Gozes O,Greenspan H",Gozes O,,Tel Aviv University,"In this paper, we present a deep learning-based image processing technique for extraction of bone structures in chest radiographs using a U-Net FCNN. The U-Net was trained to accomplish the task in a fully supervised setting. To create the training image pairs, we employed simulated X-Ray or Digitally Reconstructed Radiographs (DRR), derived from 664 CT scans belonging to the LIDC-IDRI dataset. Using HU based segmentation of bone structures in the CT domain, a synthetic 2D ""Bone x-ray"" DRR is produced and used for training the network. For the reconstruction loss, we utilize two loss functions- L1 Loss and perceptual loss. Once the bone structures are extracted, the original image can be enhanced by fusing the original input x-ray and the synthesized ""Bone X-ray"". We show that our enhancement technique is applicable to real x-ray data, and display our results on the NIH Chest X-Ray-14 dataset.","Deep learning,Image synthesis,Image enhancement,CT,X-ray,DRR",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,RIBS,,http://arxiv.org/pdf/2003.10839,
46,ZERO-SHOT MEDICAL IMAGE ARTIFACT REDUCTION,,,862-866,"Chen Yu-Jen,Chang Yen-Jung,Wen Shao-Cheng,Shi Yiyu,Xu Xiaowei,Ho Tsung-Yi,Jia Qianjun,Huang Meiping,Zhuang Jian","Chen YJ,Chang YJ,Wen SC,Shi YY,Xu XW,Ho TY,Jia QJ,Huang MP,Zhuang J",Chen YJ,,National Tsing Hua University,"Medical images may contain various types of artifacts with different patterns and mixtures, which depend on many factors such as scan setting, machine condition, patients' characteristics, surrounding environment, etc. However, existing deep learning based artifact reduction methods are restricted by their training set with specific predetermined artifact type and pattern. As such, they have limited clinical adoption. In this paper, we introduce a ""Zero-Shot"" medical image Artifact Reduction (ZSAR) framework, which leverages the power of deep learning but without using general pre-trained networks or any clean image reference. Specifically, we utilize the low internal visual entropy of an image and train a light-weight image-specific artifact reduction network to reduce artifacts in an image at test-time. We use Computed Tomography (CT) and Magnetic Resonance Imaging (MRI) as vehicles to show that ZSAR can reduce artifacts better than the state-of-the-art both qualitatively and quantitatively, while using shorter test time. To the best of our knowledge, this is the first deep learning framework that reduces artifacts in medical images without using a priori training set.","Image denoising,Deep learning,Zero-Shot",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
47,Deblurring Cataract Surgery Videos Using a Multi-Scale Deconvolutional Neural Network,,,872-876,"Ghamsarian Negin,Taschwer Mario,Schoeffmann Klaus","Ghamsarian N,Taschwer M,Schoeffmann K",Ghamsarian N,,University of Klagenfurt,"A common quality impairment observed in surgery videos is blur, caused by object motion or a defocused camera. Degraded image quality hampers the progress of machine-learning-based approaches in learning and recognizing semantic information in surgical video frames like instruments, phases, and surgical actions. This problem can be mitigated by automatically deblurring video frames as a preprocessing method for any subsequent video analysis task. In this paper, we propose and evaluate a multi-scale deconvolutional neural network to deblur cataract surgery videos. Experimental results confirm the effectiveness of the proposed approach in terms of the visual quality of frames as well as PSNR improvement.","Video Deblurring,Deconvolutional Neural Networks,Cataract Surgery Videos",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,"BLUR,IDENTIFICATION",,,
48,Substituting Gadolinium in Brain MRI Using DeepContrast,,,908-912,"Sun Haoran,Liu Xueqing,Feng Xinyang,Liu Chen,Zhu Nanyan,Gjerswold-Selleck Sabrina J.,Wei Hong-Jian,Upadhyayula Pavan S.,Mela Angeliki,Wu Cheng-Chia","Sun HR,Liu XQ,Feng XY,Liu C,Zhu NY,Gjerswold-Selleck SJ,Wei HJ,Upadhyayula PS,Mela A,Wu CC",Sun HR,,Columbia University,"Cerebral blood volume (CBV) is a hemodynamic correlate of oxygen metabolism and reflects brain activity and function. High-resolution CBV maps can be generated using the steady-state gadolinium-enhanced MRI technique. Such a technique requires an intravenous injection of exogenous gadolinium based contrast agent (GBCA) and recent studies suggest that the GBCA can accumulate in the brain after frequent use. We hypothesize that endogenous sources of contrast might exist within the most conventional and commonly acquired structural MRI, potentially obviating the need for exogenous contrast. Here, we test this hypothesis by developing and optimizing a deep learning algorithm, which we call DeepContrast, in mice. We find that DeepContrast performs equally well as exogenous GBCA in mapping CBV of the normal brain tissue and enhancing glioblastoma. Together, these studies validate our hypothesis that a deep learning approach can potentially replace the need for GBCAs in brain MRI.","Deep Learning,MRI,Gadolinium,CBV,Glioblastoma",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,"CEREBRAL,BLOOD-VOLUME,HIPPOCAMPAL,DYSFUNCTION,PERFUSION",,http://arxiv.org/pdf/2001.05551,
49,DEEP LEARNING FAST MRI USING CHANNEL ATTENTION IN MAGNITUDE DOMAIN,,,917-920,"Lee Joonhyung,Kim Hyunjong,Chung HyungJin,Ye Jong Chul","Lee J,Kim H,Chung H,Ye JC",Lee J,,Korea Advanced Institute of Science & Technology (KAIST),"Magnetic resonance imaging (MRI) acquisition is an inherently slow process whose acceleration has been the subject of much investigation. In recent years, the explosive advance of deep learning techniques for computer vision and image reconstruction has led to the investigation of deep neural networks for the reconstruction of MRI with under-sampled k-space. In this work, we propose a new image domain architecture that directly produces a sum-of-squares image from under-sampled multi-coil MRI acquisition. This model, called BarbellNet, is a fully convolutional neural network architecture that utilizes the channel attention mechanism using the residual channel attention block (RCAB). Through extensive experiments with the fastMRI data set, we confirm the efficacy of BarbellNet.","BarbellNet,Deep Learning,fastMRI,MRI Reconstruction,Fully Convolutional Network,Channel Attention",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
50,SELF-SUPERVISED PHYSICS-BASED DEEP LEARNING MRI RECONSTRUCTION WITHOUT FULLY-SAMPLED DATA,,,921-925,"Yaman Burhaneddin,Hosseini Seyed Amir Hossein,Moeller Steen,Ellermannt Jutta,Ukurbilt Kamil,Akeakaye Mehmet","Yaman B,Hosseini SAH,Moeller S,Ellermannt J,Ukurbilt K,Akeakaye M",Yaman B,,University of Minnesota System,"Deep learning (DL) has emerged as a tool for improving accelerated MRI reconstruction. A common strategy among DL methods is the physics-based approach, where a regularized iterative algorithm alternating between data consistency and a regularizer is unrolled for a finite number of iterations. This unrolled network is then trained end-to-end in a supervised manner, using fully-sampled data as ground truth for the network output. However, in a number of scenarios, it is difficult to obtain fully-sampled datasets, due to physiological constraints such as organ motion or physical constraints such as signal decay. In this work, we tackle this issue and propose a self-supervised learning strategy that enables physics-based DL reconstruction without fully-sampled data. Our approach is to divide the acquired sub-sampled points for each scan into two sets, one of which is used to enforce data consistency in the unrolled network and the other to define the loss for training. Results show that the proposed self-supervised learning method successfully reconstructs images without fully-sampled data, performing similarly to the supervised approach that is trained with fully-sampled references. This has implications for physics-based inverse problem approaches for other settings, where fully-sampled data is not available or possible to acquire.","Self-supervised learning,accelerated imaging,parallel imaging,compressed sensing,deep learning,neural networks,supervised learning",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,http://arxiv.org/pdf/1910.09116,
51,A 3D CNN WITH A LEARNABLE ADAPTIVE SHAPE PRIOR FOR ACCURATE SEGMENTATION OF BLADDER WALL USING MR IMAGES,,,935-938,"Hammouda K.,Khalifa F.,Soliman A.,Abdeltawab H.,Ghazal M.,Abou El-Ghar M.,Haddad A.,Darwish H. E.,Keynton R.,El-Baz A.","Hammouda K,Khalifa F,Soliman A,Abdeltawab H,Ghazal M,Abou El-Ghar M,Haddad A,Darwish HE,Keynton R,El-Baz A",Hammouda K,,University of Louisville,"A 3D deep learning-based convolution neural network (CNN) is developed for accurate segmentation of pathological bladder (both wall border and pathology) using T2-weighted magnetic resonance imaging (T2W-MRI). Our system starts with a preprocessing step for data normalization to a unique space and extraction of a region-of-interest (ROI). The major stage utilizes a 3D CNN for pathological bladder segmentation, which contains a network, called CNN1, that aims to segment the bladder wall (BW) with pathology. However, due to the similar visual appearance of BW and pathology, the CNN1 can not separate them. Thus, we developed another network (CNN2) with an additional pathway to extract BW only. The second pathway in CNN2 is fed with a 3D learnable adaptive shape prior model. To remove noisy and scattered predictions, the networks' soft outputs are refined using a fully connected conditional random field. Our framework achieved accurate segmentation results for the BW and tumor as documented by the Dice similarity coefficient and Hausdorff distance. Moreover, comparative results against the other segmentation approach documented the superiority of our framework to provide accurate results for pathological BW segmentation.","Bladder Cancer,3D Segmentation,CNN,T2W-MRI,Adaptive Shape Prior",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
52,MASK MINING FOR IMPROVED LIVER LESION SEGMENTATION,,,943-947,"Roth Karsten,Hesser Juergen,Konopczynski Tomasz","Roth K,Hesser J,Konopczynski T",Roth K,,Ruprecht Karls University Heidelberg,"We propose a novel procedure to improve liver and lesion segmentation from CT scans for U-Net based models. Our method extends standard segmentation pipelines to focus on higher target recall or reduction of noisy false-positive predictions, boosting overall segmentation performance. To achieve this, we include segmentation errors into a new learning process appended to the main training setup, allowing the model to find features which explain away previous errors. We evaluate this on semantically distinct architectures: cascaded two- and three-dimensional as well as combined learning setups for multitask segmentation. Liver and lesion segmentation data are provided by the Liver Tumor Segmentation challenge (LiTS), with an increase in dice score of up to 2 points.","U-Net,Liver Lesion Segmentation,Medical Imaging,Data Mining",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,http://arxiv.org/pdf/1908.05062,
53,J REGULARIZATION IMPROVES IMBALANCED MULTICLASS SEGMENTATION,,,948-952,"Guerrero Pena Fidel A.,Fernandez Pedro D. Marrero,Tarr Paul T.,Ren Tsang Ing,Meyerowitz Elliot M.,Cunha Alexandre","Pena FAG,Fernandez PDM,Tarr PT,Ren TI,Meyerowitz EM,Cunha A",Pena FAG,,Universidade Federal de Pernambuco,"We propose a new loss formulation to further advance the multiclass segmentation of cluttered cells under weakly supervised conditions. When adding a Youden's J statistic regularization term to the cross entropy loss we improve the separation of touching and immediate cells, obtaining sharp segmentation boundaries with high adequacy. This regularization intrinsically supports class imbalance thus eliminating the necessity of explicitly using weights to balance training. Simulations demonstrate this capability and show how the regularization leads to correct results by helping advancing the optimization when cross entropy stagnates. We build upon our previous work on multiclass segmentation by adding yet another training class representing gaps between adjacent cells. This addition helps the classifier identify narrow gaps as background and no longer as touching regions. We present results of our methods for 2D and 3D images, from bright field images to confocal stacks containing different types of cells, and we show that they accurately segment individual cells after training with a limited number of images, some of which are poorly annotated.","Loss modeling,deep learning,instance segmentation,multiclass segmentation,cell segmentation,data imbalance",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,http://arxiv.org/pdf/1910.09783,
54,EVALUATING MULTI-CLASS SEGMENTATION ERRORS WITH ANATOMICAL PRIORS,,,953-956,"Wang Xiaogian,Zhang Qianvi,Zhou Zhen,Liu Feng,Yu Yizhou,Wang Yizhou,Gao Wen","Wang XG,Zhang Q,Zhou Z,Liu F,Yu YZ,Wang YZ,Gao W",Wang XG,,Peking University,"Acquiring large scale annotations is challenging in medical image analysis because of the limited number of qualified annotators. Thus, it is essential to achieve high performance using a small number of labeled data, where the key lies in mining the most informative samples to annotate. In this paper, we propose two effective metrics which leverage anatomical priors to evaluate multi-class segmentation methods without ground truth (GT). Together with our smooth margin loss, these metrics can help to mine the most informative samples for training. In experiments, first we demonstrate the proposed metrics can clearly distinguish samples with different degree of errors in the task of pulmonary lobe segmentation. And then we show that our metrics synergized with the proposed loss function can reach the Pearson Correlation Coefficient (PCC) of 0.7447 with mean surface distance (MSD) and -0.5976 with Dice score, which implies the proposed metrics can be used to evaluate segmentation methods. Finally, we utilize our metrics as sample selection criteria in an active learning setting, which shows that the model trained with our anatomy based query achieves comparable performance with the one trained with random query and uncertainty based query using more annotated training data.","Segmentation evaluation,anatomical information,smooth margin loss,active learning",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
55,NUCLEI SEGMENTATION USING MIXED POINTS AND MASKS SELECTED FROM UNCERTAINTY,,,973-976,"Qu Hui,Yi Fingru,Huang Qiaoying,Wu Pengxiang,Metaxas Dimitris","Qu H,Yi R,Huang QY,Wu PX,Metaxas D",Qu H,,Rutgers State University New Brunswick,"Weakly supervised learning has drawn much attention to mitigate the manual effort of annotating pixel-level labels for segmentation tasks. In nuclei segmentation, point annotation has been successfully used for training. However, points lack the shape information. Thus the segmentation of nuclei with non-uniform color is unsatisfactory. In this paper, we propose a framework of weakly supervised nuclei segmentation using mixed points and masks annotation. To save the extra annotation effort, we select typical nuclei to annotate masks from uncertainty map. Using Bayesian deep learning tools, we first train a model with points annotation to predict the uncertainty. Then we utilize the uncertainty map to select the representative hard nuclei for mask annotation automatically. The selected nuclear masks are combined with points to train a better segmentation model. Experimental results on two nuclei segmentation datasets prove the effectiveness of our method. The code is publicly available(1).","Nuclei segmentation,weakly supervised learning,uncertainty,deep learning",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
56,VOLUMETRIC LANDMARK DETECTION WITH A MULTI-SCALE SHIFT EQUIVARIANT NEURAL NETWORK,,,981-985,"Ma Tianyu,Gupta Ajay,Sabuncu Mert R.","Ma TY,Gupta A,Sabuncu MR",Ma TY,,Cornell University,"Deep neural networks yield promising results in a wide range of computer vision applications, including landmark detection. A major challenge for accurate anatomical landmark detection in volumetric images such as clinical CT scans is that large-scale data often constrain the capacity of the employed neural network architecture due to GPU memory limitations, which in turn can limit the precision of the output. We propose a multi-scale, end-to-end deep learning method that achieves fast and memory-efficient landmark detection in 3D images. Our architecture consists of blocks of shift-equivariant networks, each of which performs landmark detection at a different spatial scale. These blocks are connected from coarse to fine-scale, with differentiable resampling layers, so that all levels can be trained together. We also present a noise injection strategy that increases the robustness of the model and allows us to quantify uncertainty at test time. We evaluate our method for carotid artery bifurcations detection on 263 CT volumes and achieve a better than state-of-the-art accuracy with mean Euclidean distance error of 2.81mm.","Convolutional Neural Networks,3D landmark detection",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,http://arxiv.org/pdf/2003.01639,
57,REINFORCEMENT TRACTOGRAPHY: A HYBRID APPROACH FOR ROBUST SEGMENTATION OF COMPLEX FIBER BUNDLES,,,999-1003,"Cabeen Ryan P.,Toga Arthur W.","Cabeen RP,Toga AW",Cabeen RP,,University of Southern California,"We develop and evaluate a novel hybrid tractography algorithm for improved segmentation of complex fiber bundles from diffusion magnetic resonance imaging datasets. We propose an approach inspired by reinforcement learning that combines the strengths of both probabilistic and deterministic tractography to better resolve pathways dominated by crossing fibers. Given a fiber bundle query, our approach first explores an array of possible pathways probabilistically, and then exploits this information with streamline tractography using globally optimal fiber compartment assignment in a conditional random field. We quantitatively evaluated our approach in comparison with deterministic and probabilistic approaches using a realistic phantom with Tractometer and 88 test-retest scans from the Human Connectome Project. We found that the proposed hybrid method offers improved accuracy with phantom data and more biologically plausible topographic organization and higher reliability with in vivo data. This demonstrates the benefits of combining tractography approaches and indicates opportunities for integrating reinforcement learning strategies into tractograpy algorithms.","tractography,diffusion MRI,acoustic radiations,reinforcement learning,machine learning",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,"DIFFUSION,VALIDATION",,,
58,DC-WCNN: A DEEP CASCADE OF WAVELET BASED CONVOLUTIONAL NEURAL NETWORKS FOR MR IMAGE RECONSTRUCTION,,,1069-1073,"Ramanarayanan Sriprabha,Murugesan Balainurali,Ram Keerthi,Sivaprakasam Mohanasankar","Ramanarayanan S,Murugesan B,Ram K,Sivaprakasam M",Ramanarayanan S,,Indian Institute of Technology System (IIT System),"Several variants of Convolutional Neural Networks (CNN) have been developed for Magnetic Resonance (MR) image reconstruction. Among them, U-Net has shown to be the baseline architecture for MR image reconstruction. However, sub-sampling is performed by its pooling layers, causing information loss which in turn leads to blur and missing fine details in the reconstructed image. We propose a modification to the U-Net architecture to recover fine structures. The proposed network is a wavelet packet transform based encoder-decoder CNN with residual learning called WCNN. The proposed WCNN has discrete wavelet transform instead of pooling and inverse wavelet transform instead of unpooling layers and residual connections. We also propose a deep cascaded framework (DC-WCNN) which consists of cascades of WCNN and k-space data fidelity units to achieve high quality MR reconstruction. Experimental results show that WCNN and DC-WCNN give promising results in terms of evaluation metrics and better recovery of fine details as compared to other methods.","MR image reconstruction,fine details,Wavelet transform,U-Net,pooling,Deep cascade",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,http://arxiv.org/pdf/2001.02397,
59,FAST AUTOMATIC PARAMETER SELECTION FOR MRI RECONSTRUCTION,,,1078-1081,"Toma Tanjin Taher,Weller Daniel S.","Toma TT,Weller DS",Toma TT,,University of Virginia,"This paper proposes an automatic parameter selection framework for optimizing the performance of parameter-dependent regularized reconstruction algorithms. The proposed approach exploits a convolutional neural network for direct estimation of the regularization parameters from the acquired imaging data. This method can provide very reliable parameter estimates in a computationally efficient way. The effectiveness of the proposed approach is verified on transform-learning-based magnetic resonance image reconstructions of two different publicly available datasets. This experiment qualitatively and quantitatively measures improvement in image reconstruction quality using the proposed parameter selection strategy versus both existing parameter selection solutions and a fully deep-learning reconstruction with limited training data. Based on the experimental results, the proposed method improves average reconstructed image peak signal-to-noise ratio by a dB or more versus all competing methods in both brain and knee datasets, over a range of subsampling factors and input noise levels.","parameter selection,image reconstruction,magnetic resonance imaging,convolutional neural network",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
60,BI-MODAL ULTRASOUND BREAST CANCER DIAGNOSIS VIA MULTI-VIEW DEEP NEURAL NETWORK SVM,,,1106-1110,"Gong Bangming,Shen Lu,Chang Cai,Zhou Shichong,Zhou Weijun,Li Shuo,Shi Jun","Gong BM,Shen L,Chang C,Zhou SC,Zhou WJ,Li S,Shi J",Shi J,,Shanghai University,"B-mode ultrasound and ultrasound elastography are two routine diagnostic modalities for breast cancer. Unfortunately, few efforts have paid attention to learn bi-modal ultrasound jointly. By combining multi-view deep mapping-based feature representation with SVM-based classification, we proposed a novel integrated deep learning model, multi-view deep neural network support vector machine (MDNNSVM), to achieve breast cancer diagnosis on bi-modal ultrasound. In particular, multi-view representation learning extracts and fuses the various ultrasound characteristics (also including hardness information of soft tissue) effectively to differentiate benign breast lesions from malignant. Further, the SVM-based objective function is used to learn a classifier jointly with DNN to improve diagnostic accuracy significantly. The experimental results on a real-world dataset of breast cancer verify the effectiveness of the MDNNSVM with the best value of classification accuracy (86.36%) and AUC (0.9079).","B-Mode Ultrasound,Ultrasound Elastography,Multi-View Learning,Deep Neural Network,SVM,Breast Cancer Diagnosis",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
61,Fully Automatic Computer-aided Mass Detection and Segmentation via Pseudo-color Mammograms and Mask R-CNN,,,1111-1115,"Min Hang,Wilson Devin,Huang Yinhuang,Liu Siyu,Crozier Stuart,Bradley Andrew P.,Chandra Shekhar S.","Min H,Wilson D,Huang YH,Liu SY,Crozier S,Bradley AP,Chandra SS",Min H,,University of Queensland,"Mammographic mass detection and segmentation are usually performed as serial and separate tasks, with segmentation often only performed on manually confirmed true positive detections in previous studies. We propose a fully-integrated computer-aided detection (CAD) system for simultaneous mammographic mass detection and segmentation without user intervention. The proposed CAD only consists of a pseudo-color image generation and a mass detection-segmentation stage based on Mask R-CNN. Grayscale mammograms are transformed into pseudo-color images based on multi-scale morphological sifting where mass-like patterns are enhanced to improve the performance of Mask R-CNN. Transfer learning with the Mask R-CNN is then adopted to simultaneously detect and segment masses on the pseudo-color images. Evaluated on the public dataset INbreast, the method outperforms the state-of-the-art methods by achieving an average true positive rate of 0.90 at 0.9 false positive per image and an average Dice similarity index of 0.88 for mass segmentation.","Mammography,computer-aided detection,breast mass,morphological sifting,Mask R-CNN",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,http://arxiv.org/pdf/1906.12118,
62,READING MAMMOGRAPHY WITH MULTIPLE PRIOR EXAMS,,,1116-1119,"Song Chao,de Cea Maria V. Sainz,Richmond David","Song C,de Cea MVS,Richmond D",Song C,,"IBM Watson Hlth, Armonk, NY 10504 USA.","Change is one of the strongest features for identifying abnormality in screening mammography exams. However, publicly available mammography datasets have so far lacked prior exams, and so the majority of algorithm development has focused on assessing isolated exams, without prior information. Recently, it was shown that a deep learning algorithm can improve its diagnostic accuracy by utilizing a single prior mammography image. In this work, we extend the previous result to address the issue of reading a variable number of prior exams. We compare two approaches: Random Forest, which requires that all inputs have the same size, and LSTM, which can handle a variable-size input. We demonstrate a significant performance improvement when using multiple priors, consistent with the standard workflow of breast imagers. We also found for both models that multiple priors improved performance over using a single prior. Interestingly, LSTM consistently outperformed the Random Forest model, and is more practical because it can naturally process any number of prior images that are available at the time of read. We expect that these results will generalize to other screening programs, such as colorectal cancer, where prior images are readily available.","CNN,Random Forest,LSTM,priors",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,BREAST-CANCER,,,
63,ANALYSIS OF THE INFLUENCE OF DIFFEOMORPHIC NORMALIZATION IN THE PREDICTION OF STABLE VS PROGRESSIVE MCI CONVERSION WITH CONVOLUTIONAL NEURAL NETWORKS,,,1120-1124,"Ramon-Julvez Ubaldo,Hernandez Monica,Mayordomo Elvira","Ramon-Julvez U,Hernandez M,Mayordomo E",Ramon-Julvez U,,University of Zaragoza,"We study the effect of the selection of diffeomorphic normalization in the performance of Spasov's deep-learning system for the problem of progressive MCI vs stable MCI discrimination. We considered different degrees of normalization (no, affine and non-rigid normalization) and two diffeomorphic registration methods (ANTS and BL PDE-LDDMM) with different image similarity metrics (SSD, NCC, and lNCC) yielding qualitatively different deformation models and quantitatively different degrees of registration accuracy. BL PDE-LDDMM NCC achieved the best performing accuracy with median values of 89%. Surprisingly, the accuracy of no and affine normalization was also among the highest, indicating that the deep-learning system is powerful enough to learn accurate models for pMCI vs sMCI discrimination without the need for normalization. However, the best sensitivity values were obtained by BL PDE-LDDMM SSD and NCC with median values of 97% and 94% while the sensitivity of the remaining methods stayed under 88%.","CNNs,multi-task learning,diffeomorphic normalization,Alzheimer's,pMCI vs sMCI",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,"MILD,COGNITIVE,IMPAIRMENT,TENSOR-BASED,MORPHOMETRY,ALZHEIMERS,CONVERSION,MRI,BIOMARKER,DISEASE",,,
64,LOOKING IN THE RIGHT PLACE FOR ANOMALIES: EXPLAINABLE AI THROUGH AUTOMATIC LOCATION LEARNING,,,1125-1129,"Kashyap Satyananda,Karargyris Alexandros,Wu Joy,Gur Yaniv,Wong Arjun Sharma Ken C. L.,Moradi Mehdi,Syeda-Mahmood Tanveer","Kashyap S,Karargyris A,Wu J,Gur Y,Wong ASKCL,Moradi M,Syeda-Mahmood T",Kashyap S,,International Business Machines (IBM),"Deep learning has now become the de facto approach to the recognition of anomalies in medical imaging. Their 'black box' way of classifying medical images into anomaly labels poses problems for their acceptance, particularly with clinicians. Current explainable AI methods offer justifications through visualizations such as heat maps but cannot guarantee that the network is focusing on the relevant image region fully containing the anomaly. In this paper we develop an approach to explainable AI in which the anomaly is assured to be overlapping the expected location when present. This is made possible by automatically extracting location-specific labels from textual reports and learning the association of expected locations to labels using a hybrid combination of Bi-Directional Long Short-Term Memory Recurrent Neural Networks (Bi-LSTM) and DenseNet-121. Use of this expected location to bias the subsequent attention-guided inference network based on ResNet101 results in the isolation of the anomaly at the expected location when present. The method is evaluated on a large chest X-ray dataset.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,http://arxiv.org/pdf/2008.00363,
65,DEEP LEARNING BASED SEGMENTATION OF BODY PARTS IN CT LOCALIZERS AND APPLICATION TO SCAN PLANNING,,,1130-1133,"Deshpande Hrishikesh,Bergtholdt Martin,Gotman Shlomo,Saalbach Axel,Senegas Julien","Deshpande H,Bergtholdt M,Gotman S,Saalbach A,Senegas J",Deshpande H,,Philips,"In this paper, we propose a deep learning approach for the segmentation of body parts in computer tomography (CT) localizer images. Such images pose difficulties in the automatic image analysis on account of variable field-of-view, diverse patient positioning and image acquisition at low dose, but are of importance pertaining to their most prominent applications in scan planning and dose modulation. Following the success of deep learning technology in image segmentation applications, we investigate the use of a fully convolutional neural network architecture to achieve the segmentation of four anatomies: abdomen, chest, pelvis and brain. The method is further extended to generate plan boxes for individual as well as multiple combined anatomies, and compared against the existing techniques. The performance of the method is evaluated on 771 multi-site localizer images.","CT,deep learning,image segmentation,fully-convolutional networks,localizers",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
66,SUPERVISED AUGMENTATION: LEVERAGE STRONG ANNOTATION FOR LIMITED DATA,,,1134-1138,"Zheng Han,Shang Hong,Sun Zhongqian,Fu Xinghui,Yao Jianhua,Huang Junzhou","Zheng H,Shang H,Sun ZQ,Fu XH,Yao JH,Huang JZ",Zheng H,,Tencent,"A previously less exploited dimension to approach the data scarcity challenge in medical imaging classification is to leverage strong annotation, when available data is limited but the annotation resource is plentiful. Strong annotation at finer level, such as region of interest, carries more information than simple image level annotation, therefore should theoretically improve performance of a classifier. In this work, we explored utilizing strong annotation by developing a new data augmentation method, which improved over common data augmentation (random crop and cutout) by significantly enriching augmentation variety and ensuring valid label given guidance from strong annotation. Experiments on a real world application of classifying gastroscopic images demonstrated that our method outperformed state-of-the-art methods by a large margin at all different settings of data scarcity. Additionally, our method is flexible to integrate with other CNN improvement techniques and handle data with mixed annotation.","Computer aided diagnosis,Classification,Strong supervision,Data augmentation",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
67,EXPLOITING ,,,1139-1143,"Ponzio Francesco,Deodato Giacomo,Macii Enrico,Cataldo Santa Di,Ficarra Elisa","Ponzio F,Deodato G,Macii E,Cataldo S,Ficarra E",Ponzio F,,Polytechnic University of Turin,"With the advent of digital pathology, there has been an increasing interest in providing pathologists with machine learning tools, often based on deep learning, to obtain faster and more robust image assessment. Nonetheless, the accuracy of these tools relies on the generation of large training sets of pre-labeled images. This is typically a challenging and cumbersome process, requiring extensive pre-processing to remove spurious samples that may lead the training to failure. Unlike their plain counterparts, which tend to provide overconfident decisions and cannot identify samples they have not been specifically trained for, Bayesian Convolutional Neural Networks provide a reliable measure of classification uncertainty. In this study, we exploit this inherent capability to automatize the data cleaning phase of histopathological image assessment. Our experiments on a case study of Colorectal Cancer image classification demonstrate that our approach can boost the accuracy of downstream classification by 15% at least.","Data Cleaning,Deep Learning,Bayesian Neural Networks,Digital Pathology,Histopathological Image Classification",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
68,DRU-NET: AN EFFICIENT DEEP CONVOLUTIONAL NEURAL NETWORK FOR MEDICAL IMAGE SEGMENTATION,,,1144-1148,"Jafari Mina,Auer Dorothee,Francis Susan,Garibaldi Jonathan,Chen Xin","Jafari M,Auer D,Francis S,Garibaldi J,Chen X",Jafari M,,University of Nottingham,"Residual network (ResNet) and densely connected network (DenseNet) have significantly improved the training efficiency and performance of deep convolutional neural networks (DCNNs) mainly for object classification tasks. In this paper, we propose an efficient network architecture by considering advantages of both networks. The proposed method is integrated into an encoder-decoder DCNN model for medical image segmentation. Our method adds additional skip connections compared to ResNet but uses significantly fewer model parameters than DenseNet. We evaluate the proposed method on a public dataset (ISIC 2018 grand-challenge) for skin lesion segmentation and a local brain MRI dataset. In comparison with ResNet-based, DenseNet-based and attention network (AttnNet) based methods within the same encoder-decoder network structure, our method achieves significantly higher segmentation accuracy with fewer number of model parameters than DenseNet and AttnNet. The code is available on GitHub (GitHub link: https://github.com/MinaJf/DRU-net).","Convolutional Neural Network,Medical Image Segmentation,U-net,Dense U-net,Residual U-net",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,http://arxiv.org/pdf/2004.13453,
69,"OBJECT SEGMENTATION WITH DEEP NEURAL NETS COUPLED WITH A SHAPE PRIOR, WHEN LEARNING FROM A TRAINING SET OF LIMITED QUALITY AND SMALL SIZE",,,1149-1153,"Shigwan Saurabh J.,Gaikwad Akshay V,Awate Suyash P.","Shigwan SJ,Gaikwad AV,Awate SP",Shigwan SJ,,Indian Institute of Technology System (IIT System),"Statistical shape priors can be crucial in segmenting objects when the data differentiates poorly between the object and its surroundings. For reliable learning, while some methods need high-quality expert segmentations, other methods need large training sets, both of which can often be difficult to obtain in clinical deployment or scientific studies. We propose to couple deep neural networks with a pointset-based shape prior that can be learned effectively despite training sets having small size and imperfections in expert curation. The prior relies on sparse Riemannian modeling in Kendall shape space. Results on clinical brain magnetic resonance imaging data show that our framework improves over the state of the art in segmenting the thalamus and the caudate.","Shape prior,DNN,small-sized training set,limited-quality training set,sparsity,Kendall shape space",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,"NETWORKS,MODELS",,,
70,ROBUST DETECTION OF ADVERSARIAL ATTACKS ON MEDICAL IMAGES,,,1154-1158,"Li Xin,Zhu Dongxiao","Li X,Zhu DX",Li X,,Wayne State University,"Although deep learning systems trained on medical images have shown state-of-the-art performance in many clinical prediction tasks, recent studies demonstrate that these systems can be fooled by carefully crafted adversarial images. It has raised concerns on the practical deployment of deep learning based medical image classification systems. To tackle this problem, we propose an unsupervised learning approach to detect adversarial attacks on medical images. Our approach is capable of detecting a wide range of adversarial attacks without knowing the attackers nor sacrificing the classification performance. More importantly, our approach can be easily embedded into any deep learning-based medical imaging system as a module to improve the system's robustness. Experiments on a public chest X-ray dataset demonstrate the strong performance of our approach in defending adversarial attacks under both white-box and black-box settings.","Adversarial attacks,Medical images,Deep learning,Lung disease classification",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
71,SELF-SUPERVISION VS. TRANSFER LEARNING: ROBUST BIOMEDICAL IMAGE ANALYSIS AGAINST ADVERSARIAL ATTACKS,,,1159-1163,"Anand Deepak,Tank Darshan,Tibrewal Harshvardhan,Sethi Amit","Anand D,Tank D,Tibrewal H,Sethi A",Anand D,,Indian Institute of Technology System (IIT System),"Deep neural networks are being increasingly used for disease diagnosis and lesion localization on biomedical images. However, training deep neural networks not only requires large sets of expensive ground truth (image labels or pixel annotations); they are also susceptible to adversarial attacks. Transfer learning alleviates the former problem to some extent, wherein the lower layers of a neural network are pre-trained on a large labeled dataset from a different domain (e.g., ImageNet), while only the upper layers are fine-tuned on the target domain (e.g., chest X-rays). An alternative to transfer learning is self-supervised learning, in which a supervised task is created using the unlabeled images from the target domain itself to pre-train the lower layers. In this work, we show that self-supervised learning combined with adversarial training offers additional advantages over transfer learning as well as vanilla self-supervised learning. In particular, the process of adversarial training itself acts as data augmentation for self-supervision. This adversarial data augmentation leads to both a reduction in the amount of supervised data required for comparable accuracy, as well as natural robustness to adversarial attacks. We support our claims using experiments on the two modalities and tasks - classification of chest X-rays, and segmentation of MRI images.","Self-supervision,transfer learning,l(infinity) adversarial attack,radiology,deep learning,annotation cost",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
72,COMBINING SHAPE PRIORS WITH CONDITIONAL ADVERSARIAL NETWORKS FOR IMPROVED SCAPULA SEGMENTATION IN MR IMAGES,,,1164-1167,"Boutillon A.,Borotikar B.,Burdin V,Conze P. H.","Boutillon A,Borotikar B,Burdin V,Conze PH",Boutillon A,,IMT - Institut Mines-Telecom,"This paper proposes an automatic method for scapula bone segmentation from Magnetic Resonance (MR) images using deep learning. The purpose of this work is to incorporate anatomical priors into a conditional adversarial framework, given a limited amount of heterogeneous annotated images. Our approach encourages the segmentation model to follow the global anatomical properties of the underlying anatomy through a learnt non-linear shape representation while the adversarial contribution refines the model by promoting realistic delineations. These contributions are evaluated on a dataset of 15 pediatric shoulder examinations, and compared to state-of-the-art architectures including UNet and recent derivatives. The significant improvements achieved bring new perspectives for the pre-operative management of musculo-skeletal diseases.","semantic segmentation,convolutional encoder-decoders,conditional adversarial networks,shape priors,shoulder bones",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,http://arxiv.org/pdf/1910.08963,
73,A GENERIC ENSEMBLE BASED DEEP CONVOLUTIONAL NEURAL NETWORK FOR SEMI-SUPERVISED MEDICAL IMAGE SEGMENTATION,,,1168-1172,"Li Ruizhe,Auer Dorothee,Wagner Christian,Chen Xin","Li RZ,Auer D,Wagner C,Chen X",Li RZ,,University of Nottingham,"Deep learning based image segmentation has achieved the state-of-the-art performance in many medical applications such as lesion quantification, organ detection, etc. However, most of the methods rely on supervised learning, which require a large set of high-quality labeled data. Data annotation is generally an extremely time-consuming process. To address this problem, we propose a generic semi-supervised learning framework for image segmentation based on a deep convolutional neural network (DCNN). An encoder-decoder based DCNN is initially trained using a few annotated training samples. This initially trained model is then copied into sub-models and improved iteratively using random subsets of unlabeled data with pseudo labels generated from models trained in the previous iteration. The number of sub-models is gradually decreased to one in the final iteration. We evaluate the proposed method on a public grand-challenge dataset for skin lesion segmentation. Our method is able to significantly improve beyond fully supervised model learning by incorporating unlabeled data. The code is available on Github(1).","Medical Image Segmentation,Semi-supervised Learning",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,http://arxiv.org/pdf/2004.07995,
74,ROBUST AUTOMATIC MULTIPLE LANDMARK DETECTION,,,1178-1182,"Jain Arjit,Powers Alexander,Johnson Hans J.","Jain A,Powers A,Johnson HJ",Jain A,,Indian Institute of Technology System (IIT System),"Reinforcement learning (RL) has proven to be a powerful tool for automatic single landmark detection in 3D medical images. In this work, we extend RL-based single landmark detection to detect multiple landmarks simultaneously in the presence of missing data in the form of defaced 3D head MR images. Our purposed technique is both time-efficient and robust to missing data. We demonstrate that adding auxiliary landmarks can improve the accuracy and robustness of estimating primary target landmark locations. The multi-agent deep Q-network (DQN) approach described here detects landmarks within 2mm, even in the presence of missing data.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
75,NEURAL NETWORK SEGMENTATION OF CELL ULTRASTRUCTURE USING INCOMPLETE ANNOTATION,,,1183-1187,"Francis John Paul,Wang Hongzhi,White Kate,Syeda-Mahmood Tanveer,Stevens Raymond","Francis JP,Wang HZ,White K,Syeda-Mahmood T,Stevens R",Francis JP,,University of Southern California,"The Pancreatic beta cell is an important target in diabetes research. For scalable modeling of beta cell ultastructure, we investigate automatic segmentation of whole cell imaging data acquired through soft X-ray tomography. During the course of the study, both complete and partial ultrastrucutre annotations were produced manually for different subsets of the data. To more effectively use existing annotations, we propose a method that enables the application of partially labeled data for full label segmentation. For experimental validation, we apply our method to train a convolutional neural network with a set of 12 fully annotated data and 12 partially annotated data and show promising improvement over standard training that uses fully annotated data alone.","Pancreatic beta cell,soft x-ray tomography,convolutional neural networks,semantic segmentation",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,"INSULIN-GRANULE,DYNAMICS",,http://arxiv.org/pdf/2004.09673,
76,LIVER GUIDED PANCREAS SEGMENTATION,,,1201-1204,"Zhang Yue,Wu Jiong,Wang Simao,Liu Yilong,Chen Yifan,Wu Ed X.,Tang Xiaoying","Zhang Y,Wu J,Wang SM,Liu YL,Chen YF,Wu EX,Tang XY",Tang XY,,Southern University of Science & Technology,"In this paper, we propose and validate a location prior guided automatic pancreas segmentation framework based on 3D convolutional neural network (CNN). To guide pancreas segmentation, centroid of the pancreas used to determine its bounding box is calculated using the location of the liver which is firstly segmented by a 2D CNN. A linear relationship between centroids of the pancreas and the liver is proposed. After that, a 3D CNN is employed the input of which is the bounding box of the pancreas to get the final segmentation. A publicly accessible pancreas dataset including 54 subjects is used to quantify the performance of the proposed framework. Experimental results reveal outstanding performance of the proposed method in terms of both computational efficiency and segmentation accuracy compared to non location guided segmentation. To be specific, the running time is 15 times faster and the segmentation accuracy in terms of Dice is higher by 4.29% (76.42% versus 80.71%).","Pancreas segmentation,location prior,convolutional neural network,CT,coarse-to-fine",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
77,INCEPTION CAPSULE NETWORK FOR RETINAL BLOOD VESSEL SEGMENTATION AND CENTERLINE EXTRACTION,,,1223-1226,"Kromm C.,Rohr K.","Kromm C,Rohr K",Kromm C,,Ruprecht Karls University Heidelberg,"Automatic segmentation and centerline extraction of retinal blood vessels from fundus image data is crucial for early detection of retinal diseases. We have developed a novel deep learning method for segmentation and centerline extraction of retinal blood vessels which is based on the Capsule network in combination with the Inception architecture. Compared to state-of-the-art deep convolutional neural networks, our method has much fewer parameters due to its shallow architecture and generalizes well without using data augmentation. We performed a quantitative evaluation using the DRIVE dataset for both vessel segmentation and centerline extraction. Our method achieved state-of-the-art performance for vessel segmentation and outperformed existing methods for centerline extraction.","Deep Learning,Fully Convolutional Neural Network,Capsule Network,Retinal Fundus Image Data,Vessel Segmentation,Centerline Extraction",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,https://doi.org/10.1101/815555,
78,SPARSE-GAN: SPARSITY-CONSTRAINED GENERATIVE ADVERSARIAL NETWORK FOR ANOMALY DETECTION IN RETINAL OCT IMAGE,,,1227-1231,"Zhou Kang,Gao Shenghua,Cheng Jun,Gu Zaiwang,Fu Huazhu,Tu Zhi,Yang Jianlong,Zhao Yitian,Liu Jiang","Zhou K,Gao SH,Cheng J,Gu ZW,Fu HZ,Tu Z,Yang JL,Zhao YT,Liu J",Gao SH,,ShanghaiTech University,"With the development of convolutional neural network, deep learning has shown its success for retinal disease detection from optical coherence tomography (OCT) images. However, deep learning often relies on large scale labelled data for training, which is oftentimes challenging especially for disease with low occurrence. Moreover, a deep learning system trained from data-set with one or a few diseases is unable to detect other unseen diseases, which limits the practical usage of the system in disease screening. To address the limitation, we propose a novel anomaly detection framework termed Sparsity-constrained Generative Adversarial Network (Sparse-GAN) for disease screening where only healthy data are available in the training set. The contributions of Sparse-GAN are two-folds: 1) The proposed Sparse-GAN predicts the anomalies in latent space rather than image-level; 2) Sparse-GAN is constrained by a novel Sparsity Regularization Net. Furthermore, in light of the role of lesions for disease screening, we present to leverage on an anomaly activation map to show the heatmap of lesions. We evaluate our proposed Sparse-GAN on a publicly available dataset, and the results show that the proposed method outperforms the state-of-the-art methods.","Anomaly Detection,Sparsity-constrained Network,Latent Feature,Adversarial Learning",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,http://arxiv.org/pdf/1911.12527,
79,LEARNING TO SEGMENT VESSELS FROM POORLY ILLUMINATED FUNDUS IMAGES,,,1232-1236,"Nasery Vibha,Soundararajan Krishna Bairavi,Galeotti John","Nasery V,Soundararajan KB,Galeotti J",Nasery V,,Regeneron,"Segmentation of retinal vessels is important for determining various disease conditions, but deep learning approaches have been limited by the unavailability of large, publicly available, and annotated datasets. The paper addresses this problem and analyses the performance of U-Net architecture on DRIVE and RIM-ONE datasets. A different approach for data augmentation using vignetting masks is presented to create more annotated fundus data. Unlike most prior efforts that attempt transforming poor images to match the images in a training set, our approach takes better quality images (which have good expert labels) and transforms them to resemble poor quality target images. We apply substantial vignetting masks to the DRIVE dataset and then train a U-net on the resulting lower quality images (using the corresponding expert label data). We quantitatively show that our approach leads to better generalized networks, and we show qualitative performance improvements in RIM-ONE images (which lack expert labels).","Retinal Vasculature Segmentation,Deep Learning,U-Net,Vignetting,Fundus Imaging",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
80,CTF-NET:RETINAL VESSEL SEGMENTATION VIA DEEP COARSE-TO-FINE SUPERVISION NETWORK,,,1237-1241,"Wang Kun,Zhang Xiaohong,Huang Sheng,Wang Qiuli,Chen Feiyu","Wang K,Zhang XH,Huang S,Wang QL,Chen FY",Wang K,,Chongqing University,"Retinal blood vessels structure plays an important role in the early diagnosis of diabetic retinopathy, which is a cause of blindness globally. However, the precise segmentation of retinal vessels is often extremely challenging due to the low contrast and noise of the capillaries. In this paper, we propose a novel model of deep coarse-to-fine supervision network (CTF-Net) to solve this problem. This model consists of two U-shaped architecture(coarse and fine segNet). The coarse segNet, which learns to predict probability retina map from input patchs, while the fine segNet refines the predicted map. To gain more paths to preserve the multi-scale and rich deep features information, we design an end-to-end training network instead of multi-stage learning framework to segment the retina vessel from coarse to fine. Furthermore, in order to improve feature representation and reduce the number of parameters of model, we introduce a novel feature augmentation module (FAM-residual block). Experiment results confirm that our method achieves the state-of-the-art performances on the popular datasets DRIVE, CHASE DB1 and STARE.","Retinal vessel segmentation,coarse-tofine segNet,deep learning,computer aided-diagnosis",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,"BLOOD-VESSELS,IMAGES",,,
81,DENSE CORRELATION NETWORK FOR AUTOMATED MULTI-LABEL OCULAR DISEASE DETECTION WITH PAIRED COLOR FUNDUS PHOTOGRAPHS,,,1250-1253,"Li Cheng,Ye Jin,He Junjun,Wang Shanshan,Qiao Yu,Gu Lixu","Li C,Ye J,He JJ,Wang SS,Qiao Y,Gu LX",Qiao Y,,Chinese Academy of Sciences,"In ophthalmology, color fundus photography is an economic and effective tool for early-stage ocular disease screening. Since the left and right eyes are highly correlated, we utilize paired color fundus photographs for our task of automated multi-label ocular disease detection. We propose a Dense Correlation Network (DCNet) to exploit the dense spatial correlations between the paired CFPs. Specifically, DCNet is composed of a backbone Convolutional Neural Network (CNN), a Spatial Correlation Module (SCM), and a classifier. The SCM can capture the dense correlations between the features extracted from the paired CFPs in a pixel-wise manner, and fuse the relevant feature representations. Experiments on a public dataset show that our proposed DCNet can achieve better performance compared to the respective baselines regardless of the backbone CNN architectures.","Ocular disease,color fundus photographs,multi-label classification,correlation",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,"DIABETIC-RETINOPATHY,DEEP,VALIDATION",,,
82,A DATA-AWARE DEEP SUPERVISED METHOD FOR RETINAL VESSEL SEGMENTATION,,,1254-1257,"Mishra Suraj,Chen Danny Z.,Hu X. Sharon","Mishra S,Chen DZ,Hu XS",Mishra S,,University of Notre Dame,"Accurate vessel segmentation in retinal images is vital for retinopathy diagnosis and analysis. However, existence of very thin vessels in low image contrast along with pathological conditions (e.g., capillary dilation or microaneurysms) render the segmentation task difficult. In this work, we present a novel approach for retinal vessel segmentation focusing on improving thin vessel segmentation. We develop a deep convolutional neural network (CNN), which exploits the specific characteristics of the input retinal data to use deep supervision, for improved segmentation accuracy. In particular, we use the average input retinal vessel width and match it with the layer-wise effective receptive fields (LERF) of the CNN to determine the location of the auxiliary supervision. This helps the network to pay more attention to thin vessels, that otherwise the network would 'ignore' during training. We verify our method on three public retinal vessel segmentation datasets (DRIVE, CHASE DB1, and STARE), achieving better sensitivity (10.18% average increase) than state-of-the-art methods while maintaining comparable specificity, accuracy, and AUC.","Retinal Vessel Segmentation,Effective Receptive Field,Convolutional Neural Networks",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
83,COMPACT REPRESENTATION LEARNING USING CLASS SPECIFIC CONVOLUTION CODERS - APPLICATION TO MEDICAL IMAGE CLASSIFICATION,,,1266-1270,"Upadhyay Uddeshya,Banerjee Biplab","Upadhyay U,Banerjee B",Upadhyay U,,Indian Institute of Technology System (IIT System),"Medical image classification using deep learning techniques rely on highly curated datasets, which are difficult and expensive to obtain in real world due significant expertise required to annotate the dataset. We propose a novel framework called Class Specific Convolutional Coders (CSCC) to tackle the problem of learning highly discriminative, compact and non-redundant feature space from a relatively small amount of labelled images. We design separate attention-driven convolution network based feature extractors for the categories. These feature learning modules are further intuitively combined so as to make the whole image recognition system endto-end trainable. Results on different medical image classification tasks show the advantages of our contributions, where our proposed methods outperforms the benchmark supervised deep convolutional networks (CNNs) trained from scratch.","Deep Learning,Class Specific Convolutional Coders,Medical Imaging,Classification,Attention",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
84,AN EFFECTIVE DEEP LEARNING ARCHITECTURE COMBINATION FOR TISSUE MICROARRAY SPOTS CLASSIFICATION OF H&E STAINED COLORECTAL IMAGES,,,1271-1274,"Huu-Giao Nguyen,Blank Annika,Lugli Alessandro,Zlobec Inti","Nguyen HG,Blank A,Lugli A,Zlobec I",Nguyen HG,,University of Bern,"Tissue microarray (TMA) assessment of histomorphological biomarkers contributes to more accurate prediction of outcome of patients with colorectal cancer (CRC), a common disease. Unfortunately, a typical problem with the use of TMAs is that the material contained in each TMA spot changes as the TMA block is cut repeatedly. A reclassification of the content within each spot would be necessary for accurate biomarker evaluation. The major challenges to this end however lie in the high heterogeneity of TMA quality and of tissue characterization in structure, size, appearance and tissue type. In this work, we propose an end-to-end framework using deep learning for TMA spot classification into three classes: tumor, normal epithelium and other tissue types. It includes a detection of TMA spots in an image, an extraction of overlapping tiles from each TMA spot image and a classification integrated into two effective deep learning architectures: convolutional neural network (CNN) and Capsule Network with the prior information of intended tissue type. A set of digitized H&E stained images from 410 CRC patients with clinicopathological information is used for the validation of the proposed method. We show experimentally that our approach brings state-of-the-art performances for several relevant CRC H&E tissue classification and that these results are promising for use in clinical practice.","Colorectal cancer,Tissue microarray,Neural networks,Image classification,Hematoxylin and Eosin (H&E)",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
85,UNSUPERVISED LEARNING OF CONTEXTUAL INFORMATION IN MULTIPLEX IMMUNOFLUORESCENCE TISSUE CYTOMETRY,,,1275-1279,"Jimenez-Sanchez Daniel,Ariz Mikel,Ortiz-de-Solorzano Carlos","Jimenez-Sanchez D,Ariz M,Ortiz-de-Solorzano C",Jimenez-Sanchez D,,University of Navarra,"New machine learning models designed to capture the histopathology of tissues should account not only for the phenotype and morphology of the cells, but also learn complex spatial relationships between them. To achieve this, we represent the tissue as an interconnected graph, where previously segmented cells become nodes of the graph. Then the relationships between cells are learned and embedded into a low-dimensional vector, using a Graph Neural Network. We name this Representation Learning based strategy NARO (NAtural Representation of biological Objects), a fully-unsupervised method that learns how to optimally encode cell phenotypes, morphologies, and cell-to-cell interactions from histological tissues labeled using multiplex immunohistochemistry. To validate NARO, we first use synthetically generated tissues to show that NARO's generated embeddings can be used to cluster cells in meaningful, distinct anatomical regions without prior knowledge of constituent cell types and interactions. Then we test NARO on real multispectral images of human lung adenocarcinoma tissue samples, to show that the generated embeddings can indeed be used to automatically infer regions with different histopathological characteristics.","Representation Learning,Cell Interactions,Multiplex Image Cytometry,Machine Learning",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
86,WEAKLY-SUPERVISED BALANCED ATTENTION NETWORK FOR GASTRIC PATHOLOGY IMAGE LOCALIZATION AND CLASSIFICATION,,,1280-1283,"Zhu Zhonghang,Ding Xin,Zhang Defu,Wang Liansheng","Zhu ZH,Ding X,Zhang DF,Wang LS",Wang LS,,Xiamen University,"Gastric cancer pathological images classification and localization are critical in early diagnosis and therapy of related diseases. Clinically, it takes a long time to scan a pathological image due to its high resolution and blurry boundaries, which leads to requirements for automatic cancer region localization over the pathological image. In this paper, a weakly supervised model is proposed to classify and localize the gastric cancer region in the pathological image with image-level labels. We propose a channel-wise attention (CA) and spatial-wise attention (SA) module to balance the feature (feature balanced module, FBM) and coalesce the dropout attention mechanism (dropout attention module, DAM) into our model to enhance the feature significance. Based on the classification model, we extract the optimal feature map to generate the localization bounding box with a cross attention module. Experiments on a sufficient gastric dataset indicate that our method outperforms other algorithms in classification accuracy and localization accuracy, which demonstrates the effectiveness of our method.","Gastric cancer pathology,Weakly supervised,Localization",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
87,ADVERSARIAL-BASED DOMAIN ADAPTATION NETWORKS FOR UNSUPERVISED TUMOUR DETECTION IN HISTOPATHOLOGY,,,1284-1288,"Figueira Goncalo,Wang Yaqi,Sun Lingling,Zhou Huiyu,Zhang Qianni","Figueira G,Wang YQ,Sun LL,Zhou HY,Zhang QN",Zhang QN,,University of London,"Developing effective deep learning models for histopathology applications is challenging, as the performance depends on large amounts of labelled training data, which is often unavailable. In this work, we address this issue by leveraging previously annotated histopathology images from unrelated source domains to build a model for the unlabelled target domain. Specifically, we propose the adversarial-based domain adaptation networks (ABDA-Net) for performing the tumour detection task in histopathology in a purely unsupervised manner. This methodology successfully promoted the alignment of the source and target feature distributions among independent datasets of three tumour types - Breast, Lung and Colon - to achieve an improvement of at least 17.51% in accuracy and 18.22% in area under the curve (AUC) when compared to a classifier trained on the source data only.","Histopathology image analysis,unsupervised domain adaptation,convolutional neural networks",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
88,REGION OF INTEREST IDENTIFICATION FOR CERVICAL CANCER IMAGES,,,1293-1296,"Gupta Manish,Das Chetna,Roy Arnab,Gupta Prashant,Pillai G. Radhakrishna,Patole Kamlakar","Gupta M,Das C,Roy A,Gupta P,Pillai GR,Patole K",Gupta M,,"Microsoft, Hyderabad, India.","Every two minutes one woman dies of cervical cancer globally, due to lack of sufficient screening. Given a whole slide image (WSI) obtained by scanning a microscope glass slide for a Liquid Based Cytology (LBC) based Pap test, our goal is to assist the pathologist to determine presence of precancerous or cancerous cervical anomalies. Inter-annotator variation, large image sizes, data imbalance, stain variations, and lack of good annotation tools make this problem challenging. Existing related work has focused on sub-problems like cell segmentation and cervical cell classification but does not provide a practically feasible holistic solution. We propose a practical system architecture which is based on displaying regions of interest on WSIs containing potential anomaly for review by pathologists to increase productivity. We build multiple deep learning classifiers as part of the proposed architecture. Our experiments with a dataset of similar to 19000 regions of interest provides an accuracy of similar to 89% for a balanced dataset both for binary as well as 6-class classification settings. Our deployed system provides a top-5 accuracy of similar to 94%.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
89,3D MAPPING OF TAU NEUROFIBRILLARY TANGLE PATHOLOGY IN THE HUMAN MEDIAL TEMPORAL LOBE,,,1312-1316,"Yushkevich Paul A.,de Onzono Martin Maria Mercedes Iniguez,Ittyerah Ranjit,Lim Sydney,Lavery Madigan,Wang Jiancong,Hung Ling Yu,Vergnet Nicolas,Ravikumar Sadhana,Xie Long","Yushkevich PA,Martin MMID,Ittyerah R,Lim S,Lavery M,Wang JC,Hung LY,Vergnet N,Ravikumar S,Xie L",Yushkevich PA,,University of Pennsylvania,"Tau protein neurofibrillary tangles (NFT) are linked to neuronal and synaptic loss and cognitive decline in Alzheimer's disease (AD) and related dementias. In AD, NFT pathology is known to spread through the cortex in a characteristic pattern, starting in the medial temporal lobe. However, the exact 3D pattern of NFT progression has not been described, and capturing this pattern quantitatively can help inform in vivo AD imaging biomarkers. We present a computational framework for generating 3D maps of NFT load from ex vivo MRI and serial histology. Weakly supervised deep learning is used to detect NFTs on histology slides prepared with an anti-tau immunohistochemistry stain, and a multi-stage registration pipeline that leverages 3D printing is used for histology-MRI alignment. Derived maps of NFT density are strongly concordant with manual NFT counting, as well as categorical NFT severity ratings used for clinical diagnosis.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
90,WEAKLY-SUPERVISED BRAIN TUMOR CLASSIFICATION WITH GLOBAL DIAGNOSIS LABEL,,,1317-1321,"Zhou Yufan,Li Zheshuo,Ma Chunwei,Chen Changyou,Gao Mingchen,Zhu Hong,Xu Jinhui","Zhou YF,Li ZS,Ma CW,Chen CY,Gao MC,Zhu H,Xu JH",Xu JH,,State University of New York (SUNY) System,"There is an increasing need for efficient and automatic evaluation of brain tumors on magnetic resonance images (MRI). Most of the previous works focus on segmentation, registration, and growth modeling of the most common primary brain tumor gliomas, or the classification of up to three types of brain tumors. In this work, we extend the study to eight types of brain tumors where only global diagnosis labels are given but not the slice-level labels. We propose a weakly supervised method and demonstrate that inferring disease types at the slice-level would help the global label prediction. We also provide an algorithm for feature extraction via randomly choosing connection paths through class-specific autoencoders with dropout to accommodate the small-dataset problem. Experimental results on both public and proprietary datasets are compared to the baseline methods. The classification with the weakly supervised setting on the proprietary data, consisting of 295 patients with eight different tumor types, shows close results to the upper bound in the supervised learning setting.","Autoencoder,small dataset,feature ensemble",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
91,DEEP LEARNING OF CORTICAL SURFACE FEATURES USING GRAPH-CONVOLUTION PREDICTS NEONATAL BRAIN AGE AND NEURODEVELOPMENTAL OUTCOME,,,1335-1338,"Liu Mengting,Duffy Ben A.,Sun Zhe,Toga Arthur W.,Barkovich A. James,Xu Duan,Kim Hosung","Liu MT,Duffy BA,Sun Z,Toga AW,Barkovich AJ,Xu D,Kim H",Liu MT,,University of Southern California,"We investigated the ability of graph convolutional network (GCN) that takes into account the mesh topology as a sparse graph to predict brain age for preterm neonates using cortical surface morphometrics, i.e. cortical thickness and sulcal depth. Compared to machine learning and deep learning methods that did not use the surface topological information, the GCN better predicted the ages for preterm neonates with none/mild perinatal brain injuries (NMI). We then tested the GCN trained using NMI brains to predict the age of neonates with severe brain injuries (SI). Results also displayed good accuracy (MAE=1.43 weeks), while the analysis of the interaction term (true age x group) showed that the slope of the predicted brain age relative to the true age for the SI group was significantly less steep than the NMI group (p<0.0001), indicating that SI can decelerate early postnatal growth. To understand regional contributions to age prediction, we applied GCNs separately to the vertices within each cortical parcellation. The middle cingulate cortex that is known to be one of the thickest cortical regions in the neonatal period showed the best accuracy in age prediction (MAE = 1.24 weeks). Furthermore, we found that the regional brain ages computed using GCN models in several frontal cortices significantly correlated with cognitive abilities at 3 years of age. Furthermore, the brain predicted age in part of the superior temporal cortex, which is the auditory and language processing locus, was related to language functional scores at 3 years. Our results demonstrate the potential of the GCN models for predicting brain age as well as localizing brain regions contributing to the prediction of age and future cognitive outcome.","Graph convolutional network,brain age prediction,neonates,relative brain age,injury detection,cognitive outcome prediction",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
92,UNSUPERVISED TASK DESIGN TO META-TRAIN MEDICAL IMAGE CLASSIFIERS,,,1339-1342,"Maicas Gabriel,Cuong Nguyen,Motlagh Farbod,Nascimento Jacinto C.,Carneiro Gustavo","Maicas G,Nguyen C,Motlagh F,Nascimento JC,Carneiro G",Maicas G,,University of Adelaide,"Meta-training has been empirically demonstrated to be the most effective pre-training method for few-shot learning of medical image classifiers (i.e., classifiers modeled with small training sets). However, the effectiveness of meta-training relies on the availability of a reasonable number of hand-designed classification tasks, which are costly to obtain, and consequently rarely available. In this paper, we propose a new method to unsupervisedly design a large number of classification tasks to meta-train medical image classifiers. We evaluate our method on a breast dynamically contrast enhanced magnetic resonance imaging (DCE-MRI) data set that has been used to benchmark few-shot training methods of medical image classifiers. Our results show that the proposed unsupervised task design to meta-train medical image classifiers builds a pre-trained model that, after fine-tuning, produces better classification results than other unsupervised and supervised pre-training methods, and competitive results with respect to meta-training that relies on hand-designed classification tasks.","meta-training,unsupervised learning,unsupervised task design,breast image analysis,magnetic resonance imaging,few-shot classification,pre-training,clustering",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,http://arxiv.org/pdf/1907.07816,
93,Computer-Aided Diagnosis of Congenital Abnormalities of the Kidney and Urinary Tract in Children Using a Multi-Instance Deep Learning Method Based on Ultrasound Imaging Data,,,1347-1350,"Yin Shi,Peng Qinmu,Li Hongming,Zhang Zhengqiang,You Xinge,Fischer Katherine,Furth Susan L.,Tasian Gregory E.,Fan Yong","Yin S,Peng QM,Li HM,Zhang ZQ,You XG,Fischer K,Furth SL,Tasian GE,Fan Y",Yin S,,Huazhong University of Science & Technology,"Ultrasound images are widely used for diagnosis of congenital abnormalities of the kidney and urinary tract (CAKUT). Since a typical clinical ultrasound image captures 2D information of a specific view plan of the kidney and images of the same kidney on different planes have varied appearances, it is challenging to develop a computer aided diagnosis tool robust to ultrasound images in different views. To overcome this problem, we develop a multi-instance deep learning method for distinguishing children with CAKUT from controls based on their clinical ultrasound images, aiming to automatic diagnose the CAKUT in children based on ultrasound imaging data. Particularly, a multi-instance deep learning method was developed to build a robust pattern classifier to distinguish children with CAKUT from controls based on their ultrasound images in sagittal and transverse views obtained during routine clinical care. The classifier was built on imaging features derived using transfer learning from a pre-trained deep learning model with a mean pooling operator for fusing instance-level classification results. Experimental results have demonstrated that the multi-instance deep learning classifier performed better than classifiers built on either individual sagittal slices or individual transverse slices.","Multi-instance deep learning,congenital abnormalities of the kidney and urinary tract,ultrasound images,sagittal view,transverse view",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
94,MACHINE-LEARNING ON LIVER ULTRASOUND TO STRATIFY MULTIPLE DISEASES VIA BLOOD-VESSELS AND PERFUSION CHARACTERISTICS,,,1351-1354,"Bayet Jules,Hoogenboom Tim,Sharma Rohini,Angelini Elsa D.","Bayet J,Hoogenboom T,Sharma R,Angelini ED",Bayet J,,Imperial College London,"Liver vessels can be visualized at sub-millimetre scale with contrast-enhanced ultrasound. In this work we exploit a cohort of 97 subjects (healthy volunteers and 4 liver disease types), exploiting multiple videos acquired at locations within the liver hand-picked by the sonographer to perform the diagnostic task. Annotation was performed at subject-level (disease subtype or healthy), along with scoring of image quality. We propose an original approach exploiting the abstraction capabilities of maximum intensity projections (MIPs) to feed a deep-learning classifier. Two architectures were tested for which we compared performance with different scenarios regarding the exploitation of transfer learning and the number of input MIPs per subjects. Our results show over 88% accuracy for a 2-class task (healthy versus disease), and 70% for a 3-class task (healthy versus 2 disease sub-types). We demonstrate, for the first time, that deep learning with minimal supervision and no pre-filtering can accurately classify liver diseases based on vascular ultrasound imaging acquired in a clinical setting. We also report findings on specific misclassication patterns which will guide further studies, augmentation of the cohort and subject annotation.","Ultrasound,contrast-enhancement,vascular imaging,liver,fibrosis,HCC,deep-learning",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
95,POLYP DETECTION IN COLONOSCOPY VIDEOS BY BOOTSTRAPPING VIA TEMPORAL CONSISTENCY,,,1360-1363,"Ma Ming,Chen Xuejin,Sun Bin","Ma M,Chen XJ,Sun B",Ma M,,Chinese Academy of Sciences,"Computer-aided polyp detection during colonoscopy is beneficial to reduce the risk of colorectal cancers. Deep learning techniques have made significant process in natural object detection. However, when applying those fully supervised methods to polyp detection, the performance is greatly depressed by the deficiency of labeled data. In this paper, we propose a novel bootstrapping method for polyp detection in colonoscopy videos by augmenting training data with temporal consistency. For a detection network that is trained on a small set of annotated polyp images, we fine-tune it with new samples selected from the test video itself, in order to more effectively represent the polyp morphology of current video. A strategy of selecting new samples is proposed by considering temporal consistency in the test video. Evaluated on 11954 endoscopic frames of the CVC-ClinicVideoDB dataset, our method yields great improvement on polyp detection for several detection networks, and achieves state-of-the-art performance on the benchmark dataset.","Polyp detection,Colonoscopy video,Bootstrapping,Neural network",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
96,ERRORNET: LEARNING ERROR REPRESENTATIONS FROM LIMITED DATA TO IMPROVE VASCULAR SEGMENTATION,,,1364-1368,"Tajbakhsh Nima,Lai Brian,Ananth Shilpa P.,Ding Xiaowei","Tajbakhsh N,Lai B,Ananth SP,Ding XW",Tajbakhsh N,,"VoxelCloud Inc, Los Angeles, CA 90024 USA.","Deep convolutional neural networks have proved effective in segmenting lesions and anatomies in various medical imaging modalities. However, in the presence of small sample size and domain shift problems, these models often produce masks with non-intuitive segmentation mistakes. In this paper, we propose a segmentation framework called ErrorNet, which learns to correct these segmentation mistakes through the repeated process of injecting systematic segmentation errors to the segmentation result based on a learned shape prior, followed by attempting to predict the injected error. During inference, ErrorNet corrects the segmentation mistakes by adding the predicted error map to the initial segmentation result. ErrorNet has advantages over alternatives based on domain adaptation or CRF-based post processing, because it requires neither domain-specific parameter tuning nor any data from the target domains. We have evaluated ErrorNet using five public datasets for the task of retinal vessel segmentation. The selected datasets differ in size and patient population, allowing us to evaluate the effectiveness of ErrorNet in handling small sample size and domain shift problems. Our experiments demonstrate that ErrorNet outperforms a base segmentation model, a CRF-based post processing scheme, and a domain adaptation method, with a greater performance gain in the presence of the aforementioned dataset limitations.","retinal vessel segmentation,limited data,domain shift,error prediction,error correction",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,http://arxiv.org/pdf/1910.04814,
97,AUTOMATED HEMORRHAGE DETECTION FROM COARSELY ANNOTATED FUNDUS IMAGES IN DIABETIC RETINOPATHY,,,1369-1372,"Huang Yijin,Lin Li,Li Meng,Wu Jiewei,Cheng Pujin,Wang Kai,Yuan Jin,Tang Xiaoying","Huang YJ,Lin L,Li M,Wu JW,Cheng PJ,Wang K,Yuan J,Tang XY",Tang XY,,Southern University of Science & Technology,"In this paper, we proposed and validated a novel and effective pipeline for automatically detecting hemorrhage from coarsely-annotated fundus images in diabetic retinopathy. The proposed framework consisted of three parts: image preprocessing, training data refining, and object detection using a convolutional neural network with label smoothing. Contrast limited adaptive histogram equalization and adaptive gamma correction with weighting distribution were adopted to improve image quality by enhancing image contrast and correcting image illumination. To refine coarsely-annotated training data, we designed a bounding box refining network (BBR-net) to provide more accurate bounding box annotations. Combined with label smoothing, RetinaNet was implemented to alleviate mislabeling issues and automatically detect hemorrhages. The proposed method was trained and evaluated on a publicly available IDRiD dataset and also one of our private datasets 1. Experimental results showed that our BBR-net could effectively refine manually-delineated coarse hemorrhage annotations, with the average IoU being 0.8715 when compared with well-annotated bounding boxes. The proposed hemorrhage detection pipeline was compared to pure RetinaNet and superior performance was observed.","Diabetic retinopathy,hemorrhage detection,bounding box refining network,coarse annotation,fundus image",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
98,HOW TO EXTRACT MORE INFORMATION WITH LESS BURDEN: FUNDUS IMAGE CLASSIFICATION AND RETINAL DISEASE LOCALIZATION WITH OPHTHALMOLOGIST INTERVENTION,,,1373-1377,"Meng Qier,Hashimoto Yohei,Satoh Shinichi","Meng QE,Hashimoto Y,Satoh S",Meng QE,,Research Organization of Information & Systems (ROIS),"Image classification using deep convolutional neural networks (DCNN) has a competitive performance as compare to other state-of-the-art methods. Here, attention can be visualized as a heatmap to improve the explainability of DCNN. We generated the initial heatmaps by using gradient-based classification activation map (Grad-CAM). We firstly assume that these Grad-CAM heatmaps can reveal the lesion regions well, then apply the attention mining on these heatmaps. Another, we assume that these Grad-CAM heatmaps can't reveal the lesion regions well then apply the dissimilarity loss on these Grad-CAM heatmaps. In this study, we asked the ophthalmologists to select 30% of the heatmaps. Furthermore, we design a knowledge preservation (KP) loss to minimize the discrepancy between heatmaps generated from the updated network and the selected heatmaps. Experiments revealed that our method improved accuracy from 90.1% to 96.2%. We also found that the attention regions are closer to the GT lesion regions.","Lesion localization,Grad-CAM,Attention mining,Knowledge preservation",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
99,SUNET: A LESION REGULARIZED MODEL FOR SIMULTANEOUS DIABETIC RETINOPATHY AND DIABETIC MACULAR EDEMA GRADING,,,1378-1382,"Tu Zhi,Gao Shenghua,Zhou Kang,Chen Xianing,Fu Huazhu,Gu Zaiwang,Cheng Jun,Yu Zehao,Liu Jiang","Tu Z,Gao SH,Zhou K,Chen XN,Fu HZ,Gu ZW,Cheng J,Yu ZH,Liu J",Tu Z,,ShanghaiTech University,"Diabetic retinopathy (DR), as a leading ocular disease, is often with a complication of diabetic macular edema (DME). However, most existing works only aim at DR grading but ignore the DME diagnosis, but doctors will do both tasks simultaneously. In this paper, motivated by the advantages of multi-task learning for image classification, and to mimic the behavior of clinicians in visual inspection for patients, we propose a feature Separation and Union Network (SUNet) for simultaneous DR and DME grading. Further, to improve the interpretability of the disease grading, a lesion regularizer is also imposed to regularize our network. Specifically, given an image, our SUNet first extracts a common feature for both DR and DME grading and lesion detection. Then a feature blending block is introduced which alternately uses feature separation and feature union for task-specific feature extraction, where feature separation learns task-specific features for lesion detection and DR and DME grading, and feature union aggregates features corresponding to lesion detection, DR and DME grading. In this way, we can distill the irrelevant features and leverage features of different but related tasks to improve the performance of each given task. Then the task-specific features of the same task at different feature separation steps are concatenated for the prediction of each task. Extensive experiments on the very challenging IDRiD dataset demonstrate that our SUNet significantly outperforms existing methods for both DR and DME grading.","Multi-disease diagnosis,Lesion regularization,Feature blending",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,,,,
100,AUTOMATIC ANGLE-CLOSURE GLAUCOMA SCREENING BASED ON THE LOCALIZATION OF SCLERAL SPUR IN ANTERIOR SEGMENT OCT,,,1387-1390,"Li Panming,Geng Le,Zhu Weifang,Shi Fei,Chen Xinjian","Li PM,Geng L,Zhu WF,Shi F,Chen XJ",Chen XJ,,Suzhou University,"As one of the major types of glaucoma, closed-angle glaucoma is the leading cause of irreversible blindness in the world. The ability of Anterior Segment Optical Coherence Tomography (AS-OCT) to obtain high-resolution cross-sectional images of the entire anterior chamber in a single image makes it an important tool for glaucoma diagnosis. In this paper, we propose a practical and efficient system based on deep learning to accurately classify anterior chamber angle (ACA) closure by using the location of scleral spur (SS) points. First, the localization problem is reformulated as a pixel-wise regression task. A fully convolutional deep neural network is optimized to predict the probability that each pixel belongs to the SS points, and the numerical coordinates are obtained by the maximum likelihood estimation theory. Second, the ACA region centered on the detected SS is cropped as the input of the classification model. The single model applied for classification is SE-ResNet18 and optimized with focal loss. In the AGE Challenge 2019[1], our proposed method obtained superior performance for angle-closure glaucoma screening.","AS-OCT,glaucoma screening,scleral spur localization,angle-closure classification",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,,"OPTICAL,COHERENCE,TOMOGRAPHY,CLASSIFICATION",,,
