,title,Volume,Issue,Pages,whole__author_name,simply_author_name,reprint author,DOI,reprint address,Abstract,Keywords,Document Type,Publisher,Research Domain,Published Date,impact_factor,Keywords_plus,joural,pdf_link,Download_SuccessOrDefeat
1,"Behavior assessment, regression analysis and support vector machine (SVM) modeling of waste tire rubberized concrete",273,,,"Jalal Mostafa,Arabali Poura,Grasley Zachary,Bullard Jeffrey W.,Jalal Hamid","Jalal M,Arabali P,Grasley Z,Bullard JW,Jalal H",Jalal M,10.1016/j.jclepro.2020.122960,Texas A&M University System,"An experimental study was undertaken to assess the properties of rubberized concrete made by recycled tire rubber. Mechanical tests were carried out and the ultrasonic technique was also used to measure the acoustic properties of the rubberized concrete, such as wave velocity, and the relationships between the mechanical properties and pulse velocity for different mixtures were investigated. Then, based on the experimental data, a thorough strength modeling was performed using regression analysis and support vector machine (SVM) technique. To develop the predictive models for strength behavior of the recycled rubber concrete, a comprehensive regression analyses including ANOVA, t-test and F-test were conducted, and the significance of the influencing variables was determined. Then, several regression models by incorporating different combinations of the variables were developed and compared. Finally, the SVM method as a machine-learning technique was utilized to develop several models by using different kernel functions, optimization algorithms, and hyperparameters optimization to predict the compressive strength of the recycled rubber concrete. The results obtained were compared with those of regression models and it was found that SVM outperforms all the regression models assessed in this study. (c) 2020 Elsevier Ltd. All rights reserved.","Waste tire rubberized concrete,Compressive strength,Regression analysis,ANOVA,Machine learning,Support-vector machine (SVM)",Article,"ELSEVIER SCI LTD, THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND","Science & Technology - Other Topics,Engineering,Environmental Sciences & Ecology",,9.444,"COMPRESSIVE,STRENGTH,PREDICTION,FREE-VIBRATION,ANALYSIS,SILICA,FUME,HARDENED,PROPERTIES,NATURAL,ZEOLITE,CEMENT,CONCRETE,ANNULAR,PLATES,TYRE-RUBBER,OPTIMIZATION,PERFORMANCE",JOURNAL OF CLEANER PRODUCTION,,
2,A hybrid intelligent system for designing optimal proportions of recycled aggregate concrete,273,,,"Zhang Junfei,Huang Yimiao,Aslani Farhad,Ma Guowei,Nener Brett","Zhang JF,Huang YM,Aslani F,Ma GW,Nener B",Huang YM,10.1016/j.jclepro.2020.122922,University of Western Australia,"The replacement of natural coarse aggregate (NCA) with recycled coarse aggregate (RCA) in concrete mixtures offers various advantages, including conservation of natural resources, reduction of CO2 emissions, and cost reduction. However, multiple related variables and objectives (e.g., mechanical, economic, and environmental objectives) need to be considered when optimizing mixtures of recycled aggregate concrete (RAC). This cannot be achieved through traditional laboratory- or statistics-based methods. This study proposes a hybrid intelligent system based on artificial intelligence (AI) and metaheuristic algorithms for designing optimal mixtures of RAC. To verify the proposed model, a data set containing 344 different RAC mixtures was collected from previous literature. A semi-supervised cotraining algorithm using two k-nearest neighbor (kNN) regressors with different distance metrics is developed to label the unlabeled data in the collected dataset. Different AI models are incorporated into the system for modeling the relationship between RAC strength and its influencing variables. A multi-objective optimization (MOO) model based on AI algorithms and on a multi-objective firefly algorithm is used to search for optimal mixtures of RAC. The results show that kNN-based semi-supervised cotraining can effectively exploit unlabeled data to improve the regression estimates. In the test set, A Random Forest and Backpropagation Neural Network achieve the best prediction accuracy for predicting, respectively, uniaxial compressive strength and splitting tensile strength of RAC, indicated by the highest correlation coefficients (0.9064 and 0.8387, respectively) and lowest root-mean-square errors (6.639 MPa and 0.5119 MPa, respectively). The Pareto fronts of the multi-objective mixture optimization problem are successfully obtained by the MOO model. The proposed system can also be used to optimize mixture proportions of other cementitious materials in civil engineering. (C) 2020 Elsevier Ltd. All rights reserved.","Recycled aggregate concrete,Mechanical properties,Artificial intelligence,Firefly algorithm,Concrete mixture optimization",Article,"ELSEVIER SCI LTD, THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND","Science & Technology - Other Topics,Engineering,Environmental Sciences & Ecology",,9.444,"UNCONFINED,COMPRESSIVE,STRENGTH,MECHANICAL-PROPERTIES,MIX,DESIGN,REGRESSION,CONSTRUCTION,PREDICTION,MODULUS,WASTE,MODEL",JOURNAL OF CLEANER PRODUCTION,,
3,Machine learning exploration of the critical factors for CO2 adsorption capacity on porous carbon materials at different pressures,273,,,"Zhu Xinzhe,Tsang Daniel C. W.,Wang Lei,Su Zhishan,Hou Deyi,Li Liangchun,Shang Jin","Zhu XZ,Tsang DCW,Wang L,Su ZS,Hou DY,Li LC,Shang J",Tsang DCW,10.1016/j.jclepro.2020.122915,Hong Kong Polytechnic University,"The growing environmental issues caused by CO2 emission accelerate the development of carbon capture and storage (CCS), especially bio-energy CCS as an environment-friendly and sustainable technique to capture CO2 using porous carbon materials (PCMs) produced from various biomass wastes. This study developed quantitative structure-property relationship models based on 6244 CO2 adsorption datasets of 155 PCMs to predict the CO2 adsorption capacity and analyze the relative significance of physicochemical properties. The results suggested that random forest (RF) models showed good accuracy and predictive performance based on physicochemical parameters of PCMs and adsorption conditions with the test dataset (R-2 > 0.9). In general, textural properties were more crucial than chemical compositions of porous carbons to the change of CO2 adsorption capacity. At a low pressure (0.1 bar), the volumes of mesopore and micropore played an important role according to the RF analysis, but had a negative correlation with CO2 adsorption capacity based on the Pearson correlation coefficient (PCC) analysis. The relative importance of ultra-micropore increased along with the increase of pressure. The PCC value between ultra-micropore volume and CO2 uptake amount was up to 0.715 (p < 0.01) at 1 bar and 0 degrees C. The influence of chemical compositions was complex. The N content was confirmed to positively correlate to the CO2 adsorption capacity but its contribution was much lower than that of ultra-micropores. This study provided a new approach for fostering the rational design of porous carbons for CO2 capture via statistical analysis and machine learning method, which facilitated adsorbents screening for the cleaner production. (C) 2020 Elsevier Ltd. All rights reserved.","CO2 sequestration,Carbon adsorbents,Sustainable waste management,Low-carbon development,Biomass utilization",Article,"ELSEVIER SCI LTD, THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND","Science & Technology - Other Topics,Engineering,Environmental Sciences & Ecology",,9.444,"SINGLE-STEP,SYNTHESIS,SODIUM,AMIDE,ACTIVATION,DIOXIDE,CAPTURE,LOW-TEMPERATURE,COMBINING,AMMOXIDATION,KOH,ACTIVATION,PORE-SIZE,ADSORBENTS,ENERGY,TEXTURE",JOURNAL OF CLEANER PRODUCTION,,
4,DL-SCALE: a novel deep learning-based model order upscaling scheme for solving topology optimization problems,33,12,7125-7144,"Kallioras Nikos Ath.,Lagaros Nikos D.","Kallioras NA,Lagaros ND",Lagaros ND,10.1007/s00521-020-05480-8,National Technical University of Athens,"The main scope of this study is to propose a novel methodology aiming at enhancing the computational efficiency of the approaches used for solving structural topology optimization (STO) problems. The methodology is based on machine learning combined with the idea of using multiple finite element (FE) models of reduced order. The capability of deep belief networks (DBNs) in discovering multiple representational levels of data nonlinearity in pattern recognition problems recently triggered the development of the DLTOP methodology by the authors Kallioras et al. (Struct Multidiscip Optim, 2020, 10.1007/s00158-020-02545-z), that is based on DBNs and the solid isotropic material with penalization (SIMP) approach. In this study, a FE model order upgrading scheme integrated with the DLTOP methodology is proposed for accelerating further the SIMP-based solution procedure of the STO problems with no scalability limitations, labeled as DL-SCALE. The framework of DL-SCALE is based on a combined implementation of DBNs and SIMP into a sequentially implemented ""model-optimize-and-order-upgrade"" scheme. DL-SCALE efficiency is validated over several benchmark topology optimization test-examples. The results obtained for the test-examples clearly prove its computational advantages; the computing time is reduced by almost one order of magnitude while the corresponding reduction in terms of iterations is more than one order of magnitude compared to the ones originally required by SIMP, without any loss with respect to objective function value. It is also concluded from the results obtained that the proposed methodology can escalate to various finite element mesh discretizations, while optimized layout information transfer is possible, contributing also in accelerating further the STO procedure.","Topology optimization,Order upgrading,Deep learning,Computational efficiency,SIMP approach,Deep belief networks",Article,"SPRINGER LONDON LTD, 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND",Computer Science,,5.573,"STRUCTURAL,OPTIMIZATION,ALGORITHM,SHAPE,DESIGN",NEURAL COMPUTING & APPLICATIONS,,
5,Uncertainty Quantification of Machine Learning Predicted Creep Property of Alumina-Forming Austenitic Alloys,73,1,164-173,"Peng Jian,Yamamoto Yukinori,Brady Michael P.,Lee Sangkeun,Haynes J. Allen,Shin Dongwon","Peng J,Yamamoto Y,Brady MP,Lee S,Haynes JA,Shin D",Shin D,10.1007/s11837-020-04423-x,United States Department of Energy (DOE),"The development of machine learning (ML) approaches in materials science offers the opportunity to exploit existing engineering and developmental alloy datasets, such as Oak Ridge National Laboratory (ORNL)'s consistently measured creep-rupture dataset for alumina-forming austenitic (AFA) alloys, to accelerate their further development. As a first step toward achieving ML insights for improved alloy design, the potential sources of uncertainty and their impacts on ML output are examined. It is observed that the selection of algorithms and features as well as data sampling significantly affects the performance of ML models, either positively or negatively. The performance of various ML models in predicting the creep properties of AFA alloys is compared, with further evaluation by assessment of a small set of new developmental AFA alloys that were not part of the training dataset. The present study demonstrates that uncertainty quantification (UQ) is essential in materials science for evaluating the performance of ML algorithms with specifically selected feature sets and obtaining a comprehensive understanding of their limitations and the resultant capability of effective prediction in complex materials systems.","STAINLESS-STEEL,THERMO-CALC",Article,"SPRINGER, ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES","Materials Science,Metallurgy & Metallurgical Engineering,Mineralogy,Mining & Mineral Processing",,2.989,"STAINLESS-STEEL,THERMO-CALC",JOM,,
6,Fe-Based Superconducting Transition Temperature Modeling through Gaussian Process Regression,202,1-2,205-218,"Zhang Yun,Xu Xiaojie","Zhang Y,Xu XJ",Zhang Y,10.1007/s10909-020-02545-9,University of North Carolina,"Extensive research has been conducted to find new superconducting materials that exhibit high critical temperature T-c, in order to fulfill the needs of practical applications with liquid-helium-free refrigeration or even at room temperature. Iron-based superconductors show high T-c and high upper critical field. The research, however, requires significant manpower for materials synthesis and characterization, and costly equipment and facilities. Computational approaches have contributed greatly to investigate the properties of solid-state matter in many fields, which can be integrated to machine learning and big-data analysis. In this work, the Gaussian process regression model is developed to predict Fe-based superconductor critical temperature based on lattice parameters. This modeling approach demonstrates a high degree of accuracy and stability that lead to the statistical relationship between the lattice parameters and T-c. The results disclosed by this work can also lead to a better understanding of the origin of superconductivity in these materials.","Critical temperature,Gaussian process regression,High-temperature superconductors,Iron-based superconductors,Lattice parameters",Article,"SPRINGER/PLENUM PUBLISHERS, 233 SPRING ST, NEW YORK, NY 10013 USA",Physics,,1.285,"T-C,IRON",JOURNAL OF LOW TEMPERATURE PHYSICS,,
7,IrO2 Surface Complexions Identified through Machine Learning and Surface Investigations,125,20,,"Timmermann Jakob,Kraushofer Florian,Resch Nikolaus,Li Peigang,Wang Yu,Mao Zhiqiang,Riva Michele,Lee Yonghyuk,Staacke Carsten,Schmid Michael","Timmermann J,Kraushofer F,Resch N,Li PG,Wang Y,Mao ZQ,Riva M,Lee Y,Staacke C,Schmid M",Reuter K,10.1103/PhysRevLett.125.206101,Technical University of Munich,A Gaussian approximation potential was trained using density-functional theory data to enable a global geometry optimization of low-index rutile IrO2 facets through simulated annealing. Ab initio thermodynamics identifies (101) and (111) (1 x 1) terminations competitive with (110) in reducing environments. Experiments on single crystals find that (101) facets dominate and exhibit the theoretically predicted (1 x 1) periodicity and x-ray photoelectron spectroscopy core-level shifts. The obtained structures are analogous to the complexions discussed in the context of ceramic battery materials.,"AB-INITIO THERMODYNAMICS,ELECTROCATALYTIC ACTIVITY,EVOLVING ACTIVITY,OXYGEN EVOLUTION,PERFORMANCE,CATALYST,NANOPARTICLES,BATTERIES,PHASE",Article,"AMER PHYSICAL SOC, ONE PHYSICS ELLIPSE, COLLEGE PK, MD 20740-3844 USA",Physics,,9.044,"AB-INITIO,THERMODYNAMICS,ELECTROCATALYTIC,ACTIVITY,EVOLVING,ACTIVITY,OXYGEN,EVOLUTION,PERFORMANCE,CATALYST,NANOPARTICLES,BATTERIES,PHASE",PHYSICAL REVIEW LETTERS,https://pure.mpg.de/pubman/item/item_3265324_2/component/file_3265816/PhysRevLett.125.206101.pdf,
8,Active learning for the power factor prediction in diamond-like thermoelectric materials,6,1,,"Sheng Ye,Wu Yasong,Yang Jiong,Lu Wencong,Villars Pierre,Zhang Wenqing","Sheng Y,Wu YS,Yang J,Lu WC,Villars P,Zhang WQ",Yang J,10.1038/s41524-020-00439-8,Shanghai University,"The Materials Genome Initiative requires the crossing of material calculations, machine learning, and experiments to accelerate the material development process. In recent years, data-based methods have been applied to the thermoelectric field, mostly on the transport properties. In this work, we combined data-driven machine learning and first-principles automated calculations into an active learning loop, in order to predict the p-type power factors (PFs) of diamond-like pnictides and chalcogenides. Our active learning loop contains two procedures (1) based on a high-throughput theoretical database, machine learning methods are employed to select potential candidates and (2) computational verification is applied to these candidates about their transport properties. The verification data will be added into the database to improve the extrapolation abilities of the machine learning models. Different strategies of selecting candidates have been tested, finally the Gradient Boosting Regression model of Query by Committee strategy has the highest extrapolation accuracy (the Pearson R = 0.95 on untrained systems). Based on the prediction from the machine learning models, binary pnictides, vacancy, and small atom-containing chalcogenides are predicted to have large PFs. The bonding analysis reveals that the alterations of anionic bonding networks due to small atoms are beneficial to the PFs in these compounds.","PLANE-WAVE,APPROXIMATION,DENSITY,COHP",Article,"NATURE RESEARCH, HEIDELBERGER PLATZ 3, BERLIN, 14197, GERMANY","Chemistry,Materials Science",,13.993,"PLANE-WAVE,APPROXIMATION,DENSITY,COHP",NPJ COMPUTATIONAL MATERIALS,https://www.nature.com/articles/s41524-020-00439-8.pdf,
9,,,,,,,,,,,,,,,,,,,,
10,,,,,,,,,,,,,,,,,,,,
11,Emerging materials intelligence ecosystems propelled by machine learning,6,8,655-678,"Batra Rohit,Song Le,Ramprasad Rampi","Batra R,Song L,Ramprasad R",Ramprasad R,10.1038/s41578-020-00255-y,University System of Georgia,"The age of cognitive computing and artificial intelligence (AI) is just dawning. Inspired by its successes and promises, several AI ecosystems are blossoming, many of them within the domain of materials science and engineering. These materials intelligence ecosystems are being shaped by several independent developments. Machine learning (ML) algorithms and extant materials data are utilized to create surrogate models of materials properties and performance predictions. Materials data repositories, which fuel such surrogate model development, are mushrooming. Automated data and knowledge capture from the literature (to populate data repositories) using natural language processing approaches is being explored. The design of materials that meet target property requirements and of synthesis steps to create target materials appear to be within reach, either by closed-loop active-learning strategies or by inverting the prediction pipeline using advanced generative algorithms. AI and ML concepts are also transforming the computational and physical laboratory infrastructural landscapes used to create materials data in the first place. Surrogate models that can outstrip physics-based simulations (on which they are trained) by several orders of magnitude in speed while preserving accuracy are being actively developed. Automation, autonomy and guided high-throughput techniques are imparting enormous efficiencies and eliminating redundancies in materials synthesis and characterization. The integration of the various parts of the burgeoning ML landscape may lead to materials-savvy digital assistants and to a human-machine partnership that could enable dramatic efficiencies, accelerated discoveries and increased productivity. Here, we review these emergent materials intelligence ecosystems and discuss the imminent challenges and opportunities.
The materials research landscape is being transformed by the infusion of approaches based on machine learning. This Review discusses the emerging materials intelligence ecosystems and the potential of human-machine partnerships for fast and efficient virtual materials screening, development and discovery.","MOLECULAR-DYNAMICS SIMULATIONS,STRUCTURE-PROPERTY LINKAGES,HIGH-CONTRAST COMPOSITES,MATERIALS DISCOVERY,DESIGN,PERFORMANCE,CRYSTAL,PREDICTION,DATABASE,MICROSTRUCTURE",Review,"NATURE PORTFOLIO, HEIDELBERGER PLATZ 3, BERLIN, 14197, GERMANY","Science & Technology - Other Topics,Materials Science",,98.837,"MOLECULAR-DYNAMICS,SIMULATIONS,STRUCTURE-PROPERTY,LINKAGES,HIGH-CONTRAST,COMPOSITES,MATERIALS,DISCOVERY,DESIGN,PERFORMANCE,CRYSTAL,PREDICTION,DATABASE,MICROSTRUCTURE",NATURE REVIEWS MATERIALS,,
12,Study on field demonstration of high-sensitivity SF6 leakage detection method for gas insulated switchgear,214,1,35-41,"Tatemi Masaru,Inami Hisao,Rokunohe Toshiaki,Hirose Makoto","Tatemi M,Inami H,Rokunohe T,Hirose M",Tatemi M,10.1002/eej.23295,Hitachi Limited,"SF6 gas is widely used in power equipment such as gas insulated switchgear or gas circuit breaker. However, its global warming potential is 23,500 times higher than that of CO2. In consideration of environmental protection, detection of SF6 gas leakage with high sensitivity is necessary. The authors proposed an algorithm to accurately estimate the average gas temperature from the tank surface temperature for establishing a high-sensitive SF6 slow leak detection technology. Since the gas temperature depends on the temperature of the conductor, the algorithm was characterized by estimating the gas temperature from the tank surface temperature and the load current. The relationship among the tank surface temperature, the gas temperature, and the current was preliminarily investigated by the use of the thermal fluid analysis and stored in the database. By use of this method, the fluctuation of the converted pressure in the basic examination test was suppressed to 20% as compared with that without considering the current. In addition, database was also created from the machine learning of field test result in substitution. In the field test, it was confirmed that the detection time of 0.5%/year slow leak could be detected in about 8 months by considering the load current.","diagnosis,gas pressure,gas&#8208,insulated switchgear,slow leakage,sulfur hexafluoride",Article,"WILEY, 111 RIVER ST, HOBOKEN 07030-5774, NJ USA",Engineering,,0.295,,ELECTRICAL ENGINEERING IN JAPAN,https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/eej.23295,
13,Technical note: Interpolated Pareto surface similarity metrics for multi-criteria optimization in radiation therapy,47,12,6450-6457,"Jensen P. James,Zhang Jiahan,Wu Q. Jackie","Jensen PJ,Zhang JH,Wu QJ",Wu QJ,10.1002/mp.14541,Duke University,"Purpose There is a strong clinical need to evaluate different multi-criteria optimization (MCO) algorithms, including inverse optimization sampling algorithms and machine learning-based predictions. This study aims to develop and compare several interpolated Pareto surface similarity metrics.
Materials and methods The first metric is the root-mean-square error (RMSE) evaluated between vertices on the interpolated surfaces, augmented by intra-simplex sampling of the barycentric coordinates of the surfaces' simplicial complexes. The second metric is the average projected distance (APD), which evaluates the displacements between the vertices and computes their projections along the mean displacement. The third metric is the average nearest-point distance (ANPD), which numerically integrates point-to-simplex distances over the sampled simplices of the interpolated surfaces. These metrics were compared by their convergence rates, the times required to achieve convergence, and their representation of the underlying surface interpolations. For analysis, several interpolated Pareto surface pairs were constructed abstractly, with one pair from a nasopharyngeal treatment planning case using MCO.
Results Convergence within 1% is typically achieved at approximately 50 and 80 samples per barycentric dimension for the RMSE and the ANPD, respectively. Calculation requires approximately 1 and 10 ms to achieve convergence for the RMSE and the ANPD in two dimensions, respectively, while the APD always requires < 1 ms. These time costs are much higher in higher dimensions for just the RMSE and ANPD. The APD values more closely approximated the ANPD limits than the RMSE limits.
Conclusion The ANPD's formulation and generality make it likely more meaningful than the RMSE and APD for representing the similarity between the underlying interpolated surfaces rather than the sampling points on the surfaces. However, in situations requiring high-speed evaluations, the APD may be more desirable due to its speed, independence from a subjectively chosen sampling rate, and similarity to the ANPD limits.","artificial intelligence,machine learning,multi&#8208,criteria optimization,Pareto surface,radiation therapy",Article,"WILEY, 111 RIVER ST, HOBOKEN 07030-5774, NJ USA","Radiology, Nuclear Medicine & Medical Imaging",,,"ALGORITHMS,PREDICTION",MEDICAL PHYSICS,,
14,Generating anthropomorphic phantoms using fully unsupervised deformable image registration with convolutional neural networks,47,12,,"Chen Junyu,Li Ye,Du Yong,Frey Eric C.","Chen JY,Li Y,Du Y,Frey EC",Chen JY,10.1002/mp.14545,Johns Hopkins University,"Purpose: Computerized phantoms have been widely used in nuclear medicine imaging for imaging system optimization and validation. Although the existing computerized phantoms can model anatomical variations through organ and phantom scaling, they do not provide a way to fully reproduce the anatomical variations and details seen in humans. In this work, we present a novel registration-based method for creating highly anatomically detailed computerized phantoms. We experimentally show substantially improved image similarity of the generated phantom to a patient image.
Methods: We propose a deep-learning-based unsupervised registration method to generate a highly anatomically detailed computerized phantom by warping an XCAT phantom to a patient computed tomography (CT) scan. We implemented and evaluated the proposed method using the NURBS-based XCAT phantom and a publicly available low-dose CT dataset from TCIA. A rigorous tradeoff analysis between image similarity and deformation regularization was conducted to select the loss function and regularization term for the proposed method. A novel SSIM-based unsupervised objective function was proposed. Finally, ablation studies were conducted to evaluate the performance of the proposed method (using the optimal regularization and loss function) and the current state-of-the-art unsupervised registration methods.
Results: The proposed method outperformed the state-of-the-art registration methods, such as SyN and VoxelMorph, by more than 8%, measured by the SSIM and less than 30%, by the MSE. The phantom generated by the proposed method was highly detailed and was almost identical in appearance to a patient image.
Conclusions: A deep-learning-based unsupervised registration method was developed to create anthropomorphic phantoms with anatomies labels that can be used as the basis for modeling organ properties. Experimental results demonstrate the effectiveness of the proposed method. The resulting anthropomorphic phantom is highly realistic. Combined with realistic simulations of the image formation process, the generated phantoms could serve in many applications of medical imaging research. (C) 2020 American Association of Physicists in Medicine","computerized Phantom,convolutional Neural Networks,deep Neural Networks,image Registration,medical Image Simulation",Article,"WILEY, 111 RIVER ST, HOBOKEN 07030-5774, NJ USA","Radiology, Nuclear Medicine & Medical Imaging",,3.767,,MEDICAL PHYSICS,http://arxiv.org/pdf/1912.02942,
15,DeepCryoPicker: fully automated deep neural network for single protein particle picking in cryo-EM,21,1,,"Al-Azzawi Adil,Ouadou Anes,Max Highsmith,Duan Ye,Tanner John J.,Cheng Jianlin","Al-Azzawi A,Ouadou A,Max H,Duan Y,Tanner JJ,Cheng JL",Cheng JL,10.1186/s12859-020-03809-7,University of Missouri System,"Background Cryo-electron microscopy (Cryo-EM) is widely used in the determination of the three-dimensional (3D) structures of macromolecules. Particle picking from 2D micrographs remains a challenging early step in the Cryo-EM pipeline due to the diversity of particle shapes and the extremely low signal-to-noise ratio of micrographs. Because of these issues, significant human intervention is often required to generate a high-quality set of particles for input to the downstream structure determination steps. Results Here we propose a fully automated approach (DeepCryoPicker) for single particle picking based on deep learning. It first uses automated unsupervised learning to generate particle training datasets. Then it trains a deep neural network to classify particles automatically. Results indicate that the DeepCryoPicker compares favorably with semi-automated methods such as DeepEM, DeepPicker, and RELION, with the significant advantage of not requiring human intervention. Conclusions Our framework combing supervised deep learning classification with automated un-supervised clustering for generating training data provides an effective approach to pick particles in cryo-EM images automatically and accurately.","Deep learning,Super clustering,Intensity based clustering (IBC),Micrograph,Cryo-EM,Singe particle pickling,Protein structure determination,AutoCryoPicker,SuperCryoPicker",Article,"BMC, CAMPUS, 4 CRINAN ST, LONDON N1 9XW, ENGLAND","Biochemistry & Molecular Biology,Biotechnology & Applied Microbiology,Mathematical & Computational Biology",,3.629,"SELECTION,RESOLUTION,COMPLEX",BMC BIOINFORMATICS,https://bmcbioinformatics.biomedcentral.com/track/pdf/10.1186/s12859-020-03809-7,
16,A Recurrence-Specific Gene-Based Prognosis Prediction Model for Lung Adenocarcinoma through Machine Learning Algorithm,2020,,,"Xu Shaohua,Zhou Jie,Liu Kai,Chen Zhoumiao,He Zhengfu","Xu SH,Zhou J,Liu K,Chen ZM,He ZF",He ZF,10.1155/2020/9124792,Zhejiang University,"Background. After curative surgical resection, about 30-75% lung adenocarcinoma (LUAD) patients suffer from recurrence with dismal survival outcomes. Identification of patients with high risk of recurrence to impose intense therapy is urgently needed. Materials and Methods. Gene expression data of LUAD were obtained from The Cancer Genome Atlas (TCGA) and Gene Expression Omnibus (GEO) databases. Differentially expressed genes (DEGs) were calculated by comparing the recurrent and primary tissues. Prognostic genes associated with the recurrence-free survival (RFS) of LUAD patients were identified using univariate analysis. LASSO Cox regression and multivariate Cox analysis were applied to extract key genes and establish the prediction model. Results. We detected 37 DEGs between primary and recurrent LUAD tumors. Using univariate analysis, 31 DEGs were found to be significantly associated with RFS. We established the RFS prediction model including thirteen genes using the LASSO Cox regression. In the training cohort, we classified patients into high- and low-risk groups and found that patients in the high-risk group suffered from worse RFS compared to those in the low-risk group (P<0.01). Concordant results were confirmed in the internal and external validation cohort. The efficiency of the prediction model was also confirmed under different clinical subgroups. The high-risk group was significantly identified as the risk factor of recurrence in LUAD by the multivariate Cox analysis (HR=13.37, P=0.01). Compared to clinicopathological features, our prediction model possessed higher accuracy to identify patients with high risk of recurrence (AUC=96.3%). Finally, we found that the G2M checkpoint pathway was enriched both in recurrent tumors and primary tumors of high-risk patients. Conclusions. Our recurrence-specific gene-based prognostic prediction model provides extra information about the risk of recurrence in LUAD, which is conducive for clinicians to conduct individualized therapy in clinic.","SURVIVAL PREDICTION,EXPRESSION,CANCER,VALIDATION,SIGNATURES,THERAPY,TARGET",Article,"HINDAWI LTD, ADAM HOUSE, 3RD FLR, 1 FITZROY SQ, LONDON, W1T 5HF, ENGLAND","Biotechnology & Applied Microbiology,Research & Experimental Medicine",,3.62,"SURVIVAL,PREDICTION,EXPRESSION,CANCER,VALIDATION,SIGNATURES,THERAPY,TARGET",BIOMED RESEARCH INTERNATIONAL,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7669350,
17,From remote sensing and machine learning to the history of the Silk Road: large scale material identification on wall paintings,10,1,,"Kogou Sotiria,Shahtahmassebi Golnaz,Lucian Andrei,Liang Haida,Shui Biwen,Zhang Wenyuan,Su Bomin,van Schaik Sam","Kogou S,Shahtahmassebi G,Lucian A,Liang HD,Shui BW,Zhang WY,Su BM,van Schaik S",Liang HD,10.1038/s41598-020-76457-9,Nottingham Trent University,"Automatic remote reflectance spectral imaging of large painted areas in high resolution, from distances of tens of meters, has made the imaging of entire architectural interior feasible. However, it has significantly increased the volume of data. Here we present a machine learning based method to automatically detect 'hidden' writings and map material variations. Clustering of reflectance spectra allowed materials at inaccessible heights to be properly identified by performing non-invasive analysis on regions in the same cluster at accessible heights using a range of complementary spectroscopic techniques. The world heritage site of the Mogao caves, along the ancient Silk Road, consists of 492 richly painted Buddhist cave temples dating from the fourth to fourteenth century. Cave 465 at the northern end of the site is unique in its Indo-Tibetan tantric Buddhist style, and like many other caves, the date of its construction is still under debate. This study demonstrates the powers of an interdisciplinary approach that combines material identification, palaeographic analysis of the revealed Sanskrit writings and archaeological evidence for the dating of the cave temple paintings, narrowing it down to the late twelfth century to thirteenth century.","REFLECTANCE IMAGING SPECTROSCOPY,PIGMENTS,LIBRARY,ART",Article,"NATURE RESEARCH, HEIDELBERGER PLATZ 3, BERLIN, 14197, GERMANY",Science & Technology - Other Topics,,5.134,"REFLECTANCE,IMAGING,SPECTROSCOPY,PIGMENTS,LIBRARY,ART",SCIENTIFIC REPORTS,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7652859,
18,Machine learning potentials for multicomponent systems: The Ti-Al binary system,102,17,,Seko Atsuto,Seko A,Seko A,10.1103/PhysRevB.102.174104,Kyoto University,"Machine learning potentials (MLPs) are becoming powerful tools for performing accurate atomistic simulations and crystal structure optimizations. An approach to developing MLPs employs a systematic set of polynomial invariants including high-order ones to represent the neighboring atomic density. In this study, a formulation of the polynomial invariants is extended to the case of multicomponent systems. The extended formulation is more complex than the formulation for elemental systems. This study also shows its application to the Ti-Al binary system. As a result, an MLP with the lowest error and MLPs with high computational cost performance are selected from the many MLPs developed systematically. The predictive powers of the developed MLPs for many properties, such as the formation energy, elastic constants, thermodynamic properties, and mechanical properties, are examined. The MLPs exhibit high predictive power for the properties in a wide variety of ordered structures. The present scheme should be systematically applicable to other multicomponent systems.",PERFORMANCE,Article,"AMER PHYSICAL SOC, ONE PHYSICS ELLIPSE, COLLEGE PK, MD 20740-3844 USA","Materials Science,Physics",,3.86,PERFORMANCE,PHYSICAL REVIEW B,https://repository.kulib.kyoto-u.ac.jp/dspace/bitstream/2433/259295/1/PhysRevB.102.174104.pdf,
19,Combinatorial Exploration and Mapping of Phase Transformation in a Ni-Ti-Co Thin Film Library,22,11,641-648,"Al Hasan Naila M.,Hou Huilong,Gao Tieren,Counsell Jonathan,Sarker Suchismita,Thienhaus Sigurd,Walton Edward,Decker Peer,Mehta Apurva,Ludwig Alfred","Al Hasan NM,Hou HL,Gao TR,Counsell J,Sarker S,Thienhaus S,Walton E,Decker P,Mehta A,Ludwig A",Takeuchi I,10.1021/acscombsci.0c00097,University System of Maryland,"Combinatorial synthesis and high-throughput characterization of a Ni-Ti-Co thin film materials library are reported for exploration of reversible martensitic transformation. The library was prepared by magnetron co-sputtering, annealed in vacuum at 500 degrees C without atmospheric exposure, and evaluated for shape memory behavior as an indicator of transformation. Composition, structure, and transformation behavior of the 177 pads in the library were characterized using high-throughput wavelength dispersive spectroscopy (WDS), X-ray photoelectron spectroscopy (XPS), X-ray diffraction (XRD), and four-point probe temperature-dependent resistance (R(T)) measurements. A new, expanded composition space having phase transformation with low thermal hysteresis and Co > 10 at. % is found. Unsupervised machine learning methods of hierarchical dustering were employed to streamline data processing of the large XRD and XPS data sets. Through duster analysis of XRD data, we identified and mapped the constituent structural phases. Composition-structure-property maps for the ternary system are made to correlate the functional properties to the local microstructure and composition the Ni-Ti-Co thin film library.","Ni-Ti-Co,shape memory alloys,elastocaloric cooling,property mapping,machine learning,cluster analysis",Article,"AMER CHEMICAL SOC, 1155 16TH ST, NW, WASHINGTON, DC 20036 USA","Chemistry,Pharmacology & Pharmacy",,3.774,"SHAPE-MEMORY,ALLOYS,MECHANICAL-PROPERTIES,CORROSION,BEHAVIOR,MARTENSITIC-TRANSFORMATION,DEFORMATION-BEHAVIOR,MICROSTRUCTURE,FE,HYSTERESIS,COBALT,CU",ACS COMBINATORIAL SCIENCE,https://www.osti.gov/biblio/1769351,
20,Forecasting the deterioration of cement-based mixtures under sulfuric acid attack using support vector regression based on Bayesian optimization,2,12,,"Wu Linping,Hu Chaoshi,Liu Wei Victor","Wu LP,Hu CS,Liu WV",Liu WV,10.1007/s42452-020-03778-9,University of Alberta,"Immersion test is time-consuming and labor-intensive in evaluating the resistance of concrete against sulfuric acid attack. An alternative way for acid resistance evaluation is to predict the sample deterioration through machine learning methods using a currently available database. However, current predictive models have failed to include testing conditions, which limits their applications to certain testing conditions. Accordingly, predictive models need to be developed to include parameters of both mixture design and testing conditions in the forecasting of deterioration of mortar under sulfuric acid attack. In this study, such predictive models were established using the Bayesian optimized-support vector regression (BO-SVR) algorithm. Prediction errors were calculated, and a superiority test was conducted to evaluate the performance of the proposed BO-SVR models. It was found that the proposed BO-SVR model outperformed the other models in predicting the mass change and the compressive strength. This provides a new way of evaluating the acid resistance of cement-based materials.","Support vector regression,Prediction,Testing conditions,Acid resistance,Mortar",Article,"SPRINGER INTERNATIONAL PUBLISHING AG, GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND",Science & Technology - Other Topics,,,"GREY,RELATIONAL,ANALYSIS,ARTIFICIAL,NEURAL-NETWORK,COMPRESSIVE,STRENGTH,FLY-ASH,ELECTRICITY,CONSUMPTION,CONCRETE,DEGRADATION,SILICA,FUME,PREDICTION,MACHINE,HYPERPARAMETERS",SN APPLIED SCIENCES,https://link.springer.com/content/pdf/10.1007/s42452-020-03778-9.pdf,
21,"Impact of online mental health screening tools on help-seeking, care receipt, and suicidal ideation and suicidal intent: Evidence from internet search behavior in a large U.S. cohort.",,,,",,,,,","Jacobson Nicholas C,Yom-Tov Elad,Lekkas Damien,Heinz Michael,Liu Lili,Barr Paul J",,10.1016/j.jpsychires.2020.11.010,,"INTRODUCTION: Most people with psychiatric illnesses do not receive treatment for almost a decade after disorder onset. Online mental health screens reflect one mechanism designed to shorten this lag in help-seeking, yet there has been limited research on the effectiveness of screening tools in naturalistic settings.DISCUSSION: Online screens may influence help-seeking behavior, suicidal ideation, and suicidal intent. Websites with referrals to in-person treatments could put persons at greater risk of active suicidal intent. Further evaluation using large-scale randomized controlled trials is needed. Copyright © 2020 Elsevier Ltd. All rights reserved.","Internet search behavior; ,Machine learning,; Online screening tool; Suicidal ideation; Suicidal intent",Journal Article,,,,,,,,
22,TOuNN: Topology Optimization using Neural Networks,63,3,1135-1149,"Chandrasekhar Aaditya,Suresh Krishnan","Chandrasekhar A,Suresh K",Suresh K,10.1007/s00158-020-02748-4,University of Wisconsin System,"Neural networks, and more broadly, machine learning techniques, have been recently exploited to accelerate topology optimization through data-driven training and image processing. In this paper, we demonstrate that one can directly execute topology optimization (TO) using neural networks (NN). The primary concept is to use the NN's activation functions to represent the popular Solid Isotropic Material with Penalization (SIMP) density field. In other words, the density function is parameterized by the weights and bias associated with the NN, and spanned by NN's activation functions; the density representation is thus independent of the finite element mesh. Then, by relying on the NN's built-in backpropogation, and a conventional finite element solver, the density field is optimized. Methods to impose design and manufacturing constraints within the proposed framework are described and illustrated. A byproduct of representing the density field via activation functions is that it leads to a crisp and differentiable boundary. The proposed framework is simple to implement and is illustrated through 2D and 3D examples. Some of the unresolved challenges with the proposed framework are also summarized.","Neural networks,Machine learning,Topology optimization",Article,"SPRINGER, ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES","Computer Science,Engineering,Mechanics",,4.715,"CODE,WRITTEN",STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,,
23,"Effect of wood surface roughness on prediction of structural timber properties by infrared spectroscopy using ANFIS, ANN and PLS regression",79,1,101-115,"Ayanleye Samuel,Nasir Vahid,Avramidis Stavros,Cool Julie","Ayanleye S,Nasir V,Avramidis S,Cool J",Nasir V,10.1007/s00107-020-01621-x,University of British Columbia,"Predicting the properties of structural timber using a rapid and reliable non-destructive method is a critical quality control task in production. This study investigates using infrared spectroscopy for predicting the density, moduli of elasticity (MOE) and rupture (MOR) of two structural softwoods. Because the produced timber is sometimes planed during manufacturing resulting in a smooth surface finish, the effect of wood surface roughness on prediction accuracy of timber properties was also investigated. Accordingly, infrared spectroscopy experiments were carried out on Douglas-fir and Western hemlock specimens having a rough and smooth surface. In addition, the effect of the infrared spectroscopy range on the predictive models was studied. Data in the visible infrared (VIS), near-infrared (NIR), and the combined VIS and NIR range were used for properties prediction. The acquired infrared data were processed using principal component analysis (PCA) for data reduction and feature selection. The output of PCA was then fed into an adaptive neuro-fuzzy inference system (ANFIS), multilayer perceptron (MLP) neural network (NN) and partial least square (PLS) regression model. The results showed that the wood surface finish, range of infrared data, and the type of machine learning model impact the prediction accuracy. ANFIS showed superior performance to MLP NN and PLS model for properties prediction. In addition, NIR data acquired from rough surface resulted in better prediction accuracy, which suggests that an infrared spectroscopy test should be performed prior to surface planing. The proposed models yield better accuracy for predicting the MOE and MOR than wood density.","IN-SITU ASSESSMENT,STRESS WAVE EVALUATION,MECHANICAL-PROPERTIES,NIR SPECTROSCOPY,MOISTURE-CONTENT,MICROFIBRIL ANGLE,BLACK SPRUCE,DENSITY,CLASSIFICATION,STIFFNESS",Article,"SPRINGER, ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES","Forestry,Materials Science",,2.164,"IN-SITU,ASSESSMENT,STRESS,WAVE,EVALUATION,MECHANICAL-PROPERTIES,NIR,SPECTROSCOPY,MOISTURE-CONTENT,MICROFIBRIL,ANGLE,BLACK,SPRUCE,DENSITY,CLASSIFICATION,STIFFNESS",EUROPEAN JOURNAL OF WOOD AND WOOD PRODUCTS,,
24,One stage detector (RetinaNet)-based crack detection for asphalt pavements considering pavement distresses and surface objects,11,1,205-222,"Tran Van Phuc,Tran Thai Son,Lee Hyun Jong,Kim Ki Deok,Baek Jongeun,Nguyen Thanh Tu","Tran VP,Tran TS,Lee HJ,Kim KD,Baek J,Nguyen TT",Tran VP,10.1007/s13349-020-00447-8,"IRIS Technol Co, 98 Gunja Dong, Seoul 143747, South Korea.","In this study, a supervised machine learning network model is proposed to detect and classify various types of cracks developed in asphalt pavements, including lane markers. Crack images captured from a digital camera are classified into nine categories following the pavement distress identification manual proposed by the Federal Highways Administration (FHWA). These categories are three different types of cracks, such as fatigue, longitudinal, and transverse cracks with three severity levels of the low, medium, and high for each crack type. To establish a training dataset for crack detection, 1000 images with the original size of 3704 x 10,000 pixels are divided into 20,000 smaller images of 1852 x 1000 pixels image size. The training images are labeled based on the nine categories and trained using an updated version of faster R-CNN called RetinaNet. The trained network model is validated using pavement surface images obtained from 2400 m of two road sections. It is observed from the validation study that the detection and classification accuracy of the trained network model is 84.9% considering both the crack type and severity level. When considering the crack type only, the detection accuracy of the network model is 89.1%.","Pavement distress,Automated distress detection,Crack detection,Deep learning,Faster R-CNN,RetinaNet",Article,"SPRINGER HEIDELBERG, TIERGARTENSTRASSE 17, D-69121 HEIDELBERG, GERMANY",Engineering,,2.67,,JOURNAL OF CIVIL STRUCTURAL HEALTH MONITORING,,
25,Intensity non-uniformity correction in MR imaging using residual cycle generative adversarial network,65,21,,"Dai Xianjin,Lei Yang,Liu Yingzi,Wang Tonghe,Ren Lei,Curran Walter J.,Patel Pretesh,Liu Tian,Yang Xiaofeng","Dai XJ,Lei Y,Liu YZ,Wang TH,Ren L,Curran WJ,Patel P,Liu T,Yang XF",Yang XF,10.1088/1361-6560/abb31f,Emory University,"Correcting or reducing the effects of voxel intensity non-uniformity (INU) within a given tissue type is a crucial issue for quantitative magnetic resonance (MR) image analysis in daily clinical practice. Although having no severe impact on visual diagnosis, the INU can highly degrade the performance of automatic quantitative analysis such as segmentation, registration, feature extraction and radiomics. In this study, we present an advanced deep learning based INU correction algorithm called residual cycle generative adversarial network (res-cycle GAN), which integrates the residual block concept into a cycle-consistent GAN (cycle-GAN). In cycle-GAN, an inverse transformation was implemented between the INU uncorrected and corrected magnetic resonance imaging (MRI) images to constrain the model through forcing the calculation of both an INU corrected MRI and a synthetic corrected MRI. A fully convolution neural network integrating residual blocks was applied in the generator of cycle-GAN to enhance end-to-end raw MRI to INU corrected MRI transformation. A cohort of 55 abdominal patients with T1-weighted MR INU images and their corrections with a clinically established and commonly used method, namely, N4ITK were used as a pair to evaluate the proposed res-cycle GAN based INU correction algorithm. Quantitatively comparisons of normalized mean absolute error (NMAE), peak signal-to-noise ratio (PSNR), normalized cross-correlation (NCC) indices, and spatial non-uniformity (SNU) were made among the proposed method and other approaches. Our res-cycle GAN based method achieved an NMAE of 0.011 +/- 0.002, a PSNR of 28.0 +/- 1.9 dB, an NCC of 0.970 +/- 0.017, and a SNU of 0.298 +/- 0.085. Our proposed method has significant improvements (p < 0.05) in NMAE, PSNR, NCC and SNU over other algorithms including conventional GAN and U-net. Once the model is well trained, our approach can automatically generate the corrected MR images in a few minutes, eliminating the need for manual setting of parameters.","magnetic resonance imaging (MRI),bias field,intensity non-uniformity,deep learning,generative adversarial network (GAN)",Article,"IOP PUBLISHING LTD, TEMPLE CIRCUS, TEMPLE WAY, BRISTOL BS1 6BE, ENGLAND","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,3.554,"BIAS,FIELD,ESTIMATION,RETROSPECTIVE,CORRECTION,FAT-SUPPRESSION,ABDOMINAL,MRI,INHOMOGENEITY,SEGMENTATION,DENSITY,TUMORS,N3",PHYSICS IN MEDICINE AND BIOLOGY,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7934018,
26,Fast spot-scanning proton dose calculation method with uncertainty quantification using a three-dimensional convolutional neural network,65,21,,"Nomura Yusuke,Wang Jeff,Shirato Hiroki,Shimizu Shinichi,Xing Lei","Nomura Y,Wang J,Shirato H,Shimizu S,Xing L",Nomura Y,10.1088/1361-6560/aba164,Hokkaido University,"This study proposes a near-real-time spot-scanning proton dose calculation method with probabilistic uncertainty estimation using a three-dimensional convolutional neural network (3D-CNN). CT images and clinical target volume contours of 215 head and neck cancer patients were collected from a public database. 1484 and 488 plans were extracted for training and testing the 3D-CNN model, respectively. Spot beam data and single-field uniform dose (SFUD) labels were calculated for each plan using an open-source dose calculation toolkit. Variable spot data were converted into a fixed-size volume hereby called a 'peak map' (PM). 300 epochs of end-to-end training was implemented using sets of stopping power ratio and PM as input. Moreover, transfer learning techniques were used to adjust the trained model to SFUD doses calculated with different beam parameters and calculation algorithm using only 7.95% of training data used for the base model. Finally, accuracy of the 3D-CNN-calculated doses and model uncertainty was reviewed with several evaluation metrics. The 3D-CNN model calculates 3D proton dose distributions accurately with a mean absolute error of 0.778 cGyE. The predicted uncertainty is correlated with dose errors at high contrast edges. Averaged Sorensen-Dice similarity coefficients between binarized outputs and ground truths are mostly above 80%. Once the 3D-CNN model was well-trained, it can be efficiently fine-tuned for different proton doses by transfer learning techniques. Inference time for calculating one dose distribution is around 0.8 s for a plan using 1500 spot beams with a consumer grade GPU. A novel spot-scanning proton dose calculation method using 3D-CNN was developed. The 3D-CNN model is able to calculate 3D doses and uncertainty with any SFUD spot data and beam irradiation angles. Our proposed method should be readily extendable to other setups and plans and be useful for dose verification, image-guided proton therapy, or other applications.","deep learning,dose calculation,uncertainty estimation,proton therapy,3D,convolutional neural network",Article,"IOP PUBLISHING LTD, TEMPLE CIRCUS, TEMPLE WAY, BRISTOL BS1 6BE, ENGLAND","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,3.554,"RANGE,UNCERTAINTIES,OPTIMIZATION,THERAPY,ALGORITHM",PHYSICS IN MEDICINE AND BIOLOGY,,
27,Machine-learning predictions of polymer properties with Polymer Genome,128,17,,"Tran Huan Doan,Kim Chiho,Chen Lihua,Chandrasekaran Anand,Batra Rohit,Venkatram Shruti,Kamal Deepak,Lightstone Jordan P.,Gurnani Rishi,Shetty Pranav","Tran HD,Kim C,Chen LH,Chandrasekaran A,Batra R,Venkatram S,Kamal D,Lightstone JP,Gurnani R,Shetty P",Ramprasad R,10.1063/5.0023759,University System of Georgia,"Polymer Genome is a web-based machine-learning capability to perform near-instantaneous predictions of a variety of polymer properties. The prediction models are trained on (and interpolate between) an underlying database of polymers and their properties obtained from first principles computations and experimental measurements. In this contribution, we first provide an overview of some of the critical technical aspects of Polymer Genome, including polymer data curation, representation, learning algorithms, and prediction model usage. Then, we provide a series of pedagogical examples to demonstrate how Polymer Genome can be used to predict dozens of polymer properties, appropriate for a range of applications. This contribution is closed with a discussion on the remaining challenges and possible future directions.","TOTAL-ENERGY CALCULATIONS,MODELS",Article,"AIP Publishing, 1305 WALT WHITMAN RD, STE 300, MELVILLE, NY 11747-4501 USA",Physics,,2.389,"TOTAL-ENERGY,CALCULATIONS,MODELS",JOURNAL OF APPLIED PHYSICS,https://aip.scitation.org/doi/pdf/10.1063/5.0023759,
28,Multiple spin-phonon relaxation pathways in a Kramer single-ion magnet,153,17,,"Lunghi Alessandro,Sanvito Stefano","Lunghi A,Sanvito S",Lunghi A,10.1063/5.0017118,Trinity College Dublin,"We present a first-principles investigation of spin-phonon relaxation in a molecular crystal of Co2+ single-ion magnets. Our study combines electronic structure calculations with machine-learning force fields and unravels the nature of both the Orbach and the Raman relaxation channels in terms of atomistic processes. We find that although both mechanisms are mediated by the excited spin states, the low temperature spin dynamics is dominated by phonons in the THz energy range, which partially suppress the benefit of having a large magnetic anisotropy. This study also determines the importance of intra-molecular motions for both the relaxation mechanisms and paves the way to the rational design of a new generation of single-ion magnets with tailored spin-phonon coupling.",DESIGN,Article,"AMER INST PHYSICS, 1305 WALT WHITMAN RD, STE 300, MELVILLE, NY 11747-4501 USA","Chemistry,Physics",,3.166,DESIGN,JOURNAL OF CHEMICAL PHYSICS,http://www.tara.tcd.ie/bitstream/2262/95653/1/Multiple%20Spin-Phonon.pdf,
29,Deep learning networks on chronic liver disease assessment with fine-tuning of shear wave elastography image sequences,65,21,,"Kagadis George C.,Drazinos Petros,Gatos Ilias,Tsantis Stavros,Papadimitroulas Panagiotis,Spiliopoulos Stavros,Karnabatidis Dimitris,Theotokas Ioannis,Zoumpoulis Pavlos,Hazle John D.","Kagadis GC,Drazinos P,Gatos I,Tsantis S,Papadimitroulas P,Spiliopoulos S,Karnabatidis D,Theotokas I,Zoumpoulis P,Hazle JD",Kagadis GC,10.1088/1361-6560/abae06,University of Patras,"Chronic liver disease (CLD) is currently one of the major causes of death worldwide. If not treated, it may lead to cirrhosis, hepatic carcinoma and death. Ultrasound (US) shear wave elastography (SWE) is a relatively new, popular, non-invasive technique among radiologists. Although many studies have been published validating the SWE technique either in a clinical setting, or by applying machine learning on SWE elastograms, minimal work has been done on comparing the performance of popular pre-trained deep learning networks on CLD assessment. Currently available literature reports suggest technical advancements on specific deep learning structures, with specific inputs and usually on a limited CLD fibrosis stage class group, with limited comparison on competitive deep learning schemes fed with different input types. The aim of the present study is to compare some popular deep learning pre-trained networks using temporally stable and full elastograms, with or without augmentation as well as propose suitable deep learning schemes for CLD diagnosis and progress assessment. 200 liver biopsy validated patients with CLD, underwent US SWE examination. Four images from the same liver area were saved to extract elastograms and processed to exclude areas that were temporally unstable. Then, full and temporally stable masked elastograms for each patient were separately fed into GoogLeNet, AlexNet, VGG16, ResNet50 and DenseNet201 with and without augmentation. The networks were tested for differentiation of CLD stages in seven classification schemes over 30 repetitions using liver biopsy as the reference. All networks achieved maximum mean accuracies ranging from 87.2%-97.4% and area under the receiver operating characteristic curves (AUCs) ranging from 0.979-0.990 while the radiologists had AUCs ranging from 0.800-0.870. ResNet50 and DenseNet201 had better average performance than the other networks. The use of the temporal stability mask led to improved performance on about 50% of inputs and network combinations while augmentation led to lower performance for all networks. These findings can provide potential networks with higher accuracy and better setting in the CLD diagnosis and progress assessment. A larger data set would help identify the best network and settings for CLD assessment in clinical practice.","deep learning,CLD,shear wave,quantitative imaging",Article,"IOP PUBLISHING LTD, TEMPLE CIRCUS, TEMPLE WAY, BRISTOL BS1 6BE, ENGLAND","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,3.554,"TIME,TISSUE,ELASTOGRAPHY,MAGNETIC-RESONANCE,ELASTOGRAPHY,1,BASIC,PRINCIPLES,TRANSIENT,ELASTOGRAPHY,HEPATIC-FIBROSIS,ULTRASOUND,ELASTOGRAPHY,CLINICAL-USE,NONINVASIVE,EVALUATION,EFSUMB,GUIDELINES,WFUMB,GUIDELINES",PHYSICS IN MEDICINE AND BIOLOGY,,
30,Feasibility study of range verification based on proton-induced acoustic signals and recurrent neural network,65,21,,"Yao Songhuan,Hu Zongsheng,Zhang Xiaoke,Lou En,Liang Zhiwen,Wang Yuenan,Peng Hao","Yao SH,Hu ZS,Zhang XK,Lou E,Liang ZW,Wang YN,Peng H",Peng H,10.1088/1361-6560/abaa5e,Wuhan University,"Range verification in proton therapy is a critical quality assurance task. We studied the feasibility of online range verification based on proton-induced acoustic signals, using a bidirectional long-short-term-memory recurrent neural network and various signal processing techniques. Dose distribution of 1D pencil proton beams inside a CT image-based phantom was analytically calculated. The propagation of acoustic signal inside the phantom was modeled using the k-Wave toolbox. For signal processing, five methods were investigated: down-sampling (DS), DS + HT (Hilbert transform), Wavelet decomposition (Wavedec db1, db4 and db20). The performances were quantitatively evaluated in terms of mean absolute error, mean relative error (MRE) and the Bragg peak localization error (Delta(BP)). In addition, the study analyzed the impact of noise levels, the number of sensors, as well as the location of sensors. For the noiseless case (32 sensors), the Wavedec db1 method demonstrates the best performance: Delta BP is less than one pixel and the dose accuracy over the region adjacent to the Bragg peak (MRE50) is similar to 3.04%. With the presence of noise, the Wavedec db1 method demonstrates the best noise immunity, achieving Delta BP less than 1 mm and an MRE50 of similar to 12%. The proposed machine learning framework may become a useful tool allowing for online range verification in proton therapy.","proton therapy,dose verification,acoustic signal,recurrent neural network (RNN),wavelet",Article,"IOP PUBLISHING LTD, TEMPLE CIRCUS, TEMPLE WAY, BRISTOL BS1 6BE, ENGLAND","Engineering,Radiology, Nuclear Medicine & Medical Imaging",,3.554,"BEAM,IRRADIATION,MONTE-CARLO,WATER,EMISSIONS,TISSUE",PHYSICS IN MEDICINE AND BIOLOGY,https://doi.org/10.1088/1361-6560/abaa5e,
31,Artificial intelligence: the silver bullet for sustainable materials development,22,21,7521-7528,"Hardian Rifan,Liang Zhenwen,Zhang Xiangliang,Szekely Gyorgy","Hardian R,Liang ZW,Zhang XL,Szekely G",Szekely G,10.1039/d0gc02956d,King Abdullah University of Science & Technology,"Materials discovery is rapidly revolutionizing all aspects of our lives. However, the design and fabrication of materials are often unsustainable and resource-intensive. Hence, we need a paradigm shift towards designing sustainable materials in silico. Machine learning, a subfield of artificial intelligence (AI), is emerging within the sustainability agenda because it promises to benefit science and engineering through improved quality, performance, and predictive power. Here we present a new methodology to extend the application of AI to develop materials in an environmentally friendly way. We demonstrate successful materials development by combining design of experiments with a new machine learning module that comprises a support vector machine, an evolutionary algorithm, and a desirability function. We use our AI-based method to realize the sustainable electrochemical synthesis of a ZIF-8 metal-organic framework and explore the hyperdimensional relationship between the synthesis parameters, product qualities, and process sustainability. The presented AI-based methodology paves the way for solving the challenge of the materials fabrication-sustainability nexus, and facilitates the paradigm shift from the wet lab to the wired lab.","GREEN CHEMISTRY,METAL,FRAMEWORKS",Article,"ROYAL SOC CHEMISTRY, THOMAS GRAHAM HOUSE, SCIENCE PARK, MILTON RD, CAMBRIDGE CB4 0WF, CAMBS, ENGLAND","Chemistry,Science & Technology - Other Topics",,10.607,"GREEN,CHEMISTRY,METAL,FRAMEWORKS",GREEN CHEMISTRY,https://repository.kaust.edu.sa/bitstream/10754/665538/1/d0gc02956d.pdf,
32,Entrack: Probabilistic Spherical Regression with Entropy Regularization for Fiber Tractography,129,3,656-680,"Wegmayr Viktor,Buhmann Joachim M.","Wegmayr V,Buhmann JM",Wegmayr V,10.1007/s11263-020-01384-1,ETH Zurich,"White matter tractography, based on diffusion-weighted magnetic resonance images, is currently the only available in vivo method to gather information on the structural brain connectivity. The low resolution of diffusion MRI data suggests to employ probabilistic methods for streamline reconstruction, i.e., for fiber crossings. We propose a general probabilistic model for spherical regression based on the Fisher-von-Mises distribution, which efficiently estimates maximum entropy posteriors of local streamline directions with machine learning methods. The optimal precision of posteriors for streamlines is determined by an information-theoretic technique, the expected log-posterior agreement concept. It relies on the requirement that the posterior distributions of streamlines, inferred on retest measurements of the same subject, should yield stable results within the precision determined by the noise level of the data source.","Diffusion MRI,Brain,Tractography,Machine Learning,Maximum-entropy inference,Algorithm validation",Article,"SPRINGER, VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS",Computer Science,,13.284,"WHITE-MATTER,CONNECTOME",INTERNATIONAL JOURNAL OF COMPUTER VISION,https://link.springer.com/content/pdf/10.1007/s11263-020-01384-1.pdf,
33,A new method of predicting the saturation pressure of oil reservoir and its application,45,55,30244-30253,"Yu Guoyi,Xu Feng,Cui Yingzhi,Li Xiangling,Kang Chujuan,Lu Cheng,Li Siyu,Bai Lin,Du Shuheng","Yu GY,Xu F,Cui YZ,Li XL,Kang CJ,Lu C,Li SY,Bai L,Du SH",Xu F,10.1016/j.ijhydene.2020.08.042,"China Natl Oil & Gas Explorat & Dev Co Ltd CNODC, Energy & Fuel Exploitat, 6 Fuchengmen North St, Beijing, Peoples R China.","Saturation pressure is a vital parameter of oil reservoir which can reflect the oilfield characteristics and determine the oilfield development process, and it is determined by experiments in the laboratory in general. However, there was only one well with saturation pressure test in this target reservoir, and it is necessary to determine whether this parameter is right or not.
In this work, we present a new method for quickly determining saturation pressure using machine learning algorithms, including random forest regressor (RF), support vector machine (SVM), decision trees (DT), and artificial neural network (ANN or NN). Using these approaches, saturation pressure was obtained by using the initial solution gas-oil ratio (GOR), temperature, API gravity and other reservoir-fluid data available in the oilfields. Compared with the empirical formula for saturation pressure calculation, the calculated result shows that the accuracy given from machine learning is higher than that from other formulas at home and abroad, and has a good match with the lab test. On the basis of the calculated saturation pressure, it can determine whether the reservoir enters into the stage of dissolved gas drive or not, which also provides the basis for maintaining the reservoir pressure by water injection in advance, rational development decision-making and work over measures.
This approach above can provide technical guidance for predicting the saturation pressure in the development of different kinds of reservoirs, including the sandstone reservoirs and carbonate reservoirs. (C) 2020 Hydrogen Energy Publications LLC. Published by Elsevier Ltd. All rights reserved.","Oil reservoir,Saturation pressure,Random forest,Decision tree,ANN,Empirical formula",Article,"PERGAMON-ELSEVIER SCIENCE LTD, THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND","Chemistry,Electrochemistry,Energy & Fuels",,5.242,"RANDOM,FOREST,HETEROGENEITY",INTERNATIONAL JOURNAL OF HYDROGEN ENERGY,,
34,Suitability of low-field nuclear magnetic resonance (LF-NMR) combining with back propagation artificial neural network (BP-ANN) to predict printability of polysaccharide hydrogels 3D printing,56,5,2264-2272,"Guo Chaofan,Zhang Min,Chen Huizhi","Guo CF,Zhang M,Chen HZ",Zhang M,10.1111/ijfs.14844,Jiangnan University,"The water state of polysaccharide hydrogel is closely related to its rheological properties, which is the critical parameter determining the printability of 3D printing. Low-field nuclear magnetic resonance (LF-NMR) results showed that T-23 (free water) values of polysaccharide hydrogels gradually decreased with increasing concentration, while the viscosity values exhibited a opposite trend. As concentration increased, the printed objects gradually changed from the fluid, which cannot form a shape, to a stable shape and then to a hydrogel of low fluidity that cannot print normally. Hydrogels became hard to extrude smoothly at the piston pressure higher than 428716 Pa. Back propagation artificial neural network (BP-ANN) nonlinear models established by taking the fingerprint LF-NMR signal as input variables yielded good predicting ability of the piston pressure values (R-adj(2) = 0.982) and printing scores (R-adj(2) = 0.988). The BP-ANN model based on LF-NMR might be a promising approach to quickly predict the 3D printability of polysaccharide hydrogels.","3D printing,BP&#8208,ANN,LF&#8208,NMR,polysaccharide hydrogel,printability",Article,"WILEY, 111 RIVER ST, HOBOKEN 07030-5774, NJ USA",Food Science & Technology,,3.408,"RHEOLOGICAL,PROPERTIES,INTELLIGENT,DETECTION,MASHED,POTATOES,GUM,WATER,RICE,STABILITY,ALGINATE",INTERNATIONAL JOURNAL OF FOOD SCIENCE AND TECHNOLOGY,,
35,An intelligent control strategy for cancer cells reduction in patients with chronic myelogenous leukaemia using the reinforcement learning and considering side effects of the drug,38,3,,"Noori Amin,Alfi Alireza,Noori Ghazaleh","Noori A,Alfi A,Noori G",Alfi A,10.1111/exsy.12655,Shahrood University of Technology,"Chronic Myelogenous Leukaemia (CML) is a haematopoietic stem cells disease with complex dynamical behaviour. One of the effective factors in treating patients is to determine the appropriate drug dosage. A physician should test the different drug dosages through trial and error in order to find its optimal value. This procedure is normally a time-consuming and error-prone task that can even be harmful. The contribution of this paper is to design an intelligent control strategy, which can be used to help physicians, by finding a drug treatment regimen to minimize the number of cancer cells for a CML patient. In this paper, the eligibility traces algorithm and Q-learning approach are adopted as sub-optimal methods for progressively reducing the population of cancer cells. In addition, the injected dosage of the drug has improved, compared with previous methods. More importantly, the proposed method is followed by the reduction in side effects of the drug. The advantage of the backward view and the previous states investigation are applied in the Eligibility Traces algorithm. These effects increase the learning procedure and decrease the growth rate of cancer cells and total dosage of the injected drug during the treatment period of time. The proposed strategy mitigates the side effects of the drug on the normal cells.","chronic myelogenous leukaemia,eligibility traces,optimal control,reinforcement learning,side effect",Article,"WILEY, 111 RIVER ST, HOBOKEN 07030-5774, NJ USA",Computer Science,,2.457,"CHRONIC,MYELOID-LEUKEMIA,MATHEMATICAL-MODEL,INSIGHTS",EXPERT SYSTEMS,,
36,Extracting Natech Reports from Large Databases: Development of a Semi-Intelligent Natech Identification Framework,11,6,735-750,"Luo Xiaolong,Cruz Ana Maria,Tzioutzios Dimitrios","Luo XL,Cruz AM,Tzioutzios D",Cruz AM,10.1007/s13753-020-00314-6,Kyoto University,"Natural hazard-triggered technological accidents (Natechs) refer to accidents involving releases of hazardous materials (hazmat) triggered by natural hazards. Huge economic losses, as well as human health and environmental problems are caused by Natechs. In this regard, learning from previous Natechs is critical for risk management. However, due to data scarcity and high uncertainty concerning such hazards, it becomes a serious challenge for risk managers to detect Natechs from large databases, such as the National Response Center (NRC) database. As the largest database of hazmat release incidents, the NRC database receives hazmat release reports from citizens in the United States. However, callers often have incomplete details about the incidents they are reporting. This results in many records having incomplete information. Consequently, it is quite difficult to identify and extract Natechs accurately and efficiently. In this study, we introduce machine learning theory into the Natech retrieving research, and a Semi-Intelligent Natech Identification Framework (SINIF) is proposed in order to solve the problem. We tested the suitability of two supervised machine learning algorithms, namely the Long Short-Term Memory (LSTM) and the Convolutional Neural Network (CNN), and selected the former for the development of the SINIF. According to the results, the SINIF is efficient (a total number of 826,078 records were analyzed) and accurate (the accuracy is over 0.90), while 32,841 Natech reports between 1990 and 2017 were extracted from the NRC database. Furthermore, the majority of those Natech reports (97.85%) were related to meteorological phenomena, with hurricanes (24.41%), heavy rains (19.27%), and storms (18.29%) as the main causes of these reported Natechs. Overall, this study suggests that risk managers can benefit immensely from SINIF in analyzing Natech data from large databases efficiently.","Data extraction method,Machine learning,Natechs,Natural hazards,NRC database",Article; Proceedings Paper,"SPRINGER, ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES","Geology,Meteorology & Atmospheric Sciences,Water Resources",,4.67,"NATURAL,HAZARDS,QUANTITATIVE,ASSESSMENT,TEXT,CATEGORIZATION,RISK-MANAGEMENT,IMPACT,CLASSIFICATION,EARTHQUAKE,MACHINE,OIL",INTERNATIONAL JOURNAL OF DISASTER RISK SCIENCE,https://link.springer.com/content/pdf/10.1007/s13753-020-00314-6.pdf,
37,Deep learning-enabled multi-organ segmentation in whole-body mouse scans,11,1,,"Schoppe Oliver,Pan Chenchen,Coronel Javier,Mai Hongcheng,Rong Zhouyi,Todorov Mihail Ivilinov,Muskes Annemarie,Navarro Fernando,Li Hongwei,Ertuerk Ali","Schoppe O,Pan CC,Coronel J,Mai H,Rong ZY,Todorov MI,Muskes A,Navarro F,Li HW,Erturk A",Schoppe O; Menze BH,10.1038/s41467-020-19449-7,Technical University of Munich,"Whole-body imaging of mice is a key source of information for research. Organ segmentation is a prerequisite for quantitative analysis but is a tedious and error-prone task if done manually. Here, we present a deep learning solution called AIMOS that automatically segments major organs (brain, lungs, heart, liver, kidneys, spleen, bladder, stomach, intestine) and the skeleton in less than a second, orders of magnitude faster than prior algorithms. AIMOS matches or exceeds the segmentation quality of state-of-the-art approaches and of human experts. We exemplify direct applicability for biomedical research for localizing cancer metastases. Furthermore, we show that expert annotations are subject to human error and bias. As a consequence, we show that at least two independently created annotations are needed to assess model performance. Importantly, AIMOS addresses the issue of human bias by identifying the regions where humans are most likely to disagree, and thereby localizes and quantifies this uncertainty for improved downstream analysis. In summary, AIMOS is a powerful open-source tool to increase scalability, reduce bias, and foster reproducibility in many areas of biomedical research. Organ segmentation of whole-body mouse images is essential for quantitative analysis, but is tedious and error-prone. Here the authors develop a deep learning pipeline to segment major organs and the skeleton in volumetric whole-body scans in less than a second, and present probability maps and uncertainty estimates.","IN-VIVO,BIODISTRIBUTION,TOMOGRAPHY,TISSUE,ORGANS",Article,"NATURE RESEARCH, HEIDELBERGER PLATZ 3, BERLIN, 14197, GERMANY",Science & Technology - Other Topics,,15.805,"IN-VIVO,BIODISTRIBUTION,TOMOGRAPHY,TISSUE,ORGANS",NATURE COMMUNICATIONS,https://www.nature.com/articles/s41467-020-19449-7.pdf,
38,A convolutional neural network-based learning approach to acute lymphoblastic leukaemia detection with automated feature extraction,58,12,3113-3121,"Anwar Shamama,Alam Afrin","Anwar S,Alam A",Anwar S,10.1007/s11517-020-02282-x,Birla Institute of Technology Mesra,"Leukaemia is a type of blood cancer which mainly occurs when bone marrow produces excess white blood cells in our body. This disease not only affects adult but also is a common cancer type among children. Treatment of leukaemia depends on its type and how far the disease has spread in the body. Leukaemia is classified into two types depending on how rapidly it grows: acute and chronic leukaemia. The early diagnosis of this disease is vital for effective treatment and recovery. This paper presents an automated diagnostic system to detect acute lymphoblastic leukaemia (ALL) using a convolutional neural network (CNN) model. The model uses labeled microscopic blood smear images to detect the malignant leukaemia cells. The current work uses data obtained from the Acute Lymphoblastic Leukaemia Image DataBase (ALL_IDB) and performs various data augmentation techniques to increase the number of training data which in effect reduces the over-training problem. The model has been trained on 515 images using a fivefold validation technique achieving an accuracy of 95.54% and further tested on the remaining 221 images achieving almost 100% accuracy during most of the trials, maintaining an average of 99.5% accuracy. The method does not need any pre-processing or segmentation technique and works efficiently on raw data. This method can, hence, prove profitable for pathologist in diagnosing ALL efficiently.","Leukaemia,Feature extraction,Convolutional neural network,Augmentation,Acute lymphoblastic leukaemia (ALL)",Article,"SPRINGER HEIDELBERG, TIERGARTENSTRASSE 17, D-69121 HEIDELBERG, GERMANY","Computer Science,Engineering,Mathematical & Computational Biology,Medical Informatics",,2.61,"BLOOD,SEGMENTATION,CLASSIFICATION,CELLS",MEDICAL & BIOLOGICAL ENGINEERING & COMPUTING,,
39,Feature selection and risk prediction for patients with coronary artery disease using data mining,58,12,3123-3140,"Md Idris Nashreen,Chiam Yin Kia,Varathan Kasturi Dewi,Wan Ahmad Wan Azman,Chee Kok Han,Liew Yih Miin","Idris NM,Chiam YK,Varathan KD,Ahmad WAW,Chee KH,Liew YM",Chiam YK,10.1007/s11517-020-02268-9,Universiti Malaya,"Coronary artery disease (CAD) is an important cause of mortality across the globe. Early risk prediction of CAD would be able to reduce the death rate by allowing early and targeted treatments. In healthcare, some studies applied data mining techniques and machine learning algorithms on the risk prediction of CAD using patient data collected by hospitals and medical centers. However, most of these studies used all the attributes in the datasets which might reduce the performance of prediction models due to data redundancy. The objective of this research is to identify significant features to build models for predicting the risk level of patients with CAD. In this research, significant features were selected using three methods (i.e., Chi-squared test, recursive feature elimination, and Embedded Decision Tree). Synthetic Minority Over-sampling Technique (SMOTE) oversampling technique was implemented to address the imbalanced dataset issue. The prediction models were built based on the identified significant features and eight machine learning algorithms, utilizing Acute Coronary Syndrome (ACS) datasets provided by National Cardiovascular Disease Database (NCVD) Malaysia. The prediction models were evaluated and compared using six performance evaluation metrics, and the top-performing models have achieved AUC more than 90%.","Data mining,Prediction model,Classification algorithms,Feature selection,Heart disease prediction,Coronary artery disease",Article,"SPRINGER HEIDELBERG, TIERGARTENSTRASSE 17, D-69121 HEIDELBERG, GERMANY","Computer Science,Engineering,Mathematical & Computational Biology,Medical Informatics",,2.61,"DIAGNOSIS,SYSTEM",MEDICAL & BIOLOGICAL ENGINEERING & COMPUTING,,
40,,,,,,,,,,,,,,,,,,,,
41,Machine learning property prediction for organic photovoltaic devices,6,1,,"Meftahi Nastaran,Klymenko Mykhailo,Christofferson Andrew J.,Bach Udo,Winkler David A.,Russo Salvy P.","Meftahi N,Klymenko M,Christofferson AJ,Bach U,Winkler DA,Russo SP",Meftahi N; Russo SP,10.1038/s41524-020-00429-w,Royal Melbourne Institute of Technology (RMIT),"Organic photovoltaic (OPV) materials are promising candidates for cheap, printable solar cells. However, there are a very large number of potential donors and acceptors, making selection of the best materials difficult. Here, we show that machine-learning approaches can leverage computationally expensive DFT calculations to estimate important OPV materials properties quickly and accurately. We generate quantitative relationships between simple and interpretable chemical signature and one-hot descriptors and OPV power conversion efficiency (PCE), open circuit potential (V-oc), short circuit density (J(sc)), highest occupied molecular orbital (HOMO) energy, lowest unoccupied molecular orbital (LUMO) energy, and the HOMO-LUMO gap. The most robust and predictive models could predict PCE (computed by DFT) with a standard error of +/- 0.5 for percentage PCE for both the training and test set. This model is useful for pre-screening potential donor and acceptor materials for OPV applications, accelerating design of these devices for green energy applications.","SIGNATURE MOLECULAR DESCRIPTOR,ROBUST QSAR MODELS,SOLAR-CELLS,SELECTION,VALENCE,DESIGN,ACCURACY,KERNEL,CORE",Article,"NATURE RESEARCH, HEIDELBERGER PLATZ 3, BERLIN, 14197, GERMANY","Chemistry,Materials Science",,13.993,"SIGNATURE,MOLECULAR,DESCRIPTOR,ROBUST,QSAR,MODELS,SOLAR-CELLS,SELECTION,VALENCE,DESIGN,ACCURACY,KERNEL,CORE",NPJ COMPUTATIONAL MATERIALS,https://www.nature.com/articles/s41524-020-00429-w.pdf,
42,,,,,,,,,,,,,,,,,,,,
43,Automatic prognosis of lung cancer using heterogeneous deep learning models for nodule detection and eliciting its morphological features,51,4,2471-2484,"Wang Weilun,Charkborty Goutam","Wang WL,Charkborty G",Wang WL,10.1007/s10489-020-01990-z,Iwate Prefectural University,"Among cancers, lung cancer has the highest morbidity, and mortality rate. The survival probability of lung cancer patients depends largely on an early diagnosis. For predicting lung cancer from low-dose Computed Tomography (LDCT) scans, computer-aided diagnosis (CAD) system needs to detect all pulmonary nodules, and combine their morphological features to assess the risk of cancer. An automatic lung cancer prognosis system is proposed. The existing CAD system is only for nodule detection. Actually, presence of a nodule does not mean cancer. Depending on its morphological features, the risk that it eventually would develop into cancer, is different. The motivation of the work is to propose a complete lung cancer prognosis system. It consists of 2 cascaded modules: nodule detection module and cancer risk evaluation module. In nodule detection module, two object detection algorithms are ensembled to minimize missing detection, i.e., maximize recall performance. They are based on 3D convolutional neural network (3D-CNN), and our recently proposed model of recurrent neural network (RNN). As they extract features in completely different ways, we call them heterogeneous deep learning models. By ensembing them, we could achieve much better recall performance compared to individual detectors. In cancer risk evaluation module, 3D-CNN based models are trained to evaluate the grade of malady of morphological features of pulmonary nodules. It will also provide medically interpretable intermediate information. Finally, a regression model is trained to match the ground truth labels describing morbidity grade of the CT-Scan. In this work, 13 features from the highest risk nodule is used to evaluate the risk of lung cancer. We also identify the subset of structural and morphological features which are strongly related to grading decision, labelled by oncologist. The final system could obtain a low logloss of 0.408.","Lung cancer,Automatic prognosis,Low-dose computed tomography,3-D convolution neural network,Recurrent neural network",Article,"SPRINGER, VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS",Computer Science,,4.602,"PULMONARY,NODULES",APPLIED INTELLIGENCE,,
44,A Review of Deep Learning on Medical Image Analysis,26,1,351-380,"Wang Jian,Zhu Hengde,Wang Shui-Hua,Zhang Yu-Dong","Wang J,Zhu HD,Wang SH,Zhang YD",Wang SH; Zhang YD,10.1007/s11036-020-01672-7,University of Leicester,"Compared with common deep learning methods (e.g., convolutional neural networks), transfer learning is characterized by simplicity, efficiency and its low training cost, breaking the curse of small datasets. Medical image analysis plays an indispensable role in both scientific research and clinical diagnosis. Common medical image acquisition methods include Computer Tomography (CT), Magnetic Resonance Imaging (MRI), Ultrasound (US), X-Ray, etc. Although these medical imaging methods can be applied for non-invasive qualitative and quantitative analysis of patients-compared with image datasets in other computer vision fields such like faces-medical images, especially its labeling, is still scarce and insufficient. Therefore, more and more researchers adopted transfer learning for medical image processing. In this study, after reviewing one hundred representative papers from IEEE, Elsevier, Google Scholar, Web of Science and various sources published from 2000 to 2020, a comprehensive review is presented, including (i) structure of CNN, (ii) background knowledge of transfer learning, (iii) different types of strategies performing transfer learning, (iv) application of transfer learning in various sub-fields of medical image analysis, and (v) discussion on the future prospect of transfer learning in the field of medical image analysis. Through this review paper, beginners could receive an overall and systematic knowledge of transfer learning application in medical image analysis. And policymaker of related realm will benefit from the summary of the trend of transfer learning in medical imaging field and may be encouraged to make policy positive to the future development of transfer learning in the field of medical image analysis.","Transfer learning,Medical image analysis,CT,Deep learning,MRI,Convolutional neural networks,Fine-tuning,Feature extractor,Artificial intelligence",Review,"SPRINGER, ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES","Computer Science,Telecommunications",,2.783,"CONVOLUTIONAL,NEURAL-NETWORKS,CLASSIFICATION,SEGMENTATION,LUNG,CLASSIFIERS,FEATURES,FUSION",MOBILE NETWORKS & APPLICATIONS,,
45,All-condition measuring methods for field performance of room air conditioner,180,,,"Yang Zixu);,Ding Lianrui);,Xiao Hansong);,Zhang Guohui);,Wang Baolong);,Shi Wenxing","Yang ZX,Ding LR,Xiao HS,Zhang GH,Wang BL,Shi WX",Shi WX,10.1016/j.applthermaleng.2020.115887,Tsinghua University,"Room air conditioners (RACs) are used worldwide and have become one of the major energy consumers in buildings. Because the actual field performance directly affects the energy consumption, the actual performance of an RAC must be determined. The compressor energy conservation (CEC) method exhibits better adaptability to the existing field test methods. However, the refrigerant mass flowrate cannot be calculated accurately under two-phase suction conditions, which is common for current high-energy-efficiency RACs. Therefore, two all-condition field measurement methods based on the CEC and compressor efficiency (CE) methods are proposed, and the isentropic and volumetric efficiency under wet compression working conditions is predicted with datadriven models trained using a back-propagation neural network (BPNN). The accuracy verification indicates that the relative errors of the refrigerant flowrate and cooling/heating capacities of both methods are approximately within +/- 15%, which is sufficient for the field performance evaluation; in addition, the CEC-compressor volumetric efficiency (CVE) method performs better. Furthermore, the proposed methods are compared, and the accuracy of the dynamic conditions is analyzed to improve the calculation performance.","Room air conditioner,Field test,Compressor energy conservation,Neural network,Energy efficiency",Article,"PERGAMON-ELSEVIER SCIENCE LTD, THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND","Thermodynamics,Energy & Fuels,Engineering,Mechanics",,5.175,"NEURAL-NETWORKS,SYSTEM",APPLIED THERMAL ENGINEERING,,
46,High-Dimensional Precision Medicine From Patient-Derived Xenografts,,,,"Rashid Naim U.,Luckett Daniel J.,Chen Jingxiang,Lawson Michael T.,Wang Longshaokan,Zhang Yunshu,Laber Eric B.,Liu Yufeng,Yeh Jen Jen,Zeng Donglin","Rashid NU,Luckett DJ,Chen JX,Lawson MT,Wang LSK,Zhang YS,Laber EB,Liu YF,Yeh JJ,Zeng DL",Kosorok MR,10.1080/01621459.2020.1828091,University of North Carolina,"The complexity of human cancer often results in significant heterogeneity in response to treatment. Precision medicine offers the potential to improve patient outcomes by leveraging this heterogeneity. Individualized treatment rules (ITRs) formalize precision medicine as maps from the patient covariate space into the space of allowable treatments. The optimal ITR is that which maximizes the mean of a clinical outcome in a population of interest. Patient-derived xenograft (PDX) studies permit the evaluation of multiple treatments within a single tumor, and thus are ideally suited for estimating optimal ITRs. PDX data are characterized by correlated outcomes, a high-dimensional feature space, and a large number of treatments. Here we explore machine learning methods for estimating optimal ITRs from PDX data. We analyze data from a large PDX study to identify biomarkers that are informative for developing personalized treatment recommendations in multiple cancers. We estimate optimal ITRs using regression-based (Q-learning) and direct-search methods (outcome weighted learning). Finally, we implement a superlearner approach to combine multiple estimated ITRs and show that the resulting ITR performs better than any of the input ITRs, mitigating uncertainty regarding user choice. Our results indicate that PDX data are a valuable resource for developing individualized treatment strategies in oncology. for this article are available online.","Biomarkers,Deep learning autoencoders,Machine learning,Outcome weighted learning,Precision medicine,Q-learning",Article; Early Access,"AMER STATISTICAL ASSOC, 732 N WASHINGTON ST, ALEXANDRIA, VA 22314-1943 USA",Mathematics,,6.086,"TUMOR,XENOGRAFTS,HETEROGENEITY,MODELS,TUBULIN",JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8451968,
47,Delineation of ischemic lesion from brain MRI using attention gated fully convolutional network,11,1,3-13,"Karthik R.,Radhakrishnan Menaka,Rajalakshmi R.,Raymann Joel","Karthik R,Radhakrishnan M,Rajalakshmi R,Raymann J",Radhakrishnan M,10.1007/s13534-020-00178-1,VIT Chennai,"Precise delineation of the ischemic lesion from unimodal Magnetic Resonance Imaging (MRI) is a challenging task due to the subtle intensity difference between the lesion and normal tissues. Hence, multispectral MRI modalities are used for characterizing the properties of brain tissues. Traditional lesion detection methods rely on extracting significant hand-engineered features to differentiate normal and abnormal brain tissues. But the identification of those discriminating features is quite complex, as the degree of differentiation varies according to each modality. This can be addressed well by Convolutional Neural Networks (CNN) which supports automatic feature extraction. It is capable of learning the global features from images effectively for image classification. But it loses the context of local information among the pixels that need to be retained for segmentation. Also, it must provide more emphasis on the features of the lesion region for precise reconstruction. The major contribution of this work is the integration of attention mechanism with a Fully Convolutional Network (FCN) to segment ischemic lesion. This attention model is applied to learn and concentrate only on salient features of the lesion region by suppressing the details of other regions. Hence the proposed FCN with attention mechanism was able to segment ischemic lesion of varying size and shape. To study the effectiveness of attention mechanism, various experiments were carried out on ISLES 2015 dataset and a mean dice coefficient of 0.7535 was obtained. Experimental results indicate that there is an improvement of 5% compared to the existing works.","Deep neural network,FCN,Attention,Ischemic lesion segmentation,MRI",Article,"SPRINGERNATURE, CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND",Engineering,,,"SEGMENTATION,STROKE",BIOMEDICAL ENGINEERING LETTERS,,
48,A generative flow-based model for volumetric data augmentation in 3D deep learning for computed tomographic colonography,16,1,81-89,"Uemura Tomoki,Nappi Janne J.,Ryu Yasuji,Watari Chinatsu,Kamiya Tohru,Yoshida Hiroyuki","Uemura T,Nappi JJ,Ryu Y,Watari C,Kamiya T,Yoshida H",Yoshida H,10.1007/s11548-020-02275-z,Harvard University,"Purpose Deep learning can be used for improving the performance of computer-aided detection (CADe) in various medical imaging tasks. However, in computed tomographic (CT) colonography, the performance is limited by the relatively small size and the variety of the available training datasets. Our purpose in this study was to develop and evaluate a flow-based generative model for performing 3D data augmentation of colorectal polyps for effective training of deep learning in CADe for CT colonography. Methods We developed a 3D-convolutional neural network (3D CNN) based on a flow-based generative model (3D Glow) for generating synthetic volumes of interest (VOIs) that has characteristics similar to those of the VOIs of its training dataset. The 3D Glow was trained to generate synthetic VOIs of polyps by use of our clinical CT colonography case collection. The evaluation was performed by use of a human observer study with three observers and by use of a CADe-based polyp classification study with a 3D DenseNet. Results The area-under-the-curve values of the receiver operating characteristic analysis of the three observers were not statistically significantly different in distinguishing between real polyps and synthetic polyps. When trained with data augmentation by 3D Glow, the 3D DenseNet yielded a statistically significantly higher polyp classification performance than when it was trained with alternative augmentation methods. Conclusion The 3D Glow-generated synthetic polyps are visually indistinguishable from real colorectal polyps. Their application to data augmentation can substantially improve the performance of 3D CNNs in CADe for CT colonography. Thus, 3D Glow is a promising method for improving the performance of deep learning in CADe for CT colonography.","Generative models,Data augmentation,Deep learning,Computer-aided detection,Virtual colonoscopy,Artificial intelligence",Article,"SPRINGER HEIDELBERG, TIERGARTENSTRASSE 17, D-69121 HEIDELBERG, GERMANY","Engineering,Radiology, Nuclear Medicine & Medical Imaging,Surgery",,,"CT,COLONOGRAPHY,COLONOSCOPY,NEOPLASIA,ACCURACY,POLYPS",INTERNATIONAL JOURNAL OF COMPUTER ASSISTED RADIOLOGY AND SURGERY,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7822776,
49,Artificial intelligence-based radiotherapy machine parameter optimization using reinforcement learning,47,12,6140-6150,"Hrinivich William Thomas,Lee Junghoon","Hrinivich WT,Lee J",Lee J,10.1002/mp.14544,Johns Hopkins University,"Purpose To develop and evaluate a volumetric modulated arc therapy (VMAT) machine parameter optimization (MPO) approach based on deep-Q reinforcement learning (RL) capable of finding an optimal machine control policy using previous prostate cancer patient CT scans and contours, and applying the policy to new cases to rapidly produce deliverable VMAT plans in a simplified beam model.
Methods A convolutional deep-Q network was employed to control the dose rate and multileaf collimator of a C-arm linear accelerator model using the current dose distribution and machine parameter state as input. A Q-value was defined as the discounted cumulative cost based on dose objectives, and experience-replay RL was performed to determine a policy to minimize the Q-value. A two-dimensional network design was employed which optimized each opposing leaf pair independently while monitoring the corresponding dose plane blocked by those leaves. This RL approach was applied to CT and contours from 40 retrospective prostate cancer patients. The dataset was split into training (15 patients) and validation (5 patients) groups to optimize the network, and its performance was tested in an independent cohort of 20 patients by comparing RL-based dose distributions to conformal arcs and clinical intensity modulated radiotherapy (IMRT) delivering a prescription dose of 78 Gy in 40 fractions.
Results Mean +/- SD execution time of the RL VMAT optimization was 1.5 +/- 0.2 s per slice. In the test cohort, mean +/- SD (P-value) planning target volume (PTV), bladder, and rectum dose were 80.5 +/- 2.0 Gy (P < 0.001), 44.2 +/- 14.6 Gy (P < 0.001), and 43.7 +/- 11.1 Gy (P < 0.001) for RL VMAT compared to 81.6 +/- 1.1 Gy, 51.6 +/- 12.9 Gy, and 36.0 +/- 12.3 Gy for clinical IMRT.
Conclusions RL was applied to VMAT MPO using clinical patient contours without independently optimized treatment plans for training and achieved comparable target and normal tissue dose to clinical plans despite the application of a relatively simple network design originally developed for video-game control. These results suggest that extending a RL approach to a full three-dimensional beam model could enable rapid artificial intelligence-based optimization of deliverable treatment plans, reducing the time required for radiotherapy planning without requiring previous plans for training.","artificial intelligence,deep&#8208,Q learning,optimization,reinforcement learning,treatment planning,volumetric modulated arc therapy",Article,"WILEY, 111 RIVER ST, HOBOKEN 07030-5774, NJ USA","Radiology, Nuclear Medicine & Medical Imaging",,3.767,"VOLUMETRIC,MODULATED,ARC,TREATMENT,PLAN,OPTIMIZATION,THERAPY,PREDICTION,IMRT",MEDICAL PHYSICS,,
50,"Unsupervised Learning-Based Multiscale Model of Thermochemistry in 1,3,5-Trinitro-1,3,5-triazinane (RDX)",124,44,9141-9155,"Sakano Michael N.,Hamed Ahmed,Kober Edward M.,Grilli Nicolo,Hamilton Brenden W.,Islam Md Mahbubul,Koslowski Marisol,Strachan Alejandro","Sakano MN,Hamed A,Kober EM,Grilli N,Hamilton BW,Islam MM,Koslowski M,Strachan A",Strachan A,10.1021/acs.jpca.0c07320,Purdue University System,"The response of high-energy-density materials to thermal or mechanical insults involves coupled thermal, mechanical, and chemical processes with disparate temporal and spatial scales that no single model can capture. Therefore, we developed a multiscale model for 1,3,5-trinitro-1,3,5-triazinane, RDX, where a continuum description is informed by reactive and nonreactive molecular dynamics (MD) simulations to describe chemical reactions and thermal transport. Reactive MD simulations under homogeneous isothermal and adiabatic conditions are used to develop a reduced-order chemical kinetics model. Coarse graining is done using unsupervised learning via non-negative matrix factorization. Importantly, the components resulting from the analysis can be interpreted as reactants, intermediates, and products, which allows us to write kinetics equations for their evolution. The kinetics parameters are obtained from isothermal MD simulations over a wide temperature range, 1200-3000 K, and the heat evolved is calibrated from adiabatic simulations. We validate the continuum model against MD simulations by comparing the evolution of a cylindrical hotspot 10 nm in diameter. We find excellent agreement in the time evolution of the hotspot temperature fields both in cases where quenching is observed and at higher temperatures for which the hotspot transitions into a deflagration wave. The validated continuum model is then used to assess the criticality of hotspots involving scales beyond the reach of atomistic simulations that are relevant to detonation initiation.",,Article,"AMER CHEMICAL SOC, 1155 16TH ST, NW, WASHINGTON, DC 20036 USA","Chemistry,Physics",,2.725,,JOURNAL OF PHYSICAL CHEMISTRY A,,
51,,,,,,,,,,,,,,,,,,,,
52,,,,,,,,,,,,,,,,,,,,
53,Localized prediction of tissue outcome in acute ischemic stroke patients using diffusion- and perfusion-weighted MRI datasets,15,11,,"Grosser Malte,Gellissen Susanne,Borchert Patrick,Sedlacik Jan,Nawabi Jawed,Fiehler Jens,Forkert Nils D.","Grosser M,Gellissen S,Borchert P,Sedlacik J,Nawabi J,Fiehler J,Forkert ND",Grosser M,10.1371/journal.pone.0241917,University of Hamburg,"Background
An accurate prediction of tissue outcome in acute ischemic stroke patients is of high interest for treatment decision making. To date, various machine learning models have been proposed that combine multi-parametric imaging data for this purpose. However, most of these machine learning models were trained using voxel information extracted from the whole brain, without taking differences in susceptibility to ischemia into account that exist between brain regions. The aim of this study was to develop and evaluate a local tissue outcome prediction approach, which makes predictions using locally trained machine learning models and thus accounts for regional differences.
Material and methods
Multi-parametric MRI data from 99 acute ischemic stroke patients were used for the development and evaluation of the local tissue outcome prediction approach. Diffusion (ADC) and perfusion parameter maps (CBF, CBV, MTT, Tmax) and corresponding follow-up lesion masks for each patient were registered to the MNI brain atlas. Logistic regression (LR) and random forest (RF) models were trained employing a local approach, which makes predictions using models individually trained for each specific voxel position using the corresponding local data. A global approach, which uses a single model trained using all voxels of the brain, was used for comparison. Tissue outcome predictions resulting from the global and local RF and LR models, as well as a combined (hybrid) approach were quantitatively evaluated and compared using the area under the receiver operating characteristic curve (ROC AUC), the Dice coefficient, and the sensitivity and specificity metrics.
Results
Statistical analysis revealed the highest ROC AUC and Dice values for the hybrid approach. With 0.872 (ROC AUC; LR) and 0.353 (Dice; RF), these values were significantly higher (p < 0.01) than the values of the two other approaches. In addition, the local approach achieved the highest sensitivity of 0.448 (LR). Overall, the hybrid approach was only outperformed in sensitivity (LR) by the local approach and in specificity by both other approaches. However, in these cases the effect sizes were comparatively small.
Conclusion
The results of this study suggest that using locally trained machine learning models can lead to better lesion outcome prediction results compared to a single global machine learning model trained using all voxel information independent of the location in the brain.","PERFORMANCE,THRESHOLDS,MISMATCH,MODELS,TIME",Article,"PUBLIC LIBRARY SCIENCE, 1160 BATTERY STREET, STE 100, SAN FRANCISCO, CA 94111 USA",Science & Technology - Other Topics,,3.788,"PERFORMANCE,THRESHOLDS,MISMATCH,MODELS,TIME",PLOS ONE,https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0241917&type=printable,
54,Measuring adverse events following hip arthroplasty surgery using administrative data without relying on ICD-codes,15,11,,"Magneli Martin,Unbeck Maria,Rogmark Cecilia,Skoldenberg Olof,Gordon Max","Magneli M,Unbeck M,Rogmark C,Skoldenberg O,Gordon M",Magneli M,10.1371/journal.pone.0242008,Danderyds Hospital,"Introduction
Measure and monitor adverse events (AEs) following hip arthroplasty is challenging. The aim of this study was to create a model for measuring AEs after hip arthroplasty using administrative data, such as length of stay and readmissions, with equal or better precision than an ICD-code based model.
Materials and methods
This study included 1 998 patients operated with an acute or elective hip arthroplasty in a national multi-centre study. We collected AEs within 90 days following surgery with retrospective record review. Additional data came from the Swedish Hip Arthroplasty Register, the Swedish National Patient Register and the Swedish National Board of Health and Welfare. We made a 2:1 split of the data into a training and a holdout set. We used the training set to train different machine learning models to predict if a patient had sustained an AE or not. After training and cross-validation we tested the best performing model on the holdout-set. We compared the results with an established ICD-code based measure for AEs.
Results
The best performing model was a logistic regression model with four natural age splines. The variables included in the model were as follows: length of stay at the orthopaedic department, discharge to acute care, age, number of readmissions and ED visits. The sensitivity and specificity for the new model was 23 and 90% for AE within 30 days, compared with 5 and 94% for the ICD-code based model. For AEs within 90 days the sensitivity and specificity were 31% and 89% compared with 16% and 92% for the ICD-code based model.
Conclusion
We conclude that a prediction model for AEs following hip arthroplasty surgery, relying on administrative data without ICD-codes is more accurate than a model based on ICD-codes.","LENGTH-OF-STAY,ORTHOPEDICS",Article,"PUBLIC LIBRARY SCIENCE, 1160 BATTERY STREET, STE 100, SAN FRANCISCO, CA 94111 USA",Science & Technology - Other Topics,,3.788,"LENGTH-OF-STAY,ORTHOPEDICS",PLOS ONE,https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0242008&type=printable,
55,Logic-in-memory based on an atomically thin semiconductor,587,7832,72-+,"Migliato Marega Guilherme,Zhao Yanfei,Avsar Ahmet,Wang Zhenyu,Tripathi Mukesh,Radenovic Aleksandra,Kis Andras","Marega GM,Zhao YF,Avsar A,Wang ZY,Tripathi M,Radenovic A,Kis A",Kis A,10.1038/s41586-020-2861-0,Ecole Polytechnique Federale de Lausanne,"The growing importance of applications based on machine learning is driving the need to develop dedicated, energy-efficient electronic hardware. Compared with von Neumann architectures, which have separate processing and storage units, brain-inspired in-memory computing uses the same basic device structure for logic operations and data storage(1-3), thus promising to reduce the energy cost of data-centred computing substantially(4). Although there is ample research focused on exploring new device architectures, the engineering of material platforms suitable for such device designs remains a challenge. Two-dimensional materials(5,6) such as semiconducting molybdenum disulphide, MoS2, could be promising candidates for such platforms thanks to their exceptional electrical and mechanical properties(7-9). Here we report our exploration of large-area MoS2 as an active channel material for developing logic-in-memory devices and circuits based on floating-gate field-effect transistors (FGFETs). The conductance of our FGFETs can be precisely and continuously tuned, allowing us to use them as building blocks for reconfigurable logic circuits in which logic operations can be directly performed using the memory elements. After demonstrating a programmable NOR gate, we show that this design can be simply extended to implement more complex programmable logic and a functionally complete set of operations. Our findings highlight the potential of atomically thin semiconductors for the development of next-generation low-power electronics.
Logic operations and reconfigurable circuits are demonstrated that can be directly implemented using memory elements based on floating-gate field-effect transistors with monolayer MoS2 as the active channel material.","MONOLAYER MOS2,NONVOLATILE,ELECTRONICS,TRANSISTORS,OPERATIONS,CIRCUITS",Article,"NATURE RESEARCH, HEIDELBERGER PLATZ 3, BERLIN, 14197, GERMANY",Science & Technology - Other Topics,,54.637,"MONOLAYER,MOS2,NONVOLATILE,ELECTRONICS,TRANSISTORS,OPERATIONS,CIRCUITS",NATURE,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7116757,
56,Machine Learning Tools to Predict Hot Injection Syntheses Outcomes for II-VI and IV-VI Quantum Dots,124,44,24298-24305,"Baum Fabio,Pretto Tatiane,Koeche Ariadne,Leite Santos Marcos Jose","Baum F,Pretto T,Koche A,Santos MJL",Baum F,10.1021/acs.jpcc.0c05993,Instituto Federal do Rio Grande do Sul (IFRS),"In order to allow quantum dots with the desired physical and chemical properties, the fine control and prediction of size during chemical syntheses is a challenge that must be addressed. In this work, we applied machine learning algorithms, with information extracted from scientific papers, to identify the most important variables in the synthesis of CdSe, CdS, PbS, PbSe, and ZnSe quantum dots. From the random forest and gradient boosting machine algorithms, the most influential parameters on the final diameter of the quantum dots were the time of reaction, temperature, and metal precursors. Our models were applied to suggest the best reaction parameters for a desired quantum dot size. This methodology shall contribute to the quantum dot community to save time and money while reaching the proper material conditions for their applications.",,Article,"AMER CHEMICAL SOC, 1155 16TH ST, NW, WASHINGTON, DC 20036 USA","Chemistry,Science & Technology - Other Topics,Materials Science",,4.467,,JOURNAL OF PHYSICAL CHEMISTRY C,,
57,Machine Learning Analysis of the Cerebrovascular Thrombi Proteome in Human Ischemic Stroke: An Exploratory Study,11,,,"Dargazanli Cyril,Zub Emma,Deverdun Jeremy,Decourcelle Mathilde,de Bock Frederic,Labreuche Julien,Lefevre Pierre-Henri,Gascou Gregory,Derraz Imad,Riquelme Bareiro Carlos","Dargazanli C,Zub E,Deverdun J,Decourcelle M,de Bock F,Labreuche J,Lefevre PH,Gascou G,Derraz I,Bareiro CR",Dargazanli C; Costalat V; Marchi N,10.3389/fneur.2020.575376,Institut National de la Sante et de la Recherche Medicale (Inserm),"Objective: Mechanical retrieval of thrombotic material from acute ischemic stroke patients provides a unique entry point for translational research investigations. Here, we resolved the proteomes of cardioembolic and atherothrombotic cerebrovascular human thrombi and applied an artificial intelligence routine to examine protein signatures between the two selected groups.
Methods: We specifically used n = 32 cardioembolic and n = 28 atherothrombotic diagnosed thrombi from patients suffering from acute stroke and treated by mechanical thrombectomy. Thrombi proteins were successfully separated by gel-electrophoresis. For each thrombi, peptide samples were analyzed by nano-flow liquid chromatography coupled to tandem mass spectrometry (nano-LC-MS/MS) to obtain specific proteomes. Relative protein quantification was performed using a label-free LFQ algorithm and all dataset were analyzed using a support-vector-machine (SVM) learning method. Data are available via ProteomeXchange with identifier PXD020398. Clinical data were also analyzed using SVM, alone or in combination with the proteomes.
Results: A total of 2,455 proteins were identified by nano-LC-MS/MS in the samples analyzed, with 438 proteins constantly detected in all samples. SVM analysis of LFQ proteomic data delivered combinations of three proteins achieving a maximum of 88.3% for correct classification of the cardioembolic and atherothrombotic samples in our cohort. The coagulation factor XIII appeared in all of the SVM protein trios, associating with cardioembolic thrombi. A combined SVM analysis of the LFQ proteome and clinical data did not deliver a better discriminatory score as compared to the proteome only.
Conclusion: Our results advance the portrayal of the human cerebrovascular thrombi proteome. The exploratory SVM analysis outlined sets of proteins for a proof-of-principle characterization of our cohort cardioembolic and atherothrombotic samples. The integrated analysis proposed herein could be further developed and retested on a larger patients population to better understand stroke origin and the associated cerebrovascular pathophysiology.","stroke,thrombus,cerebrovascular,mechanical thrombectomy,proteome,neuroradiology",Article,"FRONTIERS MEDIA SA, AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND",Neurosciences & Neurology,,4.189,"COAGULATION-FACTOR-XIII,CLASSIFICATION",FRONTIERS IN NEUROLOGY,https://hal.umontpellier.fr/hal-03226108/file/fneur-11-575376.pdf,
58,A brief review of new data analysis methods of laser-induced breakdown spectroscopy: machine learning,,,,"Zhang Dianxin,Zhang Hong,Zhao Yong,Chen Yongliang,Ke Chuan,Xu Tao,He Yaxiong","Zhang DX,Zhang H,Zhao Y,Chen YL,Ke C,Xu T,He YX",Zhang H,10.1080/05704928.2020.1843175,Southwest Jiaotong University,"Laser-induced breakdown spectroscopy (LIBS) is a technology of content analysis and composition analysis based on the atomic excitation and emission spectrum of materials. It has been intense activity in the field because of its advantages such as fast detection speed, no environmental limitation and no sample pretreatment. The low accuracy of LIBS is a primary problem in current applications, and the better data analysis methods is the key to solve this problem. Recently, machine learning algorithms significantly improve the accuracy of LIBS compared with traditional analysis methods. Therefore, the researchers gradually begin to pay attention to the application of machine learning algorithms in the LIBS data analysis. It is a programming method to study how computers simulate the learning process of human beings to acquire new knowledge and skills and continuously improve their performance. It is widely used in data analysis, pattern recognition, artificial intelligence and other fields. Here, we introduce the basic principle of LIBS and machine learning algorithms, review the research situation and progress of the application of machine learning algorithms to LIBS, and put forward the problems and challenges of its application.","LIBS,laser-induced breakdown spectroscopy,machine learning,atomic emission",Review; Early Access,"TAYLOR & FRANCIS INC, 530 WALNUT STREET, STE 850, PHILADELPHIA, PA 19106 USA","Instruments & Instrumentation,Spectroscopy",,5.771,"SUPPORT,VECTOR,MACHINE,PARTIAL,LEAST-SQUARES,QUANTITATIVE-ANALYSIS,ACCURACY,IMPROVEMENT,IRON-ORE,DISCRIMINATION,LIBS,CLASSIFICATION,IDENTIFICATION,METALS",APPLIED SPECTROSCOPY REVIEWS,,
59,,,,,,,,,,,,,,,,,,,,
60,Visual Question Answering via Combining Inferential Attention and Semantic Space Mapping,207,,,"Liu Yun,Zhang Xiaoming,Huang Feiran,Zhou Zhibo,Zhao Zhonghua,Li Zhoujun","Liu Y,Zhang XM,Huang FR,Zhou ZB,Zhao ZH,Li ZJ",Zhang XM,10.1016/j.knosys.2020.106339,Beihang University,"Visual Question Answering (VQA) has emerged and aroused widespread interest in recent years. Its purpose is to explore the close correlations between the image and question for answer inference. We have two observations about the VQA task: (1) the number of newly defined answers is ever-growing, which means that answer prediction on pre-defined labeled answers may lead to errors, as some unlabeled answers may be the right choice to the question-image pairs; (2) in the process of answering visual questions, the gradual change of human attention has an important guiding role in exploring the correlations between images and questions. Based on these observations, we propose a novel model for VQA, i.e., combining Inferential Attention and Semantic Space Mapping (IASSM). Specifically, our model has two salient aspects: (1) a semantic space shared by both the labeled and unlabeled answers is constructed to learn new answers, where the joint embedding of a question and the corresponding image is mapped and clustered around the answer exemplar; (2) a novel inferential attention model is designed to simulate the learning process of human attention to explore the correlations between the image and question. It focuses on the more important question words and image regions associated with the question. Both the inferential attention and the semantic space mapping modules are integrated into an end-to-end framework to infer the answer. Experiments performed on two public VQA datasets and our newly constructed dataset show the superiority of IASSM compared with existing methods. (C) 2020 Elsevier B.V. All rights reserved.","Visual Question Answering,Inferential attention,Semantic space mapping",Article,"ELSEVIER, RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS",Computer Science,,7.842,,KNOWLEDGE-BASED SYSTEMS,,
61,"Printed, Wireless, Soft Bioelectronics and Deep Learning Algorithm for Smart Human-Machine Interfaces",12,44,49398-49406,"Kwon Young-Tae,Kim Hojoong,Mahmood Musa,Kim Yun-Soung,Demolder Carl,Yeo Woon-Hong","Kwon YT,Kim H,Mahmood M,Kim YS,Demolder C,Yeo WH",Yeo WH,10.1021/acsami.0c14193,University System of Georgia,"Recent advances in flexible materials and wearable electronics offer a noninvasive, high-fidelity recording of biopotentials for portable healthcare, disease diagnosis, and machine interfaces. Current device-manufacturing methods, however, still heavily rely on the conventional cleanroom microfabrication that requires expensive, time-consuming, and complicated processes. Here, we introduce an additive nanomanufacturing technology that explores a contactless direct printing of aerosol nanomaterials and polymers to fabricate stretchable sensors and multilayered wearable electronics. Computational and experimental studies prove the mechanical flexibility and reliability of soft electronics, considering direct mounting to the deformable human skin with a curvilinear surface. The dry, skin-conformal graphene biosensor, without the use of conductive gels and aggressive tapes, offers an enhanced biopotential recording on the skin and multiple uses (over ten times) with consistent measurement of electromyograms. The combination of soft bioelectronics and deep learning algorithm allows classifying six classes of muscle activities with an accuracy of over 97%, which enables wireless, real-time, continuous control of external machines such as a robotic hand and a robotic arm. Collectively, the comprehensive study of nanomaterials, flexible mechanics, system integration, and machine learning shows the potential of the printed bioelectronics for portable, smart, and persistent human-machine interfaces.","additive nanomanufacturing,printed bioelectronics,deep learning algorithm,human-machine interface,electromyograms (EMGs)",Article,"AMER CHEMICAL SOC, 1155 16TH ST, NW, WASHINGTON, DC 20036 USA","Science & Technology - Other Topics,Materials Science",,9.57,,ACS APPLIED MATERIALS & INTERFACES,,
62,Introducing Biomedisa as an open-source online platform for biomedical image segmentation,11,1,,"Loesel Philipp D.,van de Kamp Thomas,Jayme Alejandra,Ershov Alexey,Farago Tomas,Pichler Olaf,Jerome Nicholas Tan,Aadepu Narendar,Bremer Sabine,Chilingaryan Suren A.","Losel PD,van de Kamp T,Jayme A,Ershov A,Farago T,Pichler O,Jerome NT,Aadepu N,Bremer S,Chilingaryan SA",Losel PD,10.1038/s41467-020-19303-w,Ruprecht Karls University Heidelberg,"We present Biomedisa, a free and easy-to-use open-source online platform developed for semi-automatic segmentation of large volumetric images. The segmentation is based on a smart interpolation of sparsely pre-segmented slices taking into account the complete underlying image data. Biomedisa is particularly valuable when little a priori knowledge is available, e.g. for the dense annotation of the training data for a deep neural network. The platform is accessible through a web browser and requires no complex and tedious configuration of software and model parameters, thus addressing the needs of scientists without substantial computational expertise. We demonstrate that Biomedisa can drastically reduce both the time and human effort required to segment large images. It achieves a significant improvement over the conventional approach of densely pre-segmented slices with subsequent morphological interpolation as well as compared to segmentation tools that also consider the underlying image data. Biomedisa can be used for different 3D imaging modalities and various biomedical applications. Manual segmentation of biological images is a time-consuming task. Here the authors present Biomedisa, an open-source online platform for segmentation of large volumetric images starting from sparsely presegmented slices.",MRI,Article,"NATURE PORTFOLIO, HEIDELBERGER PLATZ 3, BERLIN, 14197, GERMANY",Science & Technology - Other Topics,,15.805,MRI,NATURE COMMUNICATIONS,https://publikationen.bibliothek.kit.edu/1000125797/91989403,
63,Neuronal diversity and convergence in a visual system developmental atlas,589,7840,88-U93,"ozel Mehmet Neset,Simon Felix,Jafari Shadi,Holguera Isabel,Chen Yen-Chung,Benhra Najate,El-Danaf Rana Naja,Kapuralin Katarina,Malin Jennifer Amy,Konstantinides Nikolaos","Ozel MN,Simon F,Jafari S,Holguera I,Chen YC,Benhra N,El-Danaf RN,Kapuralin K,Malin JA,Konstantinides N",Konstantinides N; Desplan C,10.1038/s41586-020-2879-3,New York University,"Deciphering how neuronal diversity is established and maintained requires a detailed knowledge of neuronal gene expression throughout development. In contrast to mammalian brains(1,2), the large neuronal diversity of the Drosophila optic lobe(3) and its connectome(4-6) are almost completely characterized. However, a molecular characterization of this neuronal diversity, particularly during development, has been lacking. Here we present insights into brain development through a nearly complete description of the transcriptomic diversity of the optic lobes of Drosophila. We acquired the transcriptome of 275,000 single cells at adult and at five pupal stages, and built a machine-learning framework to assign them to almost 200 cell types at all time points during development. We discovered two large neuronal populations that wrap neuropils during development but die just before adulthood, as well as neuronal subtypes that partition dorsal and ventral visual circuits by differential Wnt signalling throughout development. Moreover, we show that the transcriptomes of neurons that are of the same type but are produced days apart become synchronized shortly after their production. During synaptogenesis we also resolved neuronal subtypes that, although differing greatly in morphology and connectivity, converge to indistinguishable transcriptomic profiles in adults. Our datasets almost completely account for the known neuronal diversity of the Drosophila optic lobes, and serve as a paradigm to understand brain development across species.
The neuronal diversity of the Drosophila optic lobe is described throughout pupal development by single-cell sequencing, leading to the discovery of transient extrinsic neurons and a dorsoventral asymmetry of the visual circuits.","DROSOPHILA,EXPRESSION,NOTCH,DIFFERENTIATION,NEUROGENESIS,CIRCUITS,CELLS,MAP",Article,"NATURE RESEARCH, HEIDELBERGER PLATZ 3, BERLIN, 14197, GERMANY",Science & Technology - Other Topics,,54.637,"DROSOPHILA,EXPRESSION,NOTCH,DIFFERENTIATION,NEUROGENESIS,CIRCUITS,CELLS,MAP",NATURE,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7790857,
64,Structure-Based Synthesizability Prediction of Crystals Using Partially Supervised Learning,142,44,18836-18843,"Jang Jidon,Gu Geun Ho,Noh Juhwan,Kim Juhwan,Jung Yousung","Jang J,Gu GH,Noh J,Kim J,Jung Y",Jung Y,10.1021/jacs.0c07384,Korea Advanced Institute of Science & Technology (KAIST),"Predicting the synthesizability of inorganic materials is one of the major challenges in accelerated material discovery. A widely employed approximate approach is to consider the thermodynamic decomposition stability due to its simplicity of computing, but it is notorious for either producing too many candidates or missing important metastable materials. These results, however, are not unexcepted since the synthesizability is a complex phenomenon, and the thermodynamic stability is just one contributor. Here, we suggest a machine-learning model to quantify the probability of synthesis based on the partially supervised learning of materials database. We adapted the positive and unlabeled machine learning (PU learning) by implementing the graph convolutional neural network as a classifier in which the model outputs crystal-likeness scores (CLscore). The model shows 87.4% true positive (CLscore > 0.5) prediction accuracy for the test set of experimentally reported cases (9356 materials) in the Materials Project. We further validated the model by predicting the synthesizability of newly reported experimental materials in the last 5 years (2015-2019) with an 86.2% true positive rate using the model trained with the database as of the end of year 2014. Our analysis shows that our model captures the structural motif for synthesizability beyond what is possible by E-hull. We find that 71 materials among the top 100 high-scoring virtual materials have indeed been previously synthesized in the literature. With the proposed data-driven metric of the crystal-likeness score, high-throughput virtual screenings and generative models can benefit significantly by effectively reducing the chemical space that needs to be explored experimentally in the future toward more rational materials design.",DESIGN,Article,"AMER CHEMICAL SOC, 1155 16TH ST, NW, WASHINGTON, DC 20036 USA",Chemistry,,15.801,DESIGN,JOURNAL OF THE AMERICAN CHEMICAL SOCIETY,,
65,Machine Learning for Advanced Additive Manufacturing,3,5,1541-1556,"Jin Zeqing,Zhang Zhizhou,Demir Kahraman,Gu Grace X.","Jin ZQ,Zhang ZZ,Demir K,Gu GX",Gu GX,10.1016/j.matt.2020.08.023,University of California System,"Increasing demand for the fabrication of components with complex designs has spurred a revolution in manufacturing methods. Additive manufacturing stands out as a promising technology when it comes to prototyping multi-functional and multi-material designs. However, challenges still exist in the additive manufacturing process, such as mismatched material properties, lack of build consistency, and pervasive imperfections in the printed part. These inherent challenges can be avoided by implementing algorithms to detect imperfections and modulate printing parameters in real time. In this paper, several algorithms, with a focus on machine learning methods, are reviewed and explored to systematically tackle the three main stages of the additive manufacturing process: geometrical design, process parameter configuration, and in situ anomaly detection. Current challenges and future opportunities for algorithmically driven additive manufacturing processes, as well as potential applications to other manufacturing methods, are also discussed.","SELF-SUPPORTING STRUCTURES,MULTIMATERIAL TOPOLOGY OPTIMIZATION,PROCESS PARAMETERS,STRUCTURE DESIGN,DEFECT DETECTION,COMPOSITES,SIMULATION,STRESS,MODELS,MATTER",Review,"ELSEVIER, RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS",Materials Science,,15.589,"SELF-SUPPORTING,STRUCTURES,MULTIMATERIAL,TOPOLOGY,OPTIMIZATION,PROCESS,PARAMETERS,STRUCTURE,DESIGN,DEFECT,DETECTION,COMPOSITES,SIMULATION,STRESS,MODELS,MATTER",MATTER,,
66,Advances in neural networks and potential for their application to steel metallurgy,36,17,1805-1819,Smith J. L.,Smith JL,Smith JL,10.1080/02670836.2020.1839206,University of Cambridge,"This review provides a timely exploration of several novel neural network (NN) architectures and learning methods, following a concise overview of the fundamentals of NNs and some important associated challenges. There are many benefits to using NNs, including deep learning models, in scientific research and, by understanding novel techniques better suited to certain applications, this benefit can be maximised. Finally, a few developed and emerging alternative learning paradigms are surveyed for their potential benefit to future research. The reviewed literature and accompanying discussion are of generic value well beyond steel metallurgy, and there is much to be gained from assessing methods used in other areas of materials science and further afield in order to apply them to steel metallurgy.","Deep learning,steel,metallurgy,modelling,machine learning,neural network,generative adversarial network",Article,"TAYLOR & FRANCIS LTD, 2-4 PARK SQUARE, MILTON PARK, ABINGDON OR14 4RN, OXON, ENGLAND","Materials Science,Metallurgy & Metallurgical Engineering",,2.475,"SOFT,COMPUTING,TECHNIQUES,ARTIFICIAL-INTELLIGENCE,FATIGUE,LIFE,TEMPERATURE,MARTENSITE,PREDICTION,HARDNESS,IMPACT,CARBON",MATERIALS SCIENCE AND TECHNOLOGY,,
67,Multiplexing the aperture of a metasurface: inverse design via deep-learning-forward genetic algorithm,53,45,,"Zhu Ruichao,Qiu Tianshuo,Wang Jiafu,Sui Sai,Li Yongfeng,Feng Mingde,Ma Hua,Qu Shaobo","Zhu RC,Qiu TS,Wang JF,Sui S,Li YF,Feng MD,Ma H,Qu SB",Wang JF,10.1088/1361-6463/aba64f,Air Force Engineering University,"The modern design of advanced functional materials or surfaces has increasingly undergone miniaturization and integration, so as to implement multiple functions using the same aperture. Metasurfaces, as an emerging kind of artificial functional surface, have provided unprecedented freedom in manipulating electomagnetic (EM) waves upon two-dimensional surfaces. It is desirable that the aperture of metasurfaces can be multiplexed with a number of functions for EM waves. Based on previous research, an artificial neural network-forward design can achieve an inverse design. In this paper, we propose an inverse design method of multiplexing metasurface apertures. A deep learning network (DLN) is trained to serve as a forward model for a genetic algorithm (GA), that is, a deep-leaning-forward genetic algorithm (DLF-GA). With the DLF-GA model, the phase of meta-atoms can be predicted for orthogonally linearly polarized waves simultaneously. The DLN was trained by a data set consisting of 70 000 samples with an accuracy of 95% on the test data. To demonstrate the competency of this method, we demonstrate the design of a multiplexed metasurface that can achieve focusing and diffuse scattering for polarization. The metasurface, which consists of 24 x 24 meta-atoms, can be generated monolithically with the input phase profile. A prototype was fabricated and measured. The predicted, simulated, and measured results are well consistent and underscore the validity of this inverse design method. This method provides an efficient and accurate method for the fast design of multiplexed metasurfaces and will find applications in microwave engineering such as in satellite communications.","multiplexed aperture,metasurface,deep learning,genetic algorithm,multifunction,inverse design",Article,"IOP PUBLISHING LTD, TEMPLE CIRCUS, TEMPLE WAY, BRISTOL BS1 6BE, ENGLAND",Physics,,3.041,,JOURNAL OF PHYSICS D-APPLIED PHYSICS,,
68,The 2020 magnetism roadmap,53,45,,"Vedmedenko E. Y.,Kawakami R. K.,Sheka D. D.,Gambardella P.,Kirilyuk A.,Hirohata A.,Binek C.,Chubykalo-Fesenko O.,Sanvito S.,Kirby B. J.","Vedmedenko EY,Kawakami RK,Sheka DD,Gambardella P,Kirilyuk A,Hirohata A,Binek C,Chubykalo-Fesenko O,Sanvito S,Kirby BJ",Berger A,10.1088/1361-6463/ab9d98,"CIC NanoGUNE BRTA, E-20018 Donostia San Sebastian, Spain.","Andreas BergerCICnanoGUNE BRTA Following the success and relevance of the 2014 and 2017 Magnetism Roadmap articles, this 2020 Magnetism Roadmap edition takes yet another timely look at newly relevant and highly active areas in magnetism research. The overall layout of this article is unchanged, given that it has proved the most appropriate way to convey the most relevant aspects of today's magnetism research in a wide variety of sub-fields to a broad readership. A different group of experts has again been selected for this article, representing both the breadth of new research areas, and the desire to incorporate different voices and viewpoints. The latter is especially relevant for thistype of article, in which one's field of expertise has to be accommodated on two printed pages only, so that personal selection preferences are naturally rather more visible than in other types of articles. Most importantly, the very relevant advances in the field of magnetism research in recent years make the publication of yet another Magnetism Roadmap a very sensible and timely endeavour, allowing its authors and readers to take another broad-based, but concise look at the most significant developments in magnetism, their precise status, their challenges, and their anticipated future developments. While many of the contributions in this 2020 Magnetism Roadmap edition have significant associations with different aspects of magnetism, the general layout can nonetheless be classified in terms of three main themes: (i) phenomena, (ii) materials and characterization, and (iii) applications and devices. While these categories are unsurprisingly rather similar to the 2017 Roadmap, the order is different, in that the 2020 Roadmap considers phenomena first, even if their occurrences are naturally very difficult to separate from the materials exhibiting such phenomena. Nonetheless, the specifically selected topics seemed to be best displayed in the order presented here, in particular, because many of the phenomena or geometries discussed in (i) can be found or designed into a large variety of materials, so that the progression of the article embarks from more general concepts to more specific classes of materials in the selected order. Given that applications and devices are based on both phenomena and materials, it seemed most appropriate to close the article with the application and devices section (iii) once again. The 2020 Magnetism Roadmap article contains 14 sections, all of which were written by individual authors and experts, specifically addressing a subject in terms of its status, advances, challenges and perspectives in just two pages. Evidently, this two-page format limits the depth to which each subject can be described. Nonetheless, the most relevant and key aspects of each field are touched upon, which enables the Roadmap as whole to give its readership an initial overview of and outlook into a wide variety of topics and fields in a fairly condensed format. Correspondingly, the Roadmap pursues the goal of giving each reader a brief reference frame of relevant and current topics in modern applied magnetism research, even if not all sub-fields can be represented here. The first block of this 2020 Magnetism Roadmap, which is focussed on (i) phenomena, contains five contributions, which address the areas of interfacial Dzyaloshinskii-Moriya interactions, and two-dimensional and curvilinear magnetism, as well as spin-orbit torque phenomena and all optical magnetization reversal.
All of these contributions describe cutting edge aspects of rather fundamental physical processes and properties, associated with new and improved magnetic materials' properties, together with potential developments in terms of future devices and technology. As such, they form part of a widening magnetism 'phenomena reservoir' for utilization in applied magnetism and related device technology. The final block (iii) of this article focuses on such applications and device-related fields in four contributions relating to currently active areas of research, which are of course utilizing magnetic phenomena to enable specific functions. These contributions highlight the role of magnetism or spintronics in the field of neuromorphic and reservoir computing, terahertz technology, and domain wall-based logic. One aspect common to all of these application-related contributions is that they are not yet being utilized in commercially available technology; it is currently still an open question, whether or not such technological applications will be magnetism-based at all in the future, or if other types of materials and phenomena will yet outperform magnetism. This last point is actually a very good indication of the vibrancy of applied magnetism research today, given that it demonstrates that magnetism research is able to venture into novel application fields, based upon its portfolio of phenomena, effects and materials. This materials portfolio in particular defines the central block (ii) of this article, with its five contributions interconnecting phenomena with devices, for which materials and the characterization of their properties is the decisive discriminator between purely academically interesting aspects and the true viability of real-life devices, because only available materials and their associated fabrication and characterization methods permit reliable technological implementation. These five contributions specifically address magnetic films and multiferroic heterostructures for the purpose of spin electronic utilization, multi-scale materials modelling, and magnetic materials design based upon machine-learning, as well as materials characterization via polarized neutron measurements. As such, these contributions illustrate the balanced relevance of research into experimental and modelling magnetic materials, as well the importance of sophisticated characterization methods that allow for an ever-more refined understanding of materials. As a combined and integrated article, this 2020 Magnetism Roadmap is intended to be a reference point for current, novel and emerging research directions in modern magnetism, just as its 2014 and 2017 predecessors have been in previous years.","applied magnetism,magnetic materials,magnetic phenomena,novel applications of magnetism",Article,"IOP PUBLISHING LTD, TEMPLE CIRCUS, TEMPLE WAY, BRISTOL BS1 6BE, ENGLAND",Physics,,3.041,"DYNAMICS,DRIVEN,SPIN,DYNAMICS,FILMS,STATE,MAGNETIZATION,REVERSAL,EMITTERS,DESIGN,LOGIC,BEAM",JOURNAL OF PHYSICS D-APPLIED PHYSICS,https://pure.mpg.de/pubman/item/item_3252689_2/component/file_3253225/Vedmedenko_2020_J._Phys._D%20_Appl._Phys._53_453001.pdf,
69,Development of Hepatitis Disease Detection System by Exploiting Sparsity in Linear Support Vector Machine to Improve Strength of AdaBoost Ensemble Model,2020,,,"Akbar Wasif,Wu Wei-ping,Saleem Sehrish,Farhan Muhammad,Saleem Muhammad Asim,Javeed Ashir,Ali Liaqat","Akbar W,Wu WP,Saleem S,Farhan M,Saleem MA,Javeed A,Ali L",Akbar W,10.1155/2020/8870240,University of Electronic Science & Technology of China,"Hepatitis disease is a deadliest disease. The management and diagnosis of hepatitis disease is expensive and requires high level of human expertise which poses challenges for the health care system in underdeveloped and developing countries. Hence, development of automated methods for accurate prediction of hepatitis disease is inevitable. In this paper, we develop a diagnostic system which hybridizes a linear support vector machine (SVM) model with adaptive boosting (AdaBoost) model. We exploit sparsity in linear SVM that is caused by L1 regularization. The sparse L1-regularized SVM is capable of eliminating redundant or irrelevant features from feature space. After filtering features through the sparse linear SVM, the output of the SVM is applied to the AdaBoost ensemble model which is used for classification purposes. Two types of numerical experiments are performed on the clinical features of hepatitis disease collected from UCI machine learning repository. In the first experiment, only conventional AdaBoost model is used, while in the second experiment, a feature vector is applied to the sparse linear SVM before its application to the AdaBoost model. Simulation results demonstrate that the strength of a conventional AdaBoost model is enhanced by 6.39% by the proposed method, and its time complexity is also reduced. In addition, the proposed method shows better performance than many previously developed methods for hepatitis disease prediction.","PRINCIPAL COMPONENT ANALYSIS,DISCRIMINANT-ANALYSIS,FEATURE-SELECTION,EXPERT-SYSTEM,B-VIRUS,DIAGNOSIS,PREDICTION,CLASSIFICATION",Article,"HINDAWI LTD, ADAM HOUSE, 3RD FLR, 1 FITZROY SQ, LONDON, W1T 5HF, ENGLAND","Computer Science,Telecommunications",,1.537,"PRINCIPAL,COMPONENT,ANALYSIS,DISCRIMINANT-ANALYSIS,FEATURE-SELECTION,EXPERT-SYSTEM,B-VIRUS,DIAGNOSIS,PREDICTION,CLASSIFICATION",MOBILE INFORMATION SYSTEMS,https://downloads.hindawi.com/journals/misy/2020/8870240.pdf,
70,LV-GAN: A deep learning approach for limited-view optoacoustic imaging based on hybrid datasets,14,2,,"Lu Tong,Chen Tingting,Gao Feng,Sun Biao,Ntziachristos Vasilis,Li Jiao","Lu T,Chen TT,Gao F,Sun B,Ntziachristos V,Li J",Li J,10.1002/jbio.202000325,Tianjin University,"The optoacoustic imaging (OAI) methods are rapidly evolving for resolving optical contrast in medical imaging applications. In practice, measurement strategies are commonly implemented under limited-view conditions due to oversized image objectives or system design limitations. Data acquired by limited-view detection may impart artifacts and distortions in reconstructed optoacoustic (OA) images. We propose a hybrid data-driven deep learning approach based on generative adversarial network (GAN), termed as LV-GAN, to efficiently recover high quality images from limited-view OA images. Trained on both simulation and experiment data, LV-GAN is found capable of achieving high recovery accuracy even under limited detection angles less than 60 degrees. The feasibility of LV-GAN for artifact removal in biological applications was validated by ex vivo experiments based on two different OAI systems, suggesting high potential of a ubiquitous use of LV-GAN to optimize image quality or system design for different scanners and application scenarios.","biomedical applications,deep learning,high quality,limited&#8208,view,optoacoustic imaging",Article,"WILEY-V C H VERLAG GMBH, POSTFACH 101161, 69451 WEINHEIM, GERMANY","Biochemistry & Molecular Biology,Biophysics,Optics",,3.403,TOMOGRAPHY,JOURNAL OF BIOPHOTONICS,,
71,Beyond kappa: an informational index for diagnostic agreement in dichotomous and multivalue ordered-categorical ratings,58,12,3089-3099,"Casagrande Alberto,Fabris Francesco,Girometti Rossano","Casagrande A,Fabris F,Girometti R",Fabris F,10.1007/s11517-020-02261-2,University of Trieste,"Agreement measures are useful tools to both compare different evaluations of the same diagnostic outcomes and validate new rating systems or devices. Cohen's kappa (kappa) certainly is the most popular agreement method between two raters, and proved its effectiveness in the last sixty years. In spite of that, this method suffers from some alleged issues, which have been highlighted since the 1970s; moreover, its value is strongly dependent on the prevalence of the disease in the considered sample. This work introduces a new agreement index, the informational agreement (IA), which seems to avoid some of Cohen's kappa's flaws, and separates the contribution of the prevalence from the nucleus of agreement. These goals are achieved by modelling the agreement-in both dichotomous and multivalue ordered-categorical cases-as the information shared between two raters through the virtual diagnostic channel connecting them: the more information exchanged between the raters, the higher their agreement. In order to test its fair behaviour and the effectiveness of the method, IA has been tested on some cases known to be problematic for kappa, in the machine learning context and in a clinical scenario to compare ultrasound (US) and automated breast volume scanner (ABVS) in the setting of breast cancer imaging.","Diagnostic agreement,Cohen's kappa statistic,Multivalue ordered-categorical ratings,Inter-reader agreement,Information measures",Article,"SPRINGER HEIDELBERG, TIERGARTENSTRASSE 17, D-69121 HEIDELBERG, GERMANY","Computer Science,Engineering,Mathematical & Computational Biology,Medical Informatics",,2.61,"WEIGHTED,KAPPA,TESTS,PERFORMANCE,COEFFICIENT,PREVALENCE,TERMS",MEDICAL & BIOLOGICAL ENGINEERING & COMPUTING,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7679268,
72,,,,,,,,,,,,,,,,,,,,
73,FLAIR-Wise Machine-Learning Classification and Lateralization of MRI-Negative F-18-FDG PET-Positive Temporal Lobe Epilepsy,11,,,"Beheshti Iman,Sone Daichi,Maikusa Norihide,Kimura Yukio,Shigemoto Yoko,Sato Noriko,Matsuda Hiroshi","Beheshti I,Sone D,Maikusa N,Kimura Y,Shigemoto Y,Sato N,Matsuda H",Matsuda H,10.3389/fneur.2020.580713,"Southern Tohoku Res Inst Neurosci, Cyclotron & Drug Discovery Res Ctr, Koriyama, Fukushima, Japan.","Objective: In this study, we investigated the ability of fluid-attenuated inversion recovery (FLAIR) data coupled with machine-leaning algorithms to differentiate normal and epileptic brains and identify the laterality of focus side in temporal lobe epilepsy (TLE) patients with visually negative MRI.
Materials and Methods: The MRI data were acquired on a 3-T MR system (Philips Medical Systems). After pre-proceeding stage, the FLAIR signal intensities were extracted from specific regions of interest, such as the amygdala, cerebral white matter, inferior temporal gyrus, middle temporal gyrus, parahippocampal gyrus, superior temporal gyrus, and temporal pole, and fed into a classification framework followed by a support vector machine as classifier. The proposed lateralization framework was assessed in a group of MRI-negative unilateral TLE patients (N = 42; 23 left TLE and 19 right TLE) and 34 healthy controls (HCs) based on a leave-one-out cross-validation strategy.
Results: Using the FLAIR data, we obtained a 75% accuracy for discriminating the three groups, as well as 87.71, 83.01, and 76.19% accuracies for HC/right TLE, HC/left TLE, and left TLE/right TLE tasks, respectively.
Interpretation: The experimental results show that FLAIR data can potentially be considered an informative biomarker for improving the pre-surgical diagnostic confidence in patients with MRI-negative TLE.","fluid-attenuated inversion recovery,temporal lobe epilepsy,machine-learning,feature extraction,MRI-negative focal epilepsy",Article,"FRONTIERS MEDIA SA, AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND",Neurosciences & Neurology,,4.189,"WHITE-MATTER,ABNORMALITIES",FRONTIERS IN NEUROLOGY,https://www.frontiersin.org/articles/10.3389/fneur.2020.580713/pdf,
74,MMHGE: detecting mild cognitive impairment based on multi-atlas multi-view hybrid graph convolutional networks and ensemble learning,24,1,103-113,"Liu Jin,Zeng Dejiao,Guo Rui,Lu Mingming,Wu Fang-Xiang,Wang Jianxin","Liu J,Zeng DJ,Guo R,Lu MM,Wu FX,Wang JX",Liu J; Wang JX,10.1007/s10586-020-03199-8,Central South University,"Currently, it is still a great challenge in clinical practice to accurately detect the early state of Alzheimer's disease (AD), i.e., mild cognitive impairment (MCI) including early MCI (EMCI) and late MCI (LMCI). To address this challenge, we propose a new MCI detection framework based on multi-atlas multi-view hybrid graph convolutional networks and ensemble learning. We first construct nine different graphs based on three brain atlases and three morphological measurements using both imaging and non-imaging data of each subject. Then, in order to integrate the information of different graphs and obtain more discriminative feature representations for detecting MCI, we propose a hybrid graph convolutional network method. Finally, a new ensemble learning method is proposed to perform MCI detection tasks. An evaluation of our proposed framework has been conducted with 369 subjects with cognitively normal (CN), 779 subjects with MCI including 310 subjects with EMCI and 469 subjects with LMCI, and 301 subjects with AD on three classification tasks. Experimental results show that our proposed framework can get an accuracy of 90.8% and an AUC of 0.932 for MCI/CN classification, an accuracy of 88.6% and an AUC of 0.908 for MCI/AD classification, and an accuracy of 83.5% and an AUC of 0.851 for EMCI/LMCI classification, respectively. Compared with some state-of-the-art methods about MCI detection, our proposed framework can get better performance. Overall, our proposed framework is effective and promising for MCI detection in clinical practice.","MCI detection,Multi-atlas multi-view feature representation,Graph convolutional networks,Ensemble learning",Article,"SPRINGER, ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES",Computer Science,,1.906,"AUTISM,SPECTRUM,DISORDER,VOXEL,BASED,MORPHOMETRY,BRAIN,CONNECTIVITY,MRI,DISEASE,CLASSIFICATION,ALZHEIMER,CORTEX",CLUSTER COMPUTING-THE JOURNAL OF NETWORKS SOFTWARE TOOLS AND APPLICATIONS,,
75,Machine learning driven simulated deposition of carbon films: From low-density to diamondlike amorphous carbon,102,17,,"Caro Miguel A.,Csanyi Gabor,Laurila Tomi,Deringer Volker L.","Caro MA,Csanyi G,Laurila T,Deringer VL",Caro MA,10.1103/PhysRevB.102.174201,Aalto University,"Amorphous carbon (a-C) materials have diverse interesting and useful properties, but the understanding of their atomic-scale structures is still incomplete. Here, we report on extensive atomistic simulations of the deposition and growth of a-C films, describing interatomic interactions using a machine learning (ML) based Gaussian approximation potential model. We expand widely on our initial work [M. A. Caro et al., Phys. Rev. Lett. 120, 166101 (2018)] by now considering a broad range of incident ion energies, thus modeling samples that span the entire range from low-density (sp(2)-rich) to high-density (sp(3)-rich, ""diamondlike"") amorphous forms of carbon. Two different mechanisms are observed in these simulations, depending on the impact energy: low-energy impacts induce sp- and sp(2)-dominated growth directly around the impact site, whereas high-energy impacts induce peening. Furthermore, we propose and apply a scheme for computing the anisotropic elastic properties of the a-C films. Our work provides fundamental insight into this intriguing class of disordered solids, as well as a conceptual and methodological blueprint for simulating the atomic-scale deposition of other materials with ML driven molecular dynamics.","MOLECULAR-DYNAMICS SIMULATIONS,CROSS-SECTIONAL STRUCTURE,AB-INITIO SIMULATIONS,REACTIVE FORCE-FIELD,PLANE-WAVE,ELECTROCHEMICAL DETECTION,STRUCTURAL MOTIFS,TOTAL-ENERGY,GROWTH,POTENTIALS",Article,"AMER PHYSICAL SOC, ONE PHYSICS ELLIPSE, COLLEGE PK, MD 20740-3844 USA","Materials Science,Physics",,3.86,"MOLECULAR-DYNAMICS,SIMULATIONS,CROSS-SECTIONAL,STRUCTURE,AB-INITIO,SIMULATIONS,REACTIVE,FORCE-FIELD,PLANE-WAVE,ELECTROCHEMICAL,DETECTION,STRUCTURAL,MOTIFS,TOTAL-ENERGY,GROWTH,POTENTIALS",PHYSICAL REVIEW B,http://arxiv.org/pdf/2006.09760,
76,"Quantitative identification of coal texture using the support vector machine with geophysical logging data: A case study using medium-rank coal from the Panjiang, Guizhou, China",8,4,T753-T762,"Xiao Zhenghui,Jiang Wei,Sun Bin,Cao Yunjiang,Jiang Lei,Cao Taotao,Yang Qing,Huang Cailun,Yang Xiansheng,Huang Xiangkuan","Xiao ZH,Jiang W,Sun B,Cao YJ,Jiang L,Cao TT,Yang Q,Huang CL,Yang XS,Huang XK",Xiao ZH,10.1190/INT-2019-0237.1,Hunan University of Science & Technology,"Coal texture is important for predicting coal seam permeability and selecting favorable blocks for coalbed methane (CBM) exploration. Drilled cores and mining seam observations are the most direct and effective methods of identifying coal texture; however, they are expensive and cannot be used in unexplored coal seams. Geophysical logging has become a common method of coal texture identification, particularly during the CBM mining stage. However, quantitative methods for identifying coal texture based on geophysical logging data require further study. The support vector machine (SVM), a machine-learning method, has received great interest due to its remarkable generalization performance, and it has been used to quantitatively identify hard and soft coal using geophysical logging data. In this study, four well-logging curves, the acoustic time difference (AC), caliper log (CAL), density (DEN), and natural gamma (GR), were used for coal texture analysis. Hard coal (undeformed and cataclastic coal) exhibited higher DEN, GR, lower CAL, and lower AC than soft coal. The accuracy rate of coal texture identification was highest (97%) when the linear kernel function was applied, and the maximum training accuracy rate was achieved when the penalty parameter value of the linear kernel increased to 1. The results of verification with a newly cored CBM exploration well indicated that the SVM-based identification method was effective for coal texture analysis. With the increasing availability of data, this method can be used to distinguish hard and soft coal in a coal-bearing basin under numerous sample learning conditions.","WESTERN GUIZHOU,QINSHUI BASIN,PERMEABILITY,METHANE,RESERVOIRS,BLOCK,SEAMS",Article,"SOC EXPLORATION GEOPHYSICISTS, 8801 S YALE ST, TULSA, OK 74137 USA",Geochemistry & Geophysics,,1.492,"WESTERN,GUIZHOU,QINSHUI,BASIN,PERMEABILITY,METHANE,RESERVOIRS,BLOCK,SEAMS",INTERPRETATION-A JOURNAL OF SUBSURFACE CHARACTERIZATION,,
77,"Precipitation Nowcasting with Weather Radar Images and Deep Learning in Sao Paulo, Brasil",11,11,,"Bonnet Suzanna Maria,Evsukoff Alexandre,Morales Rodriguez Carlos Augusto","Bonnet SM,Evsukoff A,Rodriguez CAM",Bonnet SM,10.3390/atmos11111157,Universidade Federal do Rio de Janeiro,"Precipitation nowcasting can predict and alert for any possibility of abrupt weather changes which may cause both human and material risks. Most of the conventional nowcasting methods extrapolate weather radar echoes, but precipitation nowcasting is still a challenge, mainly due to rapid changes in meteorological systems and time required for numerical simulations. Recently video prediction deep learning (VPDL) algorithms have been applied in precipitation nowcasting. In this study, we use the VPDL PredRNN++ and sequences of radar reflectivity images to predict the future sequence of reflectivity images for up to 1-h lead time for Sao Paulo, Brazil. We also verify the feasibility for the continuous use of the VPDL model, providing the meteorologist with trends and forecasts in precipitation edges regardless of the weather event occurring. The results obtained confirm the potential of the VPDL model as an additional tool to assist nowcasting. Even though meteorological systems that trigger natural disasters vary by location, a general solution can contribute as a tool to assist decision-makers and consequently issue efficient alerts.","precipitation nowcasting,spatiotemporal forecast,video prediction deep learning,weather radar",Article,"MDPI, ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND","Environmental Sciences & Ecology,Meteorology & Atmospheric Sciences",,2.848,"VERIFICATION,FORECASTS",ATMOSPHERE,https://www.mdpi.com/2073-4433/11/11/1157/pdf,
78,Game theoretic interpretability for learning based preoperative gliomas grading,112,,1-10,"Ma Laifa,Xiao Zheng,Li Kenli,Li Shengli,Li Jianlin,Yi Xiaoping","Ma LF,Xiao Z,Li KL,Li SL,Li JL,Yi XP",Xiao Z,10.1016/j.future.2020.04.038,Hunan University,"Gliomas are the most common primary tumors occurring in the central nervous system. Accurate gliomas grading is crucial for prognosis assessment and optimal treatment on the part of the patient. This study aims to develop and validate a pretreatment MRI-based noninvasive machine learning radiomics model for preoperatively grading glioma, and to simultaneously interpret the radiomics features used in the model. Firstly, wavelet transform and Laplacian of Gaussian (LoG) filtering are used during image preprocessing and a total of 1024 quantitative features are extracted from the region of interest (ROI) of the tumor, which is manually delineated on the largest slice of MRI images. Then, feature selection is performed by Pearson correlation coefficient and the least absolute shrinkage and selection operator (LASSO). Finally, extreme gradient boosting (XGBoost) is built to carry out the glioma grading, and Shapley value is used to quantitatively interpret and reveal the important features contributing to grading. Experimental results on a benchmark dataset demonstrate that XGBoost is effective and efficient for the diagnosis of glioma. Accuracy, sensitivity, specificity, and AUC are 0.83, 0.86, 0.81, 0.86, respectively. These results add evidence of the important role of radiomics model based on only one representative MRI image in preoperatively grading glioma. The quantitative analysis and interpretation may assist clinicians to better understand the disease and select appropriate treatment for improving clinical outcomes. (C) 2020 Elsevier B.V. All rights reserved.","Gliomas,Feature selection,Machine learning,Shapley value",Article,"ELSEVIER, RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS",Computer Science,,6.644,"STEREOTACTIC,BIOPSY,RADIOMICS,BRAIN,IMAGES,DIAGNOSIS,CLASSIFICATION,SEGMENTATION,PERFORMANCE,MANAGEMENT,SELECTION",FUTURE GENERATION COMPUTER SYSTEMS-THE INTERNATIONAL JOURNAL OF ESCIENCE,,
79,Design of Materials for Nuclear Energy Applications: First-Principles Calculations and Artificial Intelligence Methods,58,6,907-937,"Abrikosov I. A.,Son E. E.,Mukhamedov B. O.,Khvan A. V","Abrikosov IA,Son EE,Mukhamedov BO,Khvan AV",Son EE,10.1134/S0018151X20060024,Russian Academy of Sciences,"The review contains a description of modern methods and results of designing new materials for nuclear energy applications. The first part presented in this review contains a description of ab initio methods for calculating of materials properties. It describes rigorous and approximate quantum methods (density functional method) for the predicting the properties of materials employing modern supercomputers, and popular software packages for numerical simulation, as well as databases of first-principles calculations that exist internationally. With the accumulation of a large number of laborious quantum calculations, artificial intelligence methods are becoming more and more effective. In particular, machine learning methods are considered, and results of their use for the design of materials are presented in the review. The results of quantum mechanics modeling of binary alloys of iron, chromium and nickel are presented. The second part of the review will be devoted to quantum modeling of liquid and plasma states of matter in reactors, as well as experimental studies of materials for nuclear power. The third part of the review will be devoted to non-equilibrium plasma formed in reactors as a result of secondary processes during stopping of fast particles.",,Review,"MAIK NAUKA/INTERPERIODICA/SPRINGER, 233 SPRING ST, NEW YORK, NY 10013-1578 USA",Physics,,0.873,,HIGH TEMPERATURE,,
80,Classification of Hand Movements from EMG Signals for People with Motor Disabilities,18,11,2019-2026,"Prado Francisco J. Junior,dos Santos Flavio V.,Fernandes C. Alexandre R.","Prado FJ,dos Santos FV,Fernandes CAR",Prado FJ,10.1109/TLA.2020.9398644,Universidade Federal do Ceara,"People with disabilities correspond to about 25% of the Brazilian population. A great part of these people have physical impairments that difficult the use computer peripherals. This article presents the development of a system for detection of hand movements through the acquisition and classification of electromyographic (EMG) signals using machine learning techniques. The purpose of the proposed system is to be used by people with disabilities to control an adapted text editor. The signals are capture by surface EMG electrodes and used to the detect 4 different hand movements. In addition, a database with 3200 EMG signals generated by the hand movements was created, made by one user diagnosed with cerebral palsy and another user without diagnosed motor disabilities. Several tests were carried out, showing the good accuracy of the proposed system, with a success classification rate of 96% to 98%.","assistive technology,EMG signals,human-machine interface,machine learning,text editor",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Engineering",,0.675,,IEEE LATIN AMERICA TRANSACTIONS,,
81,Fractal characterization of thermal cracking patterns and fracture zone in low-alkali cement matrix modified with microsilica,114,,,Szelag Maciej,Szelag M,Szelag M,10.1016/j.cemconcomp.2020.103732,Lublin University of Technology,"The paper presents the results of the analysis of the thermal cracking patterns and fracture zone of microsilica-modified cement matrix made of the low-alkali cement. The material was subjected to a two-stage thermal load. To identify cracks an original, the image double-segmentation procedure, using machine learning algorithms was developed. To characterize the cracks structure, the fractal geometry was applied, measuring the fractal dimension of the cracking patterns. Moreover, the total crack area and the crack density were analyzed. In the case of the fracture line analysis the fractal dimension was examined in two variants. The applied parameters allowed to describe the complexity degree of the surface cracks structure. The process of development of thermal cracks caused by repeated thermal loading was also characterized. The obtained results showed strong correlation between the tensile strength, fractal dimension of the cracking pattern and crack density. Knowledge in this area allows to estimate mechanical properties of the cement matrix on the basis of measurement and evaluation of morphology of the thermal cracking patterns.","Cement matrix,Elevated temperature,Cracking pattern,Image analysis,Fractal dimension,ImageJ",Article,"ELSEVIER SCI LTD, THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND","Construction & Building Technology,Materials Science",,8.642,"INCORPORATING,FLY-ASH,COMPRESSIVE,STRENGTH,DIMENSION,CONCRETE,SURFACE,PASTE,MODEL,SUBMICROCRACKING,COMPOSITES,STABILITY",CEMENT & CONCRETE COMPOSITES,,
82,A smart healthcare monitoring system for heart disease prediction based on ensemble deep learning and feature fusion,63,,208-222,"Ali Farman,El-Sappagh Shaker,Islam S. M. Riazul,Kwak Daehan,Ali Amjad,Imran Muhammad,Kwak Kyung-Sup","Ali F,El-Sappagh S,Islam SMR,Kwak D,Ali A,Imran M,Kwak KS",Ali F,10.1016/j.inffus.2020.06.008,Sejong University,"The accurate prediction of heart disease is essential to efficiently treating cardiac patients before a heart attack occurs. This goal can be achieved using an optimal machine learning model with rich healthcare data on heart diseases. Various systems based on machine learning have been presented recently to predict and diagnose heart disease. However, these systems cannot handle high-dimensional datasets due to the lack of a smart framework that can use different sources of data for heart disease prediction. In addition, the existing systems utilize conventional techniques to select features from a dataset and compute a general weight for them based on their significance. These methods have also failed to enhance the performance of heart disease diagnosis. In this paper, a smart healthcare system is proposed for heart disease prediction using ensemble deep learning and feature fusion approaches. First, the feature fusion method combines the extracted features from both sensor data and electronic medical records to generate valuable healthcare data. Second, the information gain technique eliminates irrelevant and redundant features, and selects the important ones, which decreases the computational burden and enhances the system performance. In addition, the conditional probability approach computes a specific feature weight for each class, which further improves system performance. Finally, the ensemble deep learning model is trained for heart disease prediction. The proposed system is evaluated with heart disease data and compared with traditional classifiers based on feature fusion, feature selection, and weighting techniques. The proposed system obtains accuracy of 98.5%, which is higher than existing systems. This result shows that our system is more effective for the prediction of heart disease, in comparison to other state-of-the-art methods.","Feature extraction,Feature fusion,Heart disease prediction,Deep learning,Ontology",Article,"ELSEVIER, RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS",Computer Science,,12.707,"DECISION-SUPPORT-SYSTEM,RECOMMENDATION,SYSTEM,WEIGHTING,FILTER,FUZZY,RISK,MACHINE,DIAGNOSIS,ALGORITHM,ONTOLOGY,INTERNET",INFORMATION FUSION,,
83,Machine learning lattice constants for cubic perovskite A(2)XY(6) compounds,291,,,"Zhang Yun,Xu Xiaojie","Zhang Y,Xu XJ",Zhang Y,10.1016/j.jssc.2020.121558,University of North Carolina,"Metal halide perovskites have attracted great attention in the past decade due to unique and tunable optical and electrical properties, which are promising candidates for various applications such as solar cells, light emitting diodes, and laser cooling devices. The lattice constant, a, as the only variable parameter among the six parameters in the crystal structure, has a significant impact on the structural stability, bandgap structure, and thus materials performance. In this work, the Gaussian process regression (GPR) model is developed to elucidate the statistical relationship between ionic radii and lattice constants for cubic perovskite A(2)XY(6) compounds. A total of 79 samples with lattice constants ranging from 8.109 angstrom to 11.790 angstrom are explored. The modeling approach demonstrates a high degree of accuracy and stability, contributing to efficient and low-cost estimations of lattice constants.","Lattice constant,Halide,Perovskite,Semiconductor,Machine learning,Gaussian process regression",Article,"ACADEMIC PRESS INC ELSEVIER SCIENCE, 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA",Chemistry,,2.831,"CRYSTAL-STRUCTURE,MONTE-CARLO,STRUCTURAL,REFINEMENT,MAGNETIC-PROPERTIES,POLONIUM,HALIDES,DEFECT,TOLERANCE,POTASSIUM,EFFICIENCY,POWDER,HEXACHLOROTANTALATE%28IV%29",JOURNAL OF SOLID STATE CHEMISTRY,,
84,Comprehensive chemometric classification of snack products based on their near infrared spectra,133,,,"Benes Eszter,Bajusz David,Gere Attila,Fodor Marietta,Racz Anita","Benes E,Bajusz D,Gere A,Fodor M,Racz A",Racz A,10.1016/j.lwt.2020.110130,Hungarian Academy of Sciences,"The authentication and quality assurance of snack products have become important, since these convenience foods are popular in the modern lifestyle. Near infrared spectroscopy with machine learning algorithms can be used for the determination of qualitative properties of these products. Our study focuses on the determination of four important qualitative aspects of snacks: a) frying oil, b) raw material, c) place of origin, d) production technology. Our aim was to classify snack samples from various countries and producers successfully based on the aforementioned criteria. Three well-known machine learning algorithms, namely partial least squares discriminant analysis (PLS-DA), multilayer feed-forward of resilient backpropagation network (RPropMLP) and random forest (RF) were applied for the task, with custom validation protocols, optimized for the number of samples. Cross-validation and test validation were used to verify the robustness of the models. Accuracy of the models was above 0.80 in each case. The results showed that the neural network-based algorithm outperformed the other algorithms in every case based on the accuracy and area under the ROC curve values, which reveals a potential advantage of neural network-based algorithms in the case of smaller datasets. The models can be easily implemented to the quality control of snack products.","NIR,Machine learning,Quality assurance,Random forest,Neural network",Article,"ELSEVIER, RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS",Food Science & Technology,,5.383,"FRYING,OIL,DEGRADATION,RAPID-DETERMINATION,SPECTROSCOPY,RAMAN",LWT-FOOD SCIENCE AND TECHNOLOGY,https://doi.org/10.1016/j.lwt.2020.110130,
85,Deep Learning in Selected Cancers' Image Analysis-A Survey,6,11,,"Debelee Taye Girma,Kebede Samuel Rahimeto,Schwenker Friedhelm,Shewarega Zemene Matewos","Debelee TG,Kebede SR,Schwenker F,Shewarega ZM",Debelee TG,10.3390/jimaging6110121,"Artificial Intelligence Ctr, Addis Ababa 40782, Ethiopia.","Deep learning algorithms have become the first choice as an approach to medical image analysis, face recognition, and emotion recognition. In this survey, several deep-learning-based approaches applied to breast cancer, cervical cancer, brain tumor, colon and lung cancers are studied and reviewed. Deep learning has been applied in almost all of the imaging modalities used for cervical and breast cancers and MRIs for the brain tumor. The result of the review process indicated that deep learning methods have achieved state-of-the-art in tumor detection, segmentation, feature extraction and classification. As presented in this paper, the deep learning approaches were used in three different modes that include training from scratch, transfer learning through freezing some layers of the deep learning network and modifying the architecture to reduce the number of parameters existing in the network. Moreover, the application of deep learning to imaging devices for the detection of various cancer cases has been studied by researchers affiliated to academic and medical institutes in economically developed countries; while, the study has not had much attention in Africa despite the dramatic soar of cancer risks in the continent.","deep learning,medical image analysis,breast cancer,brain tumor,cervical cancer,colon cancer,lung cancer",Review,"MDPI, ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND",Imaging Science & Photographic Technology,,,"LUNG,NODULE,DETECTION,NEURAL-NETWORK,MTANN,LOW-DOSE,CT,FALSE,POSITIVES,UNITED-STATES,CLASSIFICATION,SEGMENTATION,DIAGNOSIS,NEUROONCOLOGY,REDUCTION",JOURNAL OF IMAGING,https://www.mdpi.com/2313-433X/6/11/121/pdf,
86,PlethAugment: GAN-Based PPG Augmentation for Medical Diagnosis in Low-Resource Settings,24,11,3226-3235,"Kiyasseh Dani,Tadesse Girmaw Abebe,Nhan Le Nguyen Thanh,Van Tan Le,Thwaites Louise,Zhu Tingting,Clifton David","Kiyasseh D,Tadesse GA,Nhan LNT,Tan LV,Thwaites L,Zhu TT,Clifton D",Kiyasseh D,10.1109/JBHI.2020.2979608,University of Oxford,"The paucity of physiological time-series data collected from low-resource clinical settings limits the capabilities of modern machine learning algorithms in achieving high performance. Such performance is further hindered by class imbalance; datasets where a diagnosis is much more common than others. To overcome these two issues at low-cost while preserving privacy, data augmentation methods can be employed. In the time domain, the traditional method of time-warping could alter the underlying data distribution with detrimental consequences. This is prominent when dealing with physiological conditions that influence the frequency components of data. In this paper, we propose PlethAugment; three different conditional generative adversarial networks (CGANs) with an adapted diversity term for the generation of pathological photoplethysmogram (PPG) signals in order to boost medical classification performance. To evaluate and compare the GANs, we introduce a novel metric-agnostic method; the synthetic generalization curve. We validate this approach on two proprietary and two public datasets representing a diverse set of medical conditions. Compared to training on non-augmented class-balanced datasets, training on augmented datasets leads to an improvement of the AUROC by up to 29% when using cross validation. This illustrates the potential of the proposed CGANs to significantly improve classification performance.","Generators,Biomedical imaging,Informatics,Generative adversarial networks,Training,Gallium nitride,Sensitivity,Conditional generative adversarial networks,data-augmentation,time-series,photople-thysmogram,low-resource",Article,"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","Computer Science,Mathematical & Computational Biology,Medical Informatics",,6.018,,IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS,https://ora.ox.ac.uk/objects/uuid:3ca3b2e5-f6e9-4975-9596-3e31bea8f9ea/download_file?safe_filename=09078801-VoR.pdf&type_of_work=Journal+article,
87,"Cost, performance prediction and optimization of a vanadium flow battery by machine-learning",13,11,4353-4361,"Li Tianyu,Xing Feng,Liu Tao,Sun Jiawei,Shi Dingqin,Zhang Huamin,Li Xianfeng","Li TY,Xing F,Liu T,Sun JW,Shi DQ,Zhang HM,Li XF",Li XF,10.1039/d0ee02543g,Chinese Academy of Sciences,"Performance optimization and cost reduction of a vanadium flow battery (VFB) system is essential for its commercialization and application in large-scale energy storage. However, developing a VFB stack from lab to industrial scale can take years of experiments due to the influence of complex factors, from key materials to the battery architecture. Herein, we have developed an innovative machine learning (ML) methodology to optimize and predict the efficiencies and costs of VFBs with extreme accuracy, based on our database of over 100 stacks with varying power rates. The results indicated that the cost of a VFB system (S-cost) at energy/power (E/P) = 4 h can reach around 223 $ (kW h)(-1), when the operating current density reaches 200 mA cm(-2), while the voltage efficiency (VE) and utilization ratio of the electrolyte (UE) are maintained above 90% and 80%, respectively. This work highlights the potential of the ML methodology to guide stack design and optimization of flow batteries to further accelerate their commercialization.","ELECTROCHEMICAL PERFORMANCE,TECHNOECONOMIC ASSESSMENT,POROUS MEMBRANES,ENERGY-STORAGE,CARBON FELT,SELECTIVITY,ELECTRODE,DESIGN,PROGRESS,MODEL",Article,"ROYAL SOC CHEMISTRY, THOMAS GRAHAM HOUSE, SCIENCE PARK, MILTON RD, CAMBRIDGE CB4 0WF, CAMBS, ENGLAND","Chemistry,Energy & Fuels,Engineering,Environmental Sciences & Ecology",,36.942,"ELECTROCHEMICAL,PERFORMANCE,TECHNOECONOMIC,ASSESSMENT,POROUS,MEMBRANES,ENERGY-STORAGE,CARBON,FELT,SELECTIVITY,ELECTRODE,DESIGN,PROGRESS,MODEL",ENERGY & ENVIRONMENTAL SCIENCE,,
88,Design of a visual attention model for communication campaigns: the case of Covid-19,29,6,,"Silva-Torres Juan-Jose,Martinez-Martinez Luz,Cuesta-Cambra Ubaldo","Silva-Torres JJ,Martinez-Martinez L,Cuesta-Cambra U",Silva-Torres JJ,10.3145/epi.2020.nov.27,Complutense University of Madrid,"Health is one of the main concerns of society. Empirical evidence underscores the growing importance of prevention and health education as a fundamental instrument to improve the quality of public health. Recent health crises, such as Ebola, influenza A, SARS, and Covid-19, have highlighted the importance of communication. When designing communication campaigns during a crisis, the speed of the creation of messages and their effectiveness have relevant social consequences. The objective of this work is to design and develop a mathematical tool, based on Machine Learning techniques, to enable predictions of areas of visual attention quickly and accurately without the use of eye-tracking technology. The methodology combines deep learning algorithms, to extract the characteristics of the images, and supervised modeling mathematical techniques, to predict the areas of attention. Validation is carried out by analyzing various institutional communications from the Covid-19 campaign, comparing the results with the areas of attention obtained using an eye-tracking solution with proven accuracy. The results obtained using the tool in the investigated Covid-19 communication pieces are analyzed, resulting in conclusions of interest for the development of new campaigns.","Covid-19,Coronavirus,Pandemics,Crisis communication,Health communication,Health education,Mathematical models,Machine learning,Eye-tracking,Deep learning,Visual attention",Article," EDICIONES PROFESIONALES INFORMACION SL-EPI, MISTRAL, 36, BARCELONA, ALBOLOTE, SPAIN","Communication,Information Science & Library Science",,2.285,"SCENE,PREDICT",PROFESIONAL DE LA INFORMACION,https://revista.profesionaldelainformacion.com/index.php/EPI/article/download/81086/62889,
89,"Slow pyrolysis as a platform for negative emissions technology: An integration of machine learning models, life cycle assessment, and economic analysis",223,,,"Cheng Fangwei,Luo Hongxi,Colosi Lisa M.","Cheng FW,Luo HX,Colosi LM",Colosi LM,10.1016/j.enconman.2020.113258,University of Virginia,"Biochar sequestration has gained increasing attention as a negative emissions technology to mitigate climate change. Although pyrolysis is a commercial technology, comprehensive environmental and economic assessments have been difficult to perform since biochar can be produced from a wide range of feedstocks and pyrolysis conditions. Many researchers have evaluated the environmental and economic impacts of biochar-based CO2 sequestration systems. However, most studies either worked on a single type of biomass under varying pyrolysis conditions or multiple feedstocks under the same pyrolysis conditions. To address this knowledge gap, we evaluated the energy, climate change, and economic performance of slow pyrolysis of multiple feedstocks under various processing conditions via the integration of machine learning approaches, life cycle assessment (LCA), and economic analysis. Machine learning models (i.e., random forest) were developed by fitting existing laboratory data. The models were then used to predict the yields and characteristics of biochar produced from slow pyrolysis of different feedstocks under designed processing conditions. The results were further integrated with LCA and economic analysis to compute three important metrics: energy return on investment (EROI), net global warming potential (GWP), and minimum product selling price (MPSP). The results indicate that random forest models offer good prediction accuracy for laboratory-scale (R-2 = 0.78-0.87) and pilot-scale pyrolysis data (R2 = 0.45-0.65). LCA and economic analyses reveal that feedstock characteristics and pyrolysis temperature affect energy, climate change, and financial performance. Our results demonstrate slow pyrolysis of crop residues and woody wastes holds promise as an energy-producing negative emissions technology, with EROI values from 1.9 to 3.6 (without substitution) and 2.4 to 4.3 (with substitution), and GWP values from -470 kg CO2 eq/t to -200 kg CO2 eq/t (without substitution) and -1050 kg CO2 eq/t to -770 kg CO2 eq/t (with substitution). The MSPS values evaluated in this study range from $774-1256/t, depending on temperature and feedstocks. A tradeoff between environmental and economic performance is observed. The best overall energy and climate change performances are achieved via pyrolysis of lignocellulosic biomass at high temperature, while the best MPSP is achieved with the pyrolysis of sludge at low temperature.","Pyrolysis,Biochar, Random forest (RF),Life cycle assessment (LCA),Economic analysis,Negative emission technologies (NET)",Article,"PERGAMON-ELSEVIER SCIENCE LTD, THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND","Thermodynamics,Energy & Fuels,Mechanics",,8.954,"THERMOCHEMICAL,CONVERSION,BIOMASS,RESIDUES,CORN,STOVER,BIOCHAR,FEEDSTOCK,SOIL,ENERGY,CARBONIZATION,TEMPERATURE,STABILITY",ENERGY CONVERSION AND MANAGEMENT,,
90,Prediction of water stability of metal-organic frameworks using machine learning,2,11,704-+,"Batra Rohit,Chen Carmen,Evans Tania G.,Walton Krista S.,Ramprasad Rampi","Batra R,Chen C,Evans TG,Walton KS,Ramprasad R",Ramprasad R,10.1038/s42256-020-00249-z,University System of Georgia,"Owing to their highly tunable structures, metal-organic frameworks (MOFs) are considered suitable candidates for a range of applications, including adsorption, separation, sensing and catalysis. However, MOFs must be stable in water vapour to be considered industrially viable. It is currently challenging to predict water stability in MOFs; experiments involve time-intensive MOF synthesis, while modelling techniques do not reliably capture the water stability behaviour. Here, we build a machine learning-based model to accurately and instantly classify MOFs as stable or unstable depending on the target application, or the amount of water exposed. The model is trained using an empirically measured dataset of water stabilities for over 200 MOFs, and uses a comprehensive set of chemical features capturing information about their constituent metal node, organic ligand and metal-ligand molar ratios. In addition to screening stable MOF candidates for future experiments, the trained models were used to extract a number of simple water stability trends in MOFs. This approach is general and can also be used to screen MOFs for other design criteria.","SURFACE-AREA,ADSORPTION,CLASSIFICATION,ENHANCEMENT,OXIDATION,SELECTION,CAPTURE,POLYMER,DESIGN",Article,"SPRINGERNATURE, CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND",Computer Science,,15.508,"SURFACE-AREA,ADSORPTION,CLASSIFICATION,ENHANCEMENT,OXIDATION,SELECTION,CAPTURE,POLYMER,DESIGN",NATURE MACHINE INTELLIGENCE,,
91,Real-time human blood pressure measurement based on laser self-mixing interferometry with extreme learning machine,16,6,467-470,"Wang Xiu-lin,Lu Li-ping,Hu Lu,Huang Wen-cai","Wang XL,Lu LP,Hu L,Huang WC",Huang WC,10.1007/s11801-020-0050-x,Xiamen University,"In this paper, we present a method based on self-mixing interferometry combing extreme learning machine for real-time human blood pressure measurement. A signal processing method based on wavelet transform is applied to extract reversion point in the self-mixing interference signal, thus the pulse wave profile is successfully reconstructed. Considering the blood pressure values are intrinsically related to characteristic parameters of the pulse wave, 80 samples from the MIMIC-II database are used to train the extreme learning machine blood pressure model. In the experiment, 15 measured samples of pulse wave signal are used as the prediction sets. The results show that the errors of systolic and diastolic blood pressure are both within 5 mmHg compared with that by the Coriolis method.",,Article,"TIANJIN UNIV TECHNOLOGY, TIANJIN UNIV TECHNOLOGY, TIAJIN, 00000, PEOPLES R CHINA",Optics,,,,OPTOELECTRONICS LETTERS,,
92,Rotational Pattern Recognition by Spiking Correlated Neural Network Based on Dual-Gated MoS2 Neuristor,2,11,,"Bao Lin,Wang Zongwei,Yu Zhizhen,Ling Yaotian,Cai Yimao,Huang Ru","Bao L,Wang ZW,Yu ZZ,Ling YT,Cai YM,Huang R",Wang ZW; Cai YM,10.1002/aisy.202000102,Peking University,"Beyond the great success in machine learning (ML), the engineering community has been actively exploring neuromorphic computing systems based on spiking neural networks (SNNs). In the preliminary SNN, the neuronal information is simply encoded by the firing rate, which limits the volume of information in a certain spike pattern. Studies of the biological nervous system have discovered neuronal cooperativity, where flow of neural information is collaboratively encoded and carried by the spatiotemporal stamp and the correlated neuronal activity. In this study, by encoding the control pattern on the dual gates, a MoS2-based neuristor is exploited to program spatiotemporal information and neuronal correlations via the device's internal dynamics as well as the network topology, and a spiking correlated neural network (SCNN) is demonstrated. This correlated neural network not only succeeds in identifying the patterns with translational and rotational symmetry but also correctly reveals the rotated angles. Compared with conventional SNNs, the dimension of information flow in SCNNs can be greatly enlarged by taking advantage of the synergy between the rate coding and the neuronal correlations. This proof-of-concept work provides the potential to achieve high-volume information processing with the simplified circuitry of neuromorphic computing systems.","artificial neurons,dual gate devices,ionic gate devices,neural signal correlation,neuromorphic computing,2D materials",Article,"WILEY, 111 RIVER ST, HOBOKEN 07030-5774, NJ USA","Automation & Control Systems,Computer Science,Robotics",,,,ADVANCED INTELLIGENT SYSTEMS,https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/aisy.202000102,
93,Optimal filler content for cotton fiber/PP composite based on mechanical properties using artificial neural network,251,,,"Kazi Monzure-Khoda,Eljack Fadwa,Mahdi E.","Kazi MK,Eljack F,Mahdi E",Eljack F,10.1016/j.compstruct.2020.112654,Qatar University,"In this paper, a machine learning-based approach has been proposed to integrate artificial intelligence during the designing of fiber-reinforced polymeric composites. With the help of the proposed approach, an artificial neural network (ANN) model has been developed to achieve the targeted filler content for cotton fiber/polypropylene composite while satisfying the required targeted properties. Previously obtained experimental data sets were trained on the TensorFlow backend using Keras library in Python, followed by hyperparameter tuning and k-fold cross-validation method for acquiring a better performing model to predict the amount of targeted filler content. The developed approach proved to be very efficient and reduced the time and effort of the material characterization for numerous samples, and it will help materials designers to design their future experiments effectively. The developed approach in this paper can be extended for other composite materials if the necessary experimental data are available to train the ANN model.","Machine learning,Artificial neural network,Fiber-reinforced polymer,Intelligent product design,Cotton fiber/PP composite",Letter,"ELSEVIER SCI LTD, THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND","Mechanics,Materials Science",,5.536,"FATIGUE,LIFE,PREDICTION,BEHAVIOR,TUBES",COMPOSITE STRUCTURES,,
94,Evaluating the impact of operating energy reduction measures on embodied energy,226,,,"Venkatraj Varusha,Dixit Manish Kumar,Yan Wei,Lavy Sarel","Venkatraj V,Dixit MK,Yan W,Lavy S",Venkatraj V,10.1016/j.enbuild.2020.110340,"9910 Quercia Dr, Austin, TX 78717 USA.","Annually, 48% of the global energy is used by buildings in their construction, operation, and maintenance, causing significant damage to the environment due to the resulting greenhouse gas emissions. During their life cycles, buildings use energy in the form of embodied energy (EE) and operating energy (OE). In a conventional building, EE accounts for 10-20% of a building's life cycle energy (LCE), while OE accounts for 80-90%. As a result, the building sector has taken several measures to reduce OE in buildings. These OE reducing measures fail to account for the subsequent increase in EE and might cause an increase in the building's overall LCE. A systematic review of the literature shows limited research that comprehensively evaluates the impact of design measures aimed at OE reduction on EE for different construction assemblies. In this study, we quantify and compare trade-offs on EE demand, caused by OE reduction measures for eight different building wall assemblies across four climatic zones within the United States. The EE and OE demands of the ASHRAE 90.1-2016 benchmark model and its variations were computed using Tally (TM) and Autodesk (R) Green Building Studio (R) (GBS), respectively. The results helped us determine the EE factor (EE spent per unit of OE savings) for different OE reduction measures. Although the calculated EE factors vary across different climatic zones and construction assemblies, these factors show significant EE costs for different OE reduction measures. This knowledge could help inform the design of evolutionary and deep/machine learning-based algorithms to assess and optimize building energy use. (C) 2020 Elsevier B.V. All rights reserved.","Life cycle energy,Operating energy,Embodied energy,Building envelope,Construction assemblies,Building information modeling (BIM),Genetic algorithm (GA)",Article,"ELSEVIER SCIENCE SA, PO BOX 564, 1001 LAUSANNE, SWITZERLAND","Construction & Building Technology,Energy & Fuels,Engineering",,6.175,"LIFE-CYCLE,ENERGY,BUILDING,ENVELOPE,DESIGN,PASSIVE,SOLAR,DESIGN,DOUBLE,SKIN,FACADE,RESIDENTIAL,BUILDINGS,MULTIOBJECTIVE,OPTIMIZATION,OFFICE,BUILDINGS,CONSTRUCTION,MATERIALS,ENVIRONMENTAL-IMPACT,INSULATION,THICKNESS",ENERGY AND BUILDINGS,https://oaktrust.library.tamu.edu/bitstream/1969.1/187533/1/VENKATRAJ-THESIS-2019.pdf,
95,Determination of Impact Damage in CFRP via PVDF Signal Analysis with Support Vector Machine,13,22,,"Oh Hyun-Taik,Won Jong-Ick,Woo Sung-Choong,Kim Tae-Won","Oh HT,Won JI,Woo SC,Kim TW",Kim TW,10.3390/ma13225207,Hanyang University,"Carbon fiber reinforced plastics (CFRPs) have high specific stiffness and strength, but they are vulnerable to transverse loading, especially low-velocity impact loadings. The impact damage may cause serious strength reduction in CFRP structure, but the damage in a CFRP is mainly internal and microscopic, that it is barely visible. Therefore, this study proposes a method of determining impact damage in CFRP via poly(vinylidene fluoride) (PVDF) sensor, which is convenient and has high mechanical and electrical performance. In total, 114 drop impact tests were performed to investigate on impact responses and PVDF signals due to impacts. The test results were analyzed to determine the damage of specimens and signal features, which are relevant to failure mechanisms were extracted from PVDF signals by means of discrete wavelet transform (DWT). Support vector machine (SVM) was used for optimal classification of damage state, and the model using radial basis function (RBF) kernel showed the best performance. The model was validated through a 4-fold cross-validation, and the accuracy was reported to be 92.30%. In conclusion, impact damage in CFRP structures can be effectively determined using the spectral analysis and the machine learning-based classification on PVDF signals.","poly(vinylidene fluoride),impact damage,delamination,discrete wavelet transform (DWT),support vector machine (SVM)",Article,"MDPI, ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND","Chemistry,Materials Science,Metallurgy & Metallurgical Engineering,Physics",,3.92,"LOW-VELOCITY,IMPACT,ACOUSTIC-EMISSION,DEFECT,DETECTION,COMPOSITES,PZT",MATERIALS,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7698910,
96,Machine Learning Analysis of Raman Spectra of MoS2,10,11,,"Mao Yu,Dong Ningning,Wang Lei,Chen Xin,Wang Hongqiang,Wang Zixin,Kislyakov Ivan M.,Wang Jun","Mao Y,Dong NN,Wang L,Chen X,Wang HQ,Wang ZX,Kislyakov IM,Wang J",Dong NN; Wang J,10.3390/nano10112223,Chinese Academy of Sciences,"Defects introduced during the growth process greatly affect the device performance of two-dimensional (2D) materials. Here we demonstrate the applicability of employing machine-learning-based analysis to distinguish the monolayer continuous film and defect areas of molybdenum disulfide (MoS2) using position-dependent information extracted from its Raman spectra. The random forest method can analyze multiple Raman features to identify samples, making up for the problem of not being able to effectively identify by using just one certain variable with high recognition accuracy. Even some dispersed nucleation site defects can be predicted, which would commonly be ignored under an optical microscope because of the lower optical contrast. The successful application for classification and analysis highlights the potential for implementing machine learning to tap the depth of classical methods in 2D materials research.","2D materials,machine learning,random forest algorithm,Raman spectrum",Article,"MDPI, ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND","Chemistry,Science & Technology - Other Topics,Materials Science,Physics",,5.346,"VAPOR-PHASE,GROWTH,2-PHOTON,ABSORPTION,MONOLAYER,MOS2,ATOMIC,LAYERS,GRAPHENE,PHOTOLUMINESCENCE,IDENTIFICATION,THICKNESS,DISORDER,PHONON",NANOMATERIALS,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7695331,
97,Vital Signs Prediction and Early Warning Score Calculation Based on Continuous Monitoring of Hospitalised Patients Using Wearable Technology,20,22,,"Amer Ahmed Youssef Ali,Wouters Femke,Vranken Julie,de Korte-de Boer Dianne,Smit-Fun Valerie,Duflot Patrick,Beaupain Marie-Helene,Vandervoort Pieter,Luca Stijn,Aerts Jean-Marie","Amer AYA,Wouters F,Vranken J,de Korte-de Boer D,Smit-Fun V,Duflot P,Beaupain MH,Vandervoort P,Luca S,Aerts JM",Vanrumste B,10.3390/s20226593,KU Leuven,"In this prospective, interventional, international study, we investigate continuous monitoring of hospitalised patients' vital signs using wearable technology as a basis for real-time early warning scores (EWS) estimation and vital signs time-series prediction. The collected continuous monitored vital signs are heart rate, blood pressure, respiration rate, and oxygen saturation of a heterogeneous patient population hospitalised in cardiology, postsurgical, and dialysis wards. Two aspects are elaborated in this study. The first is the high-rate (every minute) estimation of the statistical values (e.g., minimum and mean) of the vital signs components of the EWS for one-minute segments in contrast with the conventional routine of 2 to 3 times per day. The second aspect explores the use of a hybrid machine learning algorithm of kNN-LS-SVM for predicting future values of monitored vital signs. It is demonstrated that a real-time implementation of EWS in clinical practice is possible. Furthermore, we showed a promising prediction performance of vital signs compared to the most recent state of the art of a boosted approach of LSTM. The reported mean absolute percentage errors of predicting one-hour averaged heart rate are 4.1, 4.5, and 5% for the upcoming one, two, and three hours respectively for cardiology patients. The obtained results in this study show the potential of using wearable technology to continuously monitor the vital signs of hospitalised patients as the real-time estimation of EWS in addition to a reliable prediction of the future values of these vital signs is presented. Ultimately, both approaches of high-rate EWS computation and vital signs time-series prediction is promising to provide efficient cost-utility, ease of mobility and portability, streaming analytics, and early warning for vital signs deterioration.","vital signs,early warning score,time-series prediction,kNN-LS-SVM,wearable technology",Article,"MDPI, ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND","Chemistry,Engineering,Instruments & Instrumentation",,3.735,"NOVELTY,DETECTION,SUPPORT,ANTECEDENTS,RISK",SENSORS,https://lirias.kuleuven.be/bitstream/123456789/668427/2/21-07.pdf,
98,Wearable Sensors Incorporating Compensatory Reserve Measurement for Advancing Physiological Monitoring in Critically Injured Trauma Patients,20,22,,"Convertino Victor A.,Schauer Steven G.,Weitzel Erik K.,Cardin Sylvain,Stackle Mark E.,Talley Michael J.,Sawka Michael N.,Inan Omer T.","Convertino VA,Schauer SG,Weitzel EK,Cardin S,Stackle ME,Talley MJ,Sawka MN,Inan OT",Convertino VA,10.3390/s20226413,"US Army, Battlefield Hlth & Trauma Ctr Human Integrat Phys, Inst Surg Res, JBSA Ft Sam Houston, San Antonio, TX 78234 USA.","Vital signs historically served as the primary method to triage patients and resources for trauma and emergency care, but have failed to provide clinically-meaningful predictive information about patient clinical status. In this review, a framework is presented that focuses on potential wearable sensor technologies that can harness necessary electronic physiological signal integration with a current state-of-the-art predictive machine-learning algorithm that provides early clinical assessment of hypovolemia status to impact patient outcome. The ability to study the physiology of hemorrhage using a human model of progressive central hypovolemia led to the development of a novel machine-learning algorithm known as the compensatory reserve measurement (CRM). Greater sensitivity, specificity, and diagnostic accuracy to detect hemorrhage and onset of decompensated shock has been demonstrated by the CRM when compared to all standard vital signs and hemodynamic variables. The development of CRM revealed that continuous measurements of changes in arterial waveform features represented the most integrated signal of physiological compensation for conditions of reduced systemic oxygen delivery. In this review, detailed analysis of sensor technologies that include photoplethysmography, tonometry, ultrasound-based blood pressure, and cardiogenic vibration are identified as potential candidates for harnessing arterial waveform analog features required for real-time calculation of CRM. The integration of wearable sensors with the CRM algorithm provides a potentially powerful medical monitoring advancement to save civilian and military lives in emergency medical settings.","wearable sensors,physiology,medical monitoring,vital signs,compensatory reserve",Review,"MDPI, ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND","Chemistry,Engineering,Instruments & Instrumentation",,3.735,"BODY,NEGATIVE-PRESSURE,WAVE-FORM,ANALYSIS,HEART-RATE,BLOOD-PRESSURE,NONINVASIVE,MEASUREMENT,FLUID,RESUSCITATION,CENTRAL,HYPOVOLEMIA,PULSE,PRESSURE,VITAL,SIGNS,INDEX",SENSORS,https://www.mdpi.com/1424-8220/20/22/6413/pdf,
99,Can We Ditch Feature Engineering? End-to-End Deep Learning for Affect Recognition from Physiological Sensor Data,20,22,,"Dziezyc Maciej,Gjoreski Martin,Kazienko Przemyslaw,Saganowski Stanislaw,Gams Matjaz","Dziezyc M,Gjoreski M,Kazienko P,Saganowski S,Gams M",Dziezyc M,10.3390/s20226535,Wroclaw University of Science & Technology,"To further extend the applicability of wearable sensors in various domains such as mobile health systems and the automotive industry, new methods for accurately extracting subtle physiological information from these wearable sensors are required. However, the extraction of valuable information from physiological signals is still challenging-smartphones can count steps and compute heart rate, but they cannot recognize emotions and related affective states. This study analyzes the possibility of using end-to-end multimodal deep learning (DL) methods for affect recognition. Ten end-to-end DL architectures are compared on four different datasets with diverse raw physiological signals used for affect recognition, including emotional and stress states. The DL architectures specialized for time-series classification were enhanced to simultaneously facilitate learning from multiple sensors, each having their own sampling frequency. To enable fair comparison among the different DL architectures, Bayesian optimization was used for hyperparameter tuning. The experimental results showed that the performance of the models depends on the intensity of the physiological response induced by the affective stimuli, i.e., the DL models recognize stress induced by the Trier Social Stress Test more successfully than they recognize emotional changes induced by watching affective content, e.g., funny videos. Additionally, the results showed that the CNN-based architectures might be more suitable than LSTM-based architectures for affect recognition from physiological sensors.","deep learning,multimodal deep learning,affect recognition,emotion recognition,stress detection,wearables,physiological signals,personal sensors",Article,"MDPI, ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND","Chemistry,Engineering,Instruments & Instrumentation",,3.735,"EMOTION,RECOGNITION,STRESS,OPTIMIZATION",SENSORS,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7697590,
100,An sEMG-Controlled 3D Game for Rehabilitation Therapies: Real-Time Time Hand Gesture Recognition Using Deep Learning Techniques,20,22,,"Nasri Nadia,Orts-Escolano Sergio,Cazorla Miguel","Nasri N,Orts-Escolano S,Cazorla M",Nasri N,10.3390/s20226451,Universitat d'Alacant,"In recent years the advances in Artificial Intelligence (AI) have been seen to play an important role in human well-being, in particular enabling novel forms of human-computer interaction for people with a disability. In this paper, we propose a sEMG-controlled 3D game that leverages a deep learning-based architecture for real-time gesture recognition. The 3D game experience developed in the study is focused on rehabilitation exercises, allowing individuals with certain disabilities to use low-cost sEMG sensors to control the game experience. For this purpose, we acquired a novel dataset of seven gestures using the Myo armband device, which we utilized to train the proposed deep learning model. The signals captured were used as an input of a Conv-GRU architecture to classify the gestures. Further, we ran a live system with the participation of different individuals and analyzed the neural network's classification for hand gestures. Finally, we also evaluated our system, testing it for 20 rounds with new participants and analyzed its results in a user study.","electromyography sensor,deep learning,hand gesture recognition,virtual reality,rehabilitation",Article,"MDPI, ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND","Chemistry,Engineering,Instruments & Instrumentation",,3.735,"VIRTUAL-REALITY,POSTSTROKE,DEPRESSION,EXOSKELETON,EMG,DESIGN",SENSORS,https://rua.ua.es/dspace/bitstream/10045/110347/1/Nasri_etal_2020_Sensors.pdf,
