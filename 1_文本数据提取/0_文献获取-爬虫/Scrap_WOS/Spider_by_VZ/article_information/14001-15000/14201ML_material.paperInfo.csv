,title,Volume,Issue,Pages,whole__author_name,simply_author_name,reprint author,DOI,reprint address,Abstract,Keywords,Document Type,Publisher,Research Domain,Published Date,impact_factor,Keywords_plus,joural,pdf_link,Download_SuccessOrDefeat
1,Exploration of Machine Learning to Identify Community Dwelling Older Adults with Balance Dysfunction Using Short Duration Accelerometer Data,,,812-815,"Hu Yang,Bishnoi Alka,Kaur Rachneet,Sowers Richard,Hernandez Manuel E.","Hu Y,Bishnoi A,Kaur R,Sowers R,Hernandez ME",Hu Y,,University of Illinois System,"The incidence of fall-related injuries in older adults is high. Given the significant and adverse outcomes that arise from injurious falls in older adults, it is of the utmost importance to identify older adults at greater risk for falls as early as possible. Given that balance dysfunction provides a significant risk factor for falls, an automated and objective identification of balance dysfunction in community dwelling older adults using wearable sensor data when walking may be beneficial. In this study, we examine the feasibility of using wearable sensors, when walking, to identify older adults who have trouble with balance at an early stage using state-of-the-art machine learning techniques. We recruited 21 community dwelling older women. The experimental paradigm consisted of two tasks: Normal walking with a self-selected comfortable speed on an instrumented treadmill and a test of reflexive postural response, using the motor control test (MCT). Based on the MCT, identification of older women with low or high balance function was performed. Using short duration accelerometer data from sensors placed on the knee and hip while walking, supervised machine learning was carried out to classify subjects with low and high balance function. Using a Gradient Boosting Machine (GBM) algorithm, we classified balance function in older adults using 60 seconds of accelerometer data with an average cross validation accuracy of 91.5% and area under the receiver operating characteristic curve (AUC) of 0.97. Early diagnosis of balance dysfunction in community dwelling older adults through the use of user friendly and inexpensive wearable sensors may help in reducing future fall risk in older adults through earlier interventions and treatments, and thereby significantly reduce associated healthcare costs.","Balance,Gait,Aging,Accelerometer,Statistical analysis,Machine learning",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,CLASSIFICATION,,,
2,Predicting Core Characteristics of ASD Through Facial Emotion Recognition and Eye Tracking in Youth,,,871-875,"Jiang Ming,Francis Sunday M.,Tseng Angela,Srishyla Diksha,DuBois Megan,Beard Katie,Conelea Christine,Zhao Qi,Jacob Suma","Jiang M,Francis SM,Tseng A,Srishyla D,DuBois M,Beard K,Conelea C,Zhao Q,Jacob S",Jiang M,,University of Minnesota System,"Autism Spectrum Disorder (ASD) is a heterogeneous neurodevelopmental disorder (NDD) with a high rate of comorbidity. The implementation of eye-tracking methodologies has informed behavioral and neurophysiological patterns of visual processing across ASD and comorbid NDDs. In this study, we propose a machine learning method to predict measures of two core ASD characteristics: impaired social interactions and communication, and restricted, repetitive, and stereotyped behaviors and interests. Our method extracts behavioral features from task performance and eye-tracking data collected during a facial emotion recognition paradigm. We achieved high regression accuracy using a Random Forest regressor trained to predict scores on the SRS-2 and RBS-R assessments; this approach may serve as a classifier for ASD diagnosis.","AUTISM SPECTRUM DISORDER,PSYCHIATRIC-DISORDERS,ATTENTION,CHILDREN,COMORBIDITY,FACES",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,"AUTISM,SPECTRUM,DISORDER,PSYCHIATRIC-DISORDERS,ATTENTION,CHILDREN,COMORBIDITY,FACES",,,
3,"Machine Learning Approaches For Improved Continuous, Non-occlusive Arterial Pressure Monitoring Using Photoplethysmography",,,910-913,"Jorge Joao,Proenca Martin,Aguet Clementine,Van Zaen Jerome,Bonnier Guillaume,Renevey Phillipe,Lemkaddem Alia);,Schoettker Patrick,Lemay Mathieu","Jorge J,Proenca M,Aguet C,Van Zaen J,Bonnier G,Renevey P,Lemkaddem A,Schoettker P,Lemay M",Jorge J,,Swiss Center for Electronics & Microtechnology (CSEM),"Arterial pressure (AP) is a crucial biomarker for cardiovascular disease prevention and management. Photoplethysmography (PPG) could provide a novel, paradigm-shifting approach for continuous, non-obtrusive AP monitoring, comfortably integrated in wearable and mobile devices; yet, it still faces challenges in accuracy and robustness. In this work, we sought to integrate machine learning (ML) techniques into a previously established, clinically-validated classical approach (oBPM (R)) to develop new accurate AP estimation tools based on PPG, and at the same time improve our understanding of the underlying physiological parameters. In this novel approach, oBPM (R) was used to pre-process PPG signals and robustly extract physiological features, and ML models were trained on these features to estimate systolic AP (SAP). A feature relevance analysis showed that reference (calibration) information, followed by various morphological parameters of the PPG pulse wave, comprised the most important features for SAP estimation. A performance analysis then revealed that LASSO-regularized linear regression, Gaussian process regression and support vector regression are effective for SAP estimation, particularly when operating on reduced feature sets previously obtained with e.g. LASSO. These approaches yielded substantial reductions in error standard deviation of 9-15% relative to conventional oBPM (R). Altogether, these results indicate that ML approaches are well-suited, and promising tools to help overcoming the challenges of ubiquitous AP monitoring.","BLOOD-PRESSURE,INDUCTION",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,"BLOOD-PRESSURE,INDUCTION",,,
4,Extraction of Nystagmus Patterns from Eye-Tracker Data with Convolutional Sparse Coding,,,928-931,"Lalanne Clement,Rateaux Maxence,Oudre Laurent,Robert Matthieu P.,Moreau Thomas","Lalanne C,Rateaux M,Oudre L,Robert MP,Moreau T",Moreau T,,CEA,"The analysis of the Nystagmus waveforms from eye-tracking records is crucial for the clinical interpretation of this pathological movement. A major issue to automatize this analysis is the presence of natural eye movements and eye blink artefacts that are mixed with the signal of interest.We propose a method based on Convolutional Dictionary Learning that is able to automatically highlight the Nystagmus waveforms, separating the natural motion from the pathological movements. We show on simulated signals that our method can indeed improve the pattern recovery rate and provide clinical examples to illustrate how this algorithm performs.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,https://hal.archives-ouvertes.fr/hal-03022547/file/root.pdf,
5,Wavelet Spectral Deep-training of Convolutional Neural Networks for Accurate Identification of High-Frequency Micro-Scale Spike Transients in the Post-Hypoxic-Ischemic EEG of Preterm Sheep,,,1011-1014,"Abbasi Hamid,Gunn Alistair J.,Bennet Laura,Unsworth Charles P.","Abbasi H,Gunn AJ,Bennet L,Unsworth CP",Abbasi H,,University of Auckland,"Early diagnosis and prognosis of babies with signs of hypoxic-ischemic encephalopathy (HIE) is currently limited and requires reliable prognostic biomarkers to identify at risk infants. Using our pre-clinical fetal sheep models, we have demonstrated that micro-scale patterns evolve over a profoundly suppressed EEG background within the first 6 hours of recovery, post HI insult. In particular, we have shown that high-frequency micro-scale spike transients (in the gamma frequency band, 80-120Hz) emerge immediately after an HI event, with much higher numbers around 2-2.5 h of the insult, with numbers gradually declining thereafter. We have also shown that the automatically quantified sharp waves in this phase are predictive of neural outcome. Initiation of some neuroprotective treatments within this limited window of opportunity, such as therapeutic hypothermia, optimally reduces neural injury. In clinical practice, it is hard to determine the exact timing of the injury, therefore, reliable automatic identification of EEG transients could be beneficial to help specify the phases of injury. Our team has previously developed successful machine- and deep-learning strategies for the identification of post-HI EEG patterns in an HI preterm fetal sheep model.
This paper introduces, for the first time, a novel online fusion approach to train an 11-layers deep convolutional neural network (CNN) classifier using Wavelet-Fourier (WF) spectral features of EEG segments for accurate identification of high-frequency micro-scale spike transients in 1024Hz EEG recordings in our preterm fetal sheep. Sets of robust features were extracted using reverse biorthogonal wavelet (rbio2.8 at scale 7) and considering an 80-120Hz spectral frequency range. The WF-CNN classifier was able to accurately identify spike transients with a reliable high-performance of 99.03 +/- 0.86%.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,,
6,Deep Convolutional Neural Network and Reverse Biorthogonal Wavelet Scalograms for Automatic Identification of High Frequency Micro-Scale Spike Transients in the Post-Hypoxic-Ischemic EEG,,,1015-1018,"Abbasi Hamid,Gunn Alistair J.,Bennet Laura,Unsworth Charles P.","Abbasi H,Gunn AJ,Bennet L,Unsworth CP",Abbasi H,,University of Auckland,"Diagnosis of hypoxic-ischemic encephalopathy (HIE) is currently limited and prognostic biological markers are required for early identification of at risk infants at birth. Using pre-clinical data from our fetal sheep models, we have shown that micro-scale EEG patterns, such as high-frequency spikes and sharp waves, evolve superimposed on a significantly suppressed background during the early hours of recovery (0-6 h), after an HI insult. In particular, we have demonstrated that the number of micro-scale gamma spike transients peaks within the first 2-2.5 hours of the insult and automatically quantified sharp waves in this period are predictive of neural outcome. This period of time is optimal for the initiation of neuroprotection treatments such as therapeutic hypothermia, which has a limited window of opportunity for implementation of 6 h or less after an HI insult. Clinically, it is hard to determine when an insult has started and thus the window of opportunity for treatment. Thus, reliable automatic algorithms that could accurately identify EEG patterns that denote the phase of injury is a valuable clinical tool. We have previously developed successful machine-learning strategies for the identification of HI micro- scale EEG patterns in a preterm fetal sheep model of HI. This paper employs, for the first time, reverse biorthogonal Wavelet-Scalograms (WS) as the inputs to a 17-layer deep-trained convolutional neural network (CNN) for the precise identification of high-frequency micro-scale spike transients that occur in the 80-120Hz gamma band during first 2 h period of an HI insult. The rbio-WS-CNN classifier robustly identified spike transients with an exceptionally high-performance of 99.82%.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,,
7,Detection of Transient Bursts in the EEG of Preterm Infants using Time-Frequency Distributions and Machine Learning,,,1023-1026,"Murphy Brian M.,Goulding Robert M.,O'Toole John M.","Murphy BM,Goulding RM,O'Toole JM",Murphy BM,,"INFANT Res Ctr, Cork, Ireland.","Short-duration bursts of spontaneous activity are important markers of maturation in the electroencephalogram (EEG) of premature infants. This paper examines the application of a feature-less machine learning approach for detecting these bursts. EEGs were recorded over the first 3 days of life for infants with a gestational age below 30 weeks. Bursts were annotated on the EEG from 36 infants. In place of feature extraction, the time-series EEG is transformed into a time-frequency distribution (TFD). A gradient boosting machine is then trained directly on the whole TFD using a leave-one-out procedure. TFD kernel parameters, length of the Doppler and lag windows, are selected within a nested cross-validation procedure during training. Results indicate that detection performance is sensitive to Doppler-window length but not lag-window length. Median area under the receiver operator characteristic for detection is 0.881 (inter-quartile range 0.850 to 0.913). Examination of feature importance highlights a critical wideband region <15 Hz in the TFD. Burst detection methods form an important component in any fully-automated brain-health index for the vulnerable preterm infant.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,,
8,Wavelet Spectral Time-Frequency Training of Deep Convolutional Neural Networks for Accurate Identification of Micro-Scale Sharp Wave Biomarkers in the Post-Hypoxic-Ischemic EEG of Preterm Sheep,,,1039-1042,"Abbasi Hamid,Gunn Alistair J.,Unsworth Charles P.,Bennet Laura","Abbasi H,Gunn AJ,Unsworth CP,Bennet L",Abbasi H,,University of Auckland,"Neonatal hypoxic-ischemic encephalopathy (HIE) evolves over different phases of time during recovery. Some neuroprotection treatments are only effective for specific, short windows of time during this evolution of injury. Clinically, we often do not know when an insult may have started, and thus which phase of injury the brain may be experiencing. To improve diagnosis, prognosis and treatment efficacy, we need to establish biomarkers which denote phases of injury. Our pre-clinical research, using preterm fetal sheep, show that micro-scale EEG patterns (e.g. spikes and sharp waves), superimposed on suppressed EEG background, primarily occur during the early recovery from an HI insult (0-6 h), and that numbers of events within the first 2 h are strongly predictive of neural survival. Thus, real-time automated algorithms that could reliably identify EEG patterns in this phase will help clinicians to determine the phases of injury, to help guide treatment options. We have previously developed successful automated machine learning approaches for accurate identification and quantification of HI micro-scale EEG patterns in preterm fetal sheep post-HI. This paper introduces, for the first time, a novel online fusion strategy that employs a high-level wavelet-Fourier (WF) spectral feature extraction method in conjunction with a deep convolutional neural network (CNN) classifier for accurate identification of micro-scale preterm fetal sheep post-HI sharp waves in 1024Hz EEG recordings, along with 256Hz down-sampled data. The classifier was trained and tested over 4120 EEG segments within the first 2 hours latent phase recordings. The WF-CNN classifier can robustly identify sharp waves with considerable high-performance of 99.86% in 1024Hz and 99.5% in 256Hz data. The method is an alternative deep-structure approach with competitive high-accuracy compared to our computationally-intensive WS-CNN sharp wave classifier.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,,
9,A Two Cascaded Network Integrating Regional-based YOLO and 3D-CNN for Cerebral Microbleeds Detection,,,1055-1058,"Al-masni Mohammed A.,Kim Woo-Ram,Kim Eung Yeop,Noh Young,Kim Dong-Hyun","Al-masni MA,Kim WR,Kim EY,Noh Y,Kim DH",Kim DH,,Yonsei University,"Cerebral Microbleeds (CMBs) are small chronic brain hemorrhages, which have been considered as diagnostic indicators for different cerebrovascular diseases including stroke, dysfunction, dementia, and cognitive impairment. In this paper, we propose a fully automated two-stage integrated deep learning approach for efficient CMBs detection, which combines a regional-based You Only Look Once (YOLO) stage for potential CMBs candidate detection and three-dimensional convolutional neural networks (3D-CNN) stage for false positives reduction. Both stages are conducted using the 3D contextual information of microbleeds from the MR susceptibility-weighted imaging (SWI) and phase images. However, we average the adjacent slices of SWI and complement the phase images independently and utilize them as a two-channel input for the regional- based YOLO method. The results in the first stage show that the proposed regional- based YOLO efficiently detected the CMBs with an overall sensitivity of 93.62% and an average number of false positives per subject (FPavg) of 52.18 throughout the five-folds cross-validation. The 3D-CNN based second stage further improved the detection performance by reducing the FPavg to 1.42. The outcomes of this work might provide useful guidelines towards applying deep learning algorithms for automatic CMBs detection.","Cerebral Microbleeds,Computer-Aided Detection,Deep Learning",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,"DEEP,CLASSIFICATION,SEGMENTATION,MAMMOGRAMS,SYSTEM",,,
10,Minimizing Hybrid Dice Loss for Highly Imbalanced 3D Neuroimage Segmentation,,,1059-1062,"Lu Yuhao,Zhou Juan Helen,Guan Cuntai","Lu YH,Zhou JH,Guan CT",Lu YH,,Nanyang Technological University & National Institute of Education (NIE) Singapore,"Recent advances in medical image segmentation have largely been driven by the success of deep learning algorithms. However, one main challenge for the training of one-stage segmentation networks is the serious imbalance between the number of examples that are easy and hard to classify or in positive and negative classes. In this paper, we first investigate and compare the strategies that were proposed parallelly to handle one or two of these imbalance problems. And we propose a hybrid loss that addresses these two imbalance problems together by combining the merits of Exponential logarithmic Dice and weighted Cross entropy Loss (EDCL). Without any whistles and bells, the proposed EDC loss with 3D Unet achieves mean dice of 57.38%, which surpasses the other state-of-the-art methods with 5-fold cross-validation on a public dataset for 3D brain lesion segmentation, Anatomical Tracings of Lesions After Stroke (ATLAS) v1.2.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,,
11,A Cascaded Deep-Learning Framework for Segmentation of Metastatic Brain Tumors Before and After Stereotactic Radiation Therapy,,,1063-1066,"Jalalifar Ali,Soliman Hany,Sahgal Arjun,Sadeghi-Naini Ali","Jalalifar A,Soliman H,Sahgal A,Sadeghi-Naini A",Jalalifar A,,York University - Canada,"Radiation therapy is a major treatment option for brain metastasis. For radiation treatment planning and outcome evaluation, magnetic resonance (MR) images are acquired before and at multiple sessions after the treatment. Accurate segmentation of brain tumors on MR images is crucial for treatment planning, response evaluation, and developing data-driven models for outcome prediction. Due to the high volume of imaging data acquired from each patient at multiple follow-up sessions, manual tumor segmentation is resource- and time-consuming in clinic, hence developing an automatic segmentation framework is highly desirable. In this work, we proposed a cascaded 2D-3D Unet framework to segment brain tumors automatically on contrast-enhanced T1-weighted images acquired before and at multiple scan sessions after radiotherapy. 2D Unet is a well-known structure for medical image segmentation. 3D Unet is an extension of 2D Unet with a volumetric input image to provide richer spatial information. The limitation of 3D Unet is that it is memory consuming and cannot process large volumetric images. To address this limitation, a large volumetric input of 3D Unet is often patched to smaller volumes which leads to loss of context. To overcome this problem, we proposed using two cascaded 2D Unets to crop the input volume around the tumor area and reduce the input size of the 3D Unet, obviating the need to patch the input images. The framework was trained using images acquired from 96 patients before radiation therapy and tested using images acquired from 10 patients before and at four follow-up scans after radiotherapy. The segmentation results for the images of independent test set demonstrated that the cascaded framework outperformed the 2D and 3D Unets alone, with an average Dice score of 0.9 versus 0.86 and 0.88 for the baseline, and 0.87 versus 0.83 and 0.84 for the first followup. Similar results were obtained for the other follow-up scans.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,,
12,A novel Graph Attention Network Architecture for modeling multimodal brain connectivity,,,1071-1074,"Filip Alexandru-Catalin,Azevedo Tiago,Passamonti Luca,Toschi Nicola,Lio Pietro","Filip AC,Azevedo T,Passamonti L,Toschi N,Lio P",Filip AC,,University of Cambridge,"While Deep Learning methods have been successfully applied to tackle a wide variety of prediction problems, their application has been mostly limited to data structured in a grid-like fashion. However, the study of the human brain ""connectome"" involves the representation of the brain as a graph with interacting nodes. In this paper, we extend the Graph Attention Network (GAT), a novel neural network (NN) architecture acting on the features of the nodes of a binary graph, to handle a set of graphs provided with node features and non-binary edge weights. We demonstrate the effectiveness of our architecture by training it multimodal data collected from a large homogeneous fMRI dataset (n=1003 individuals with multiple fMRI sessions per subject) made publicly available by the Human Connectome Project (HCP), demonstrating good performance and seamless integration of multimodal neuroimaging data. Our adaptation provides a powerful and flexible deep learning tool to integrate multimodal neuroimaging connectomics data in a predictive context.",SPHERICAL-DECONVOLUTION,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,SPHERICAL-DECONVOLUTION,,,
13,A deep spatiotemporal graph learning architecture for brain connectivity analysis,,,1120-1123,"Azevedo Tiago,Passamonti Luca,Lio Pietro,Toschi Nicola","Azevedo T,Passamonti L,Lio P,Toschi N",Azevedo T,,University of Cambridge,"In recent years, the conceptualisation of the brain as a ""connectome"" as summary measures derived from graph theory analyses, has become increasingly popular. Still, such approaches are inherently limited by the need to condense and simplify temporal fMRI dynamics and architecture into a purely spatial representation. We formulate a novel architecture based on Geometric Deep Learning which is specifically tailored to the one-step integration of spatial relationship between nodes and single-node temporal dynamics. We compare different spatiotemporal modelling mechanisms and demonstrate the effectiveness of our architecture in a binary prediction task based on a large homogeneous fMRI dataset made publicly available by the Human Connectome Project (HCP). As the idea of e.g. a dynamical network connectivity is beginning to make its way into the more mainstream toolset which neuroscientists commonly employ with neuroimaging data, our model can contribute to laying the groundwork for explicitly incorporating spatiotemporal information into every association and prediction problem in neuroscience.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,,
14,Weakly-Supervised Self-Training for Breast Cancer Localization,,,1124-1127,"Liang Gongbo,Wang Xiaoqin,Zhang Yu,Jacobs Nathan","Liang GB,Wang XQ,Zhang Y,Jacobs N",Liang GB,,University of Kentucky,"The use of deep learning methods has dramatically increased the state-of-the-art performance in image object localization. However, commonly used supervised learning methods require large training datasets with pixel-level or bounding box annotations. Obtaining such fine-grained annotations is extremely costly, especially in the medical imaging domain. In this work, we propose a novel weakly supervised method for breast cancer localization. The essential advantage of our approach is that the model only requires image-level labels and uses a self-training strategy to refine the predicted localization in a step-wise manner. We evaluated our approach on a large, clinically relevant mammogram dataset. The results show that our model significantly improves performance compared to other methods trained similarly.","Object localization,mammography,convolutional neural network",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,,
15,A Two-Stage Multiple Instance Learning Framework for the Detection of Breast Cancer in Mammograms,,,1128-1131,"Chandra Sarath K.,Chakravarty Arunava,Ghosh Nirmalya,Sarkar Tandra,Sethuraman Ramanathan,Sheet Debdoot","Chandra KS,Chakravarty A,Ghosh N,Sarkar T,Sethuraman R,Sheet D",Chandra KS,,"Indian Inst Technol Kharagman, Kharagpur 721302, W Bengal, India.","Mammograms are commonly employed in the large scale screening of breast cancer which is primarily characterized by the presence of malignant masses. However, automated image-level detection of malignancy is a challenging task given the small size of the mass regions and difficulty in discriminating between malignant, benign mass and healthy dense fibro-glandular tissue. To address these issues, we explore a two-stage Multiple Instance Learning (MIL) framework. A Convolutional Neural Network (CNN) is trained in the first stage to extract local candidate patches in the mammograms that may contain either a benign or malignant mass. The second stage employs a MIL strategy for an image level benign vs. malignant classification. A global image-level feature is computed as a weighted average of patch-level features learned using a CNN. Our method performed well on the task of localization of masses with an average Precision/Recall of 0.76/0.80 and acheived an average AUC of 0.91 on the image-level classification task using a five-fold cross-validation on the INbreast dataset. Restricting the MIL only to the candidate patches extracted in Stage 1 led to a significant improvement in classification performance in comparison to a dense extraction of patches from the entire mammogram.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,http://arxiv.org/pdf/2004.11726,
16,A Comparison of Regions of Interest in Parenchymal Analysis for Breast Cancer Risk Assessment,,,1136-1139,"Africano Gerson,Arponen Otso,Sassi Antti,Karivaara-Makela Mirva,Holli-Helenius Kirsi,Rinta-Kiikka Irina,Laaperi Anna-Leena,Pertuz Said","Africano G,Arponen O,Sassi A,Karivaara-Makela M,Holli-Helenius K,Rinta-Kiikka I,Laaperi AL,Pertuz S",Pertuz S,,Universidad Industrial de Santander,"Computerized parenchymal analysis has shown potential to be utilized as an imaging biomarker to estimate the risk of breast cancer. Parenchymal analysis of digital mammograms is based on the extraction of computerized measures to build machine learning-based models for the prediction of breast cancer risk. However, the choice of the region of interest (ROI) for feature extraction within the breast remains an open problem. In this work we perform a comparison between five different methods suggested in the literature for automated ROI selection, including the whole breast (WB), the maximum squared (MS), the retro-areolar region (RA), the lattice-based (LB), and the polar-based (PB) selection methods. For the experiments, we built a retrospective dataset of 896 screening mammograms from 224 women (112 cases and 112 healthy controls). The performance of each ROI selection method was measured in terms of the area under the curve (AUC) values. The AUC values varied between 0.55 and 0.79 depending on the method and experimental settings. The best performance on an independent test set was achieved by the MS method (AUC of 0.59, 95% CI: 0.55-0.64). This method is fully-automated and does not require adjusting hyper-parameters. Based on our results, we prompt the use of the MS method for ROI selection in the computerized parenchymal analysis for breast cancer risk assessment.",DIGITAL MAMMOGRAPHY,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,"DIGITAL,MAMMOGRAPHY",,,
17,Deep Understanding of Breast Density Classification,,,1140-1143,"Cogan Timothy,Tamil Lakshman","Cogan T,Tamil L",Cogan T,,University of Texas System,"We have developed a deep learning architecture, DualViewNet, for mammogram density classification as well as a novel metric for quantifying network preference of mediolateral oblique (MLO) versus craniocaudal (CC) views in density classification. Also, we have provided thorough analysis and visualization to better understand the behavior of deep neural networks in density classification. Our proposed architecture, DualViewNet, simultaneously examines and classifies both MLO and CC views corresponding to the same breast, and shows best performance with a macro average AUC of 0.8970 and macro average 95% confidence interval of 0.8239-0.9450 obtained via bootstrapping 1000 test sets. By leveraging DualViewNet we provide a novel algorithm and quantitative comparison of MLO versus CC views for classification and find that MLO provides stronger influence in 1,187 out of 1,323 breasts.",RISK,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,RISK,,,
18,Convolutional Neural Network Based Breast Cancer Histopathology Image Classification,,,1144-1147,"Yamlome Pascal,Akwaboah Akwasi Darkwa,Marz Aylin,Deo Makarand","Yamlome P,Akwaboah AD,Marz A,Deo M",Yamlome P,,"Norfolk State Univ, Dept Engn, Norfolk, VA 23504 USA.","Breast cancer is a global health concern, with approximately 30 million new cases projected to be reported by 2030. While efforts are being channeled into curative measures, preventive and diagnostic measures also need to be improved to curb the situation. Convolutional Neural Networks (CNNs) are a class of deep learning algorithms that have been widely adopted for the computerized classification of breast cancer histopathology images. In this work, we propose a set of training techniques to improve the performance of CNN-based classifiers for breast cancer identification. We combined transfer learning techniques with data augmentation and whole image training to improve the performance of the CNN classifier. Instead of conventional image patch extraction for training and testing, we employed a high-resolution whole-image training and testing on a modified network that was pre-trained on the Imagenet dataset. Despite the computational complexity, our proposed classifier achieved significant improvement over the previously reported studies on the open-source BreakHis dataset, with an average image level accuracy of about 91% and patient scores as high as 95%.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,,
19,Combining multiple contrasts for improving machine learning-based classification of cervical cancers with a low-cost point-of-care Pocket colposcope,,,1148-1151,"Asiedu Mercy N.,Skerrett Erica);,Sapiro Guillermo,Ramanujam Nirmala","Asiedu MN,Skerrett E,Sapiro G,Ramanujam N",Asiedu MN,,Duke University,"We apply feature-extraction and machine learning methods to multiple sources of contrast (acetic acid, Lugol's iodine and green light) from the white Pocket Colposcope, a low-cost point of care colposcope for cervical cancer screening. We combine features from the sources of contrast and analyze diagnostic improvements with addition of each contrast. We find that overall AUC increases with additional contrast agents compared to using only one source.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8359705,
20,Computer-Aided Diagonosis for Colorectal Cancer using Deep Learning with Visual Explanations,,,1156-1159,"Choi Kihwan,Choi Seong Ji,Kim Eun Sun","Choi K,Choi SJ,Kim ES",Choi K,,Korea Institute of Science & Technology (KIST),"Detection, diagnosis, and removal of colorectal neoplasms are well-accepted colorectal cancer prevention methods. Although promising endoscopic imaging techniques including narrow-band imaging have been developed, these techniques are operator-dependent and interpretations of the results may vary. To overcome these limitations, we applied deep learning to develop a computer-aided diagnostic (CAD) system of colorectal adenoma. We collected and divided 3000 colonoscopic images into 4 categories according to the final pathology, normal, lowgrade dysplasia, high-grade dysplasia, and adenocarcinoma. We implemented three convolutional neural networks (CNNs) using Inception-v3, ResNet-50, and DenseNet-161 as baseline models. We further altered the models using several strategies: replacement of the top layer, transfer learning from pre-trained models, fine-tuning of the model weights, rebalancing and augmentation of the training data, and 10-fold cross-validation. We compared the outcomes of the three CNN models to those of two endoscopist groups having different years of experience, and visualized the model predictions using Class Activation Mapping (CAM). The CNN-CAD achieved the best performance in our experiments with a 92.48% classification accuracy rate. The CNN-CAD results showed a better performance in all criteria than those of endoscopic experts. The model visualization results showed reasonable regions of interest to explain pathology classification decisions. We demonstrated that CNN-CAD can distinguish the pathology of colorectal adenoma, yielding better outcomes than the endoscopic experts group.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,,
21,A Novel and Efficient Tumor Detection Framework for Pancreatic Cancer via CT Images,,,1160-1164,"Zhang Zhengdong,Li Shuai,Wang Ziyang,Lu Yun","Zhang ZD,Li S,Wang ZY,Lu Y",Zhang ZD,,Beihang University,"As Deep Convolutional Neural Networks (DCNNs) have shown robust performance and results in medical image analysis, a number of deep-learning-based tumor detection methods were developed in recent years. Nowadays, the automatic detection of pancreatic tumors using contrast-enhanced Computed Tomography (CT) is widely applied for the diagnosis and staging of pancreatic cancer. Traditional hand-crafted methods only extract low-level features. Normal convolutional neural networks, however, fail to make full use of effective context information, which causes inferior detection results. In this paper, a novel and efficient pancreatic tumor detection framework aiming at fully exploiting the context information at multiple scales is designed. More specifically, the contribution of the proposed method mainly consists of three components: Augmented Feature Pyramid networks, Self-adaptive Feature Fusion and a Dependencies Computation (DC) Module. A bottom-up path augmentation to fully extract and propagate low-level accurate localization information is established firstly. Then, the Self-adaptive Feature Fusion can encode much richer context information at multiple scales based on the proposed regions. Finally, the DC Module is specifically designed to capture the interaction information between proposals and surrounding tissues. Experimental results achieve competitive performance in detection with the AUC of 0.9455, which outperforms other state-of-the-art methods to our best of knowledge, demonstrating the proposed framework can detect the tumor of pancreatic cancer efficiently and accurately.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,http://arxiv.org/pdf/2002.04493,
22,Automated Classification of Osteosarcoma and Benign Tumors using RNA-seq and Plain X-ray,,,1165-1168,"Alge Olivia,Lu Lu,Li Zhi,Hua Yingqi,Gryak Jonathan,Najarian Kayvan","Alge O,Lu L,Li Z,Hua YQ,Gryak J,Najarian K",Alge O,,University of Michigan System,"Osteosarcoma is a prominent bone cancer that typically affects adolescents or people in late adulthood. Early recognition of this disease relies on imaging technologies such as x-ray radiography to detect tumor size and location. This paper aims to differentiate osteosarcoma from benign tumors by analyzing both imaging and RNA-seq data through a combination of image processing and machine learning. In experimental results, the proposed method achieved an Area Under the Receiver Operator Characteristic Curve (AUC) of 0.7272 in three-fold cross-validation, and an AUC of 0.9015 using leave-one-out cross-validation.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7971184,
23,Hyperspectral imaging for colon cancer classification in surgical specimens: towards optical biopsy during image-guided surgery,,,1169-1173,"Manni Francesca,Fonolla Roger,van der Sommen Fons,Zinger Svetlana,Shan Caifeng,Kho Esther,de Koning Susan Brouwer,Ruers Theo,de With Peter H. N.","Manni F,Fonolla R,van der Sommen F,Zinger S,Shan CF,Kho E,de Koning SB,Ruers T,de With PHN",Manni F,,Eindhoven University of Technology,"The main curative treatment for localized colon cancer is surgical resection. However, when tumor residuals are left, positive margins are found during the histological examinations and additional treatment is needed to inhibit recurrence. Hyperspectral imaging (HSI) can offer non-invasive surgical guidance with the potential of optimizing the surgical effectiveness. In this paper, we investigate the capability of HSI for automated colon cancer detection in six ex-vivo specimens, employing a spectral-spatial patch-based classification approach. The results demonstrate the feasibility in assessing the benign and malignant boundaries of the lesion with a sensitivity of 0.88 and specificity of 0.78. The results are compared with the state-of-the-art deep learning based approaches. The method with a new hybrid CNN outperforms the state-of theart approaches, (0.74 vs. 0.82 AUC). This study paves the way for further investigation towards improving surgical outcomes with HSI.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,,
24,Convolution Pyramid Network: A Classification Network on Coronary Artery Angiogram Images,,,1186-1189,"Chen Shuang,Tang Yang,Shi Xiaotong,Zhang Honggang,Xie Lihua,Xu Bo","Chen S,Tang Y,Shi XT,Zhang HG,Xie LH,Xu B",Chen S,,Beijing University of Posts & Telecommunications,"With the development of Convolutional Neural Network, the classification on ordinary natural images has made remarkable progress by using single feature maps. However, it is difficult to always produce good results on coronary artery angiograms because there is a lot of photographing noise and small class gaps between the classification targets on angiograms. In this paper, we propose a new network to enhance the richness and relevance of features in the training process by using multiple convolutions with different kernel sizes, which can improve the final classification result. Our network has a strong generalization ability, that is, it can perform a variety of classification tasks on angiograms better. Compared with some state-of-the-art image classification networks, the classification recall increases by 30.5% and precision increases by 19.1% in the best results of our network.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,,
25,End-to-End Deep Learning Model for Cardiac Cycle Synchronization from Multi-View Angiographic Sequences,,,1190-1193,"Royer-Rivard Raphael,Girard Fantin,Dahdah Nagib,Cheriet Farida","Royer-Rivard R,Girard F,Dahdah N,Cheriet F",Royer-Rivard R,,Universite de Montreal,"Dynamic reconstructions (3D+T) of coronary arteries could give important perfusion details to clinicians. Temporal matching of the different views, which may not be acquired simultaneously, is a prerequisite for an accurate stereo-matching of the coronary segments. In this paper, we show how a neural network can be trained from angiographic sequences to synchronize different views during the cardiac cycle using raw x-ray angiography videos exclusively. First, we train a neural network model with angiographic sequences to extract features describing the progression of the cardiac cycle. Then, we compute the distance between the feature vectors of every frame from the first view with those from the second view to generate distance maps that display stripe patterns. Using pathfinding, we extract the best temporally coherent associations between each frame of both videos. Finally, we compare the synchronized frames of an evaluation set with the ECG signals to show an alignment with 96.04% accuracy.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,http://arxiv.org/pdf/2009.02345,
26,Improving the generalization of deep learning methods to segment the left ventricle in short axis MR images,,,1203-1206,"Graves Catharine V.,Moreno Ramon A.,Rebelo Marina S.,Nomura Cesar H.,Gutierrez Marco A.","Graves CV,Moreno RA,Rebelo MS,Nomura CH,Gutierrez MA",Graves CV,,Universidade de Sao Paulo,"Cardiovascular disease is one of the major health problems worldwide. In clinical practice, cardiac magnetic resonance imaging (CMR) is considered the gold-standard imaging modality for the evaluation of the function and structure of the left ventricle (LV). More recently, deep learning methods have been used to segment LV with impressive results. On the other hand, this kind of approach is prone to overfit the training data, and it does not generalize well between different data acquisition centers, thus creating constraints to the use in daily routines. In this paper, we explore methods to improve the generalization in the segmentation performed by a convolutional neural network. We applied a U-net based architecture and compared two different pre-processing methods to improve uniformity in the image contrast between five cross-dataset training and testing. Overall, we were able to perform the segmentation of the left ventricle using multiple cross-dataset combinations of train and test, with a mean endocardium dice score of 0.82.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,,
27,Automatic Pulmonary Vein and Left Atrium Segmentation for TAPVC Preoperative Evaluation Using V-Net with Grouped Attention,,,1207-1210,"Li Jiang,Chen Huai,Zhu Fang,Wen Chen,Chen Huiwen,Wang Lisheng","Li J,Chen H,Zhu F,Wen C,Chen HW,Wang LS",Li J,,Shanghai Jiao Tong University,"Accurate segmentation of pulmonary vein (PV) and left atrium (LA) is essential for the preoperative evaluation and planning of total anomalous pulmonary venous connection (TAPVC), which is a rare but mortal congenital heart disease of children. However, manual segmentation is time-consuming and insipid. To free radiologists from the repetitive work, we propose an automatic deep learning method to segment PV and LA from Low-Dose CT images. In the method, attention mechanism is incorporated into the widely used V-Net and a novel grouped attention module is applied to enforce the segmentation performance of the V-Net. We evaluate our method on 68 3D Low-Dose CT images scanned from patients with TAPVC. The experiment result shows that our method outperforms the popular 3D-UNet and V-Net, with mean dice similarity coefficient (DSC) of 0.795 and 0.834 for the PV and LA respectively.",VENOUS CONNECTION,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,"VENOUS,CONNECTION",,,
28,A Systematic Search over Deep Convolutional Neural Network Architectures for Screening Chest Radiographs,,,1225-1228,"Mitra Arka,Chakravarty Arunava,Ghosh Nirmalya,Sarkar Tandra,Sethuraman Ramanathan,Sheet Debdoot","Mitra A,Chakravarty A,Ghosh N,Sarkar T,Sethuraman R,Sheet D",Chakravarty A,,Indian Institute of Technology System (IIT System),"Chest radiographs are primarily employed for the screening of pulmonary and cardio-/thoracic conditions. Being undertaken at primary healthcare centers, they require the presence of an on-premise reporting Radiologist, which is a challenge in low and middle income countries. This has inspired the development of machine learning based automation of the screening process. While recent efforts demonstrate a performance benchmark using an ensemble of deep convolutional neural networks (CNN), our systematic search over multiple standard CNN architectures identified single candidate CNN models whose classification performances were found to be at par with ensembles. Over 63 experiments spanning 400 hours, executed on a 11:3 FP32 TensorTFLOPS compute system, we found the Xception and ResNet-18 architectures to be consistent performers in identifying co-existing disease conditions with an average AUC of 0.87 across nine pathologies. We conclude on the reliability of the models by assessing their saliency maps generated using the randomized input sampling for explanation (RISE) method and qualitatively validating them against manual annotations locally sourced from an experienced Radiologist. We also draw a critical note on the limitations of the publicly available CheXpert dataset primarily on account of disparity in class distribution in training vs. testing sets, and unavailability of sufficient samples for few classes, which hampers quantitative reporting due to sample insufficiency.","Chest X-ray,CNN,transfer learning",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,http://arxiv.org/pdf/2004.11693,
29,Learning Decision Ensemble using a Graph Neural Network for Comorbidity Aware Chest Radiograph Screening,,,1234-1237,"Chakravarty Arunava,Sarkar Tandra,Ghosh Nirmalya,Sethuraman Ramanathan,Sheet Debdoot","Chakravarty A,Sarkar T,Ghosh N,Sethuraman R,Sheet D",Chakravarty A,,Indian Institute of Technology System (IIT System),"Chest radiographs are primarily employed for the screening of cardio, thoracic and pulmonary conditions. Machine learning based automated solutions are being developed to reduce the burden of routine screening on Radiologists, allowing them to focus on critical cases. While recent efforts demonstrate the use of ensemble of deep convolutional neural networks (CNN), they do not take disease comorbidity into consideration, thus lowering their screening performance. To address this issue, we propose a Graph Neural Network (GNN) based solution to obtain ensemble predictions which models the dependencies between different diseases. A comprehensive evaluation of the proposed method demonstrated its potential by improving the performance over standard ensembling technique across a wide range of ensemble constructions. The best performance was achieved using the GNN ensemble of DenseNet121 with an average AUC of 0.821 across thirteen disease comorbidities.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,http://arxiv.org/pdf/2004.11721,
30,Multi-View Ensemble Convolutional Neural Network to Improve Classification of Pneumonia in Low Contrast Chest X-Ray Images,,,1238-1241,"Ferreira Junior Jose Raniery,Cardona Cardenas Diego Armando,Moreno Ramon Alfredo,de Sa Rebelo Marina de Fatima,Krieger Jose Eduardo,Gutierrez Marco Antonio","Ferreira JR,Cardenas DAC,Moreno RA,Rebelo MDD,Krieger JE,Gutierrez MA",Ferreira JR,,Universidade de Sao Paulo,"Pneumonia is one of the leading causes of childhood mortality worldwide. Chest x-ray (CXR) can aid the diagnosis of pneumonia, but in the case of low contrast images, it is important to include computational tools to aid specialists. Deep learning is an alternative because it can identify patterns automatically, even in low-resolution images. We propose herein a convolutional neural network (CNN) architecture with different training strategies towards detecting pneumonia on CXRs and distinguishing its subforms of bacteria and virus. We also evaluated different image pre-processing methods to improve the classification. This study used CXRs from pediatric patients from a public pneumonia CXR dataset. The pre-processing methods evaluated were image cropping and histogram equalization. To classify the images, we adopted the VGG16 CNN and replaced its fully-connected layers with a customized multilayer perceptron. With this architecture, we proposed and evaluated four different training strategies: original CXR image (baseline), chest-cavity-cropped image (A), and histogram-equalized segmented image (B). The last strategy method (C) implemented is based on ensemble between strategies A and B. The performance was assessed by the area under the ROC curve (AUC) with 95% confidence interval (CI), accuracy, sensitivity, specificity, and F1-score. The ensemble model C yielded the highest performances: AUC of 0.97 (CI: 0.96-0.99) to classify pneumonia vs. normal, and AUC of 0.91 (CI: 0.88-0.94) to classify bacterial vs. viral cases. All models that used pre-processed images showed higher AUC than baseline, which used the original CXR image. Image cropping and histogram equalization reduced irrelevant information from the exam, enhanced contrast, and was able to identify fine CXR texture details. The proposed ensemble model increased the representation of inflammatory patterns from bacteria and viruses with few epochs to train the deep CNNs.",DISEASES,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,DISEASES,,,
31,Dense-Unet: a light model for lung fields segmentation in Chest X-Ray images,,,1242-1245,"Yahyatabar Mohammad,Jouvet Philippe,Cheriet Farida","Yahyatabar M,Jouvet P,Cheriet F",Yahyatabar M,,Universite de Montreal,"Automatic and accurate lung segmentation in chest X-ray (CXR) images is fundamental for computer-aided diagnosis systems since the lung is the region of interest in many diseases and also it can reveal useful information by its contours. While deep learning models have reached high performances in the segmentation of anatomical structures, the large number of training parameters is a concern since it increases memory usage and reduces the generalization of the model. To address this, a deep CNN model called Dense-Unet is proposed in which, by dense connectivity between various layers, information flow increases throughout the network. This lets us design a network with significantly fewer parameters while keeping the segmentation robust. To the best of our knowledge, Dense-Unet is the lightest deep model proposed for the segmentation of lung fields in CXR images. The model is evaluated on the JSRT and Montgomery datasets and experiments show that the performance of the proposed model is comparable with state-of-the-art methods.",RADIOGRAPHS,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,RADIOGRAPHS,,,
32,Machine Learning-Based A Priori Chemotherapy Response Prediction in Breast Cancer Patients using Textural CT Biomarkers,,,1250-1253,"Moghadas-Dastjerdi Hadi,Sha-E-Tallat Hira R.,Sannachi Lakshmanan,Osapoeta Laurentius O.,Sadeghi-Naini Ali,Czarnota Gregory J.","Moghadas-Dastjerdi H,Sha-E-Tallat HR,Sannachi L,Osapoeta LO,Sadeghi-Naini A,Czarnota GJ",Moghadas-Dastjerdi H,,University of Toronto,"Early prediction of cancer response to neoadjuvant chemotherapy (NAC) could permit personalized treatment adjustments for patients, which would improve treatment outcomes and patient survival. For the first time, the efficiency of quantitative computed tomography (qCT) textural and second derivative of textural (SDT) features were investigated and compared in this study. It was demonstrated that intra-tumour heterogeneity can be probed through these biomarkers and used as chemotherapy tumour response predictors in breast cancer patients prior to the start of treatment. These features were used to develop a machine learning approach which provided promising results with cross-validated AUC(0.632+), accuracy, sensitivity and specificity of 0.86, 81%, 74% and 88%, respectively.","PREOPERATIVE CHEMOTHERAPY,NEOADJUVANT,PERFORMANCE,SURVIVAL,CRITERIA,WOMEN",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,"PREOPERATIVE,CHEMOTHERAPY,NEOADJUVANT,PERFORMANCE,SURVIVAL,CRITERIA,WOMEN",,,
33,,,,,,,,,,,,,,,,,,,,
34,An Attention-Guided Deep Neural Network for Annotating Abnormalities in Chest X-ray Images: Visualization of Network Decision Basis,,,1258-1261,"Saednia Khadijeh,Jalalifar Ali,Ebrahimi Shahin,Sadeghi-Naini Ali","Saednia K,Jalalifar A,Ebrahimi S,Sadeghi-Naini A",Saednia K,,York University - Canada,"Despite the potential of deep convolutional neural networks for classification of thorax diseases from chest X-ray images, this task is still challenging as it is categorized as a weakly supervised learning problem, and deep neural networks in general suffer from a lack of interpretability. In this paper, a deep convolutional neural network framework with recurrent attention mechanism was investigated to annotate abnormalities in chest X-ray images. A modified MobileNet architecture was adapted in the framework for classification and the prediction difference analysis method was utilized to visualize the basis of network's decision on each image. A long short-term memory network was utilized as the attention model to focus on relevant regions of each image for classification. The framework was evaluated on NIH chest X-ray dataset. The attention-guided model versus the model with no attention mechanism could annotate the images in an independent test set with an F1-score of 0.58 versus 0.46, and an AUC of 0.94 versus 0.73. The obtained results implied that the proposed attention-guided model could outperform the other methods investigated previously for annotating the same dataset.","Deep convolutional neural network,Visualization method,Attention mechanism,Chest X-ray Annotation",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,,
35,A Deep Learning Aided Drowning Diagnosis for Forensic Investigations using Post-Mortem Lung CT Images,,,1262-1265,"Homma Noriyasu,Zhang Xiaoyong,Qureshi Amber,Konno Takuya,Kawasumi Yusuke,Usui Akihito,Funayama Masato,Bukovsky Ivo,Ichiji Kei,Sugita Norihiro","Homma N,Zhang XY,Qureshi A,Konno T,Kawasumi Y,Usui A,Funayama M,Bukovsky I,Ichiji K,Sugita N",Homma N,,Tohoku University,"Feasibility of computer-aided diagnosis (CAD) systems has been demonstrated in the field of medical image diagnosis. Especially, deep learning based CAD systems showed high performance thanks to its capability of image recognition. However, there is no CAD system developed for post-mortem imaging diagnosis and thus it is still unclear if the CAD system is effective for this purpose. Particulally, the drowning diagnosis is one of the most difficult tasks in the field of forensic medicine because findings of the post-mortem image diagnosis are not specific. To address this issue, we develop a CAD system consisting of a deep convolution neural network (DCNN) to classify post-mortem lung computed tomography (CT) images into two categories of drowning and non-drowning cases. The DCNN was trained by means of transfer learning and performance evaluation was conducted by 10-fold cross validation using 140 drowning cases and 140 non-drowning cases of the CT images. The area under the receiver operating characteristic curve (AUC-ROC) for the DCNN was achieved 0.88 in average. This high performance clearly demonstrated that the proposed DCNN based CAD system has a potential for post-mortem image diagnosis of drowning.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,,
36,Y-Net for Chest X-Ray Preprocessing: Simultaneous Classification of Geometry and Segmentation of Annotations,,,1266-1269,"McManigle John E.,Bartz Raquel R.,Carin Lawrence","McManigle JE,Bartz RR,Carin L",McManigle JE,,Duke University,"Over the last decade, convolutional neural networks (CNNs) have emerged as the leading algorithms in image classification and segmentation. Recent publication of large medical imaging databases have accelerated their use in the biomedical arena. While training data for photograph classification benefits from aggressive geometric augmentation, medical diagnosis - especially in chest radiographs - depends more strongly on feature location. Diagnosis classification results may be artificially enhanced by reliance on radiographic annotations. This work introduces a general pre-processing step for chest x-ray input into machine learning algorithms. A modified Y-Net architecture based on the VGG11 encoder is used to simultaneously learn geometric orientation (similarity transform parameters) of the chest and segmentation of radiographic annotations. Chest x-rays were obtained from published databases. The algorithm was trained with 1000 manually labeled images with augmentation. Results were evaluated by expert clinicians, with acceptable geometry in 95.8% and annotation mask in 96.2% (n = 500), compared to 27.0% and 34.9% respectively in control images (n = 241). We hypothesize that this pre-processing step will improve robustness in future diagnostic algorithms.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,http://arxiv.org/pdf/2005.03824,
37,3D Auto-Segmentation of Mandibular Condyles,,,1270-1273,"Brosset Serge,Dumont Maxime,Bianchi Jonas,Ruellas Antonio,Cevidanes Lucia,Yatabe Marilia,Goncalves Joao,Benavides Erika,Soki Fabiana,Paniagua Beatriz","Brosset S,Dumont M,Bianchi J,Ruellas A,Cevidanes L,Yatabe M,Goncalves J,Benavides E,Soki F,Paniagua B",Brosset S,,University of Michigan System,"Temporomandibular joints (TMJ) like a hinge connect the jawbone to the skull. TMJ disorders could cause pain in the jaw joint and the muscles controlling jaw movement. However, the disease cannot be diagnosed until it becomes symptomatic. It has been shown that bone resorption at the condyle articular surface is already evident at initial diagnosis of TMJ Osteoarthritis (OA). Therefore, analyzing the bone structure will facilitate the disease diagnosis. The important step towards this analysis is the condyle segmentation. This article deals with a method to automatically segment the temporomandibular joint condyle out of cone beam CT (CBCT) scans. In the proposed method we denoise images and apply 3D active contour and morphological operations to segment the condyle. The experimental results show that the proposed method yields the Dice score of 0.9461 with the standards deviation of 0.0888 when it is applied on CBCT images of 95 patients. This segmentation will allow large datasets to be analyzed more efficiently towards data sciences and machine learning approaches for disease classification.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7771389,
38,Multiphase Computed Tomographic Angiography with Bone Subtraction Using 3D Multichannel Convolution Neural Networks,,,1274-1277,"Huang Adam,Cheng Wen-Hsiang,Lee Chung-Wei,Yang Chung-Yi,Liu Hon-Man","Huang A,Cheng WH,Lee CW,Yang CY,Liu HM",Huang A,,National Central University,"Multiphase computed tomographic angiography (CTA) have been demonstrated to be a reliable imaging tool for evaluating cerebral collateral circulation that can be used to select acute ischemic patients for recanalization therapy. We proposed using bone subtraction techniques to visualize multiphase CTA for clinicians to make fast and consistent decisions in the imaging triage of acute stroke patients. A total of 40 multiphase brain CTA datasets were collected and processed by two bone subtraction methods. The reference method used pre-contrast (phase 0) scans to create ground truth bone masks by thresholding. The tested method used only contrast enhanced (phases 1, 2, and 3) scans to extract bone masks with two versions (U-net and atrous) of 3D multichannel convolution neural networks (CNNs) in a supervised deep learning paradigm for semantic segmentation. Half (n = 20) of the datasets were used to train and half (n = 20) were used to test the conventional 3D U-net and a patch-based 3D multichannel atrous CNN. The tested U-net and atrous CNNs achieved a mean intersection over union (IoU) scores of 90.0% +/- 2.2 and 93.9% +/- 1.2 respectively.","CT ANGIOGRAPHY,STROKE,THROMBECTOMY",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,"CT,ANGIOGRAPHY,STROKE,THROMBECTOMY",,,
39,Deep Learning Based Lung Region Segmentation with Data Preprocessing by Generative Adversarial Nets,,,1278-1281,"Nitta Jumpei,Nakao Megumi,Imanishi Keiho,Matsuda Tetsuya","Nitta J,Nakao M,Imanishi K,Matsuda T",Nitta J,,Kyoto University,"In endoscopic surgery, it is necessary to understand the three-dimensional structure of the target region to improve safety. For organs that do not deform much during surgery, preoperative computed tomography (CT) images can be used to understand their three-dimensional structure, however, deformation estimation is necessary for organs that deform substantially. Even though the intraoperative deformation estimation of organs has been widely studied, two-dimensional organ region segmentations from camera images are necessary to perform this estimation. In this paper, we propose a region segmentation method using U-net for the lung, which is an organ that deforms substantially during surgery. Because the accuracy of the results for smoker lungs is lower than that for non-smoker lungs, we improved the accuracy by translating the texture of the lung surface using a CycleGAN.",IMAGES,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,IMAGES,,https://repository.kulib.kyoto-u.ac.jp/dspace/bitstream/2433/265387/1/EMBC44109.2020.9176214.pdf,
40,MDL-IWS: Multi-view Deep Learning with Iterative Watershed for Pulmonary Fissure Segmentation,,,1282-1285,"Roy Rukhmini,Mazumdar Suparna,Chowdhury Ananda S.","Roy R,Mazumdar S,Chowdhury AS",Roy R,,Jadavpur University,"Pulmonary fissure segmentation is important for localization of lung lesions which include nodules at respective lobar territories. This can be very useful for diagnosis as well as treatment planning. In this paper, we propose a novel coarse-to-fine fissure segmentation approach by proposing a Multi-View Deep Learning driven Iterative WaterShed Algorithm (MDL-IWS). Coarse fissure segmentation obtained from multi-view deep learning yields incomplete fissure volume of interest (VOI) with additional false positives. An iterative watershed algorithm (IWS) is presented to achieve fine segmentation of fissure surfaces. As a part of the IWS algorithm, surface fitting is used to generate a more accurate fissure VOI with substantial reduction in false positives. Additionally, a weight map is used to reduce the over-segmentation of watershed in subsequent iterations. Experiments on the publicly available LOLA11 dataset clearly reveal that our method outperforms several state-of-the-art competitors.","Pulmonary Fissure Segmentation,Iterative watershed,Deep Learning",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,,
41,Volumetric Choroidal Segmentation Using Sequential Deep Learning Approach in High Myopia Subjects,,,1286-1289,"Cahyo Dheo A. Y.,Wong Damon W. K.,Yow Ai Ping,Saw Seang-Mei,Schmetterer Leopold","Cahyo DAY,Wong DWK,Yow AP,Saw SM,Schmetterer L",Cahyo DAY,,"Singapore Eye Res Inst SERI, Ocular Imaging Dept, Singapore, Singapore.","Many ocular diseases are associated with choroidal changes. Therefore, it is crucial to be able to segment the choroid to study its properties. Previous methods for choroidal segmentation have focused on single cross-sectional scans. Volumetric choroidal segmentation has yet to be widely reported. In this paper, we propose a sequential segmentation approach using a variation of U-Net with a bidirectional CLSTM(Convolutional Long Short Term Memory) module in the bottleneck region. The model is evaluated on volumetric scans from 40 high myopia subjects, obtained using SS-OCT(Swept Source Optical Coherence Tomography). A comparison with other U-Net-based variants is also presented. The results demonstrate that volumetric segmentation of the choroid can be achieved with an accuracy of IoU(Intersection over Union) 0.92.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,,
42,Feasible Study on Intracranial Hemorrhage Detection and Classification using a CNN-LSTM Network,,,1290-1293,"Ko Hoon,Chung Heewon,Lee Hooseok,Lee Jinseok","Ko H,Chung H,Lee H,Lee J",Ko H,,Wonkwang University,"Intracranial hemorrhage (ICH) is a life-threatening condition, the outcome of which is associated with stroke, trauma, aneurysm, vascular malformations, high blood pressure, illicit drugs and blood clotting disorders. In this study, we presented the feasibility of the automatic identification and classification of ICH using a head CT image based on deep learning technique. The subtypes of ICH for the classification was intraparenchymal, intraventricular, subarachnoid, subdural and epidural. We first performed windowing to provide three different images: brain window, bone window and subdural window, and trained 4,516,842 head CT images using CNN-LSTM model. We used the Xception model for the deep CNN, and 64 nodes and 32 timesteps for LSTM. For the performance evaluation, we tested 727,392 head CT images, and found the resultant weighted multi-label logarithmic loss was 0.07528. We believe that our proposed method enhances the accuracy of ICH identification and classification and can assist radiologists in the interpretation of head CT images, particularly for brain-related quantitative analysis.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,,
43,Malocclusion Classification on 3D Cone-Beam CT Craniofacial Images Using Multi-Channel Deep Learning Models,,,1294-1298,"Kim Incheol,Misra Dharitri,Rodriguez Laritza,Gill Michael,Liberton Denise K.,Almpani Konstantinia,Lee Janice S.,Antani Sameer","Kim I,Misra D,Rodriguez L,Gill M,Liberton DK,Almpani K,Lee JS,Antani S",Antani S,,National Institutes of Health (NIH) - USA,"Analyzing and interpreting cone-beam computed tomography (CBCT) images is a complicated and often time-consuming process. In this study, we present two different architectures of multi-channel deep learning (DL) models: ""Ensemble"" and ""Synchronized multi-channel"", to automatically identify and classify skeletal malocclusions from 3D CBCT craniofacial images. These multi-channel models combine three individual single-channel base models using a voting scheme and a two-step learning process, respectively, to simultaneously extract and learn a visual representation from three different directional views of 2D images generated from a single 3D CBCT image. We also employ a visualization method called ""Class-selective Relevance Mapping"" (CRM) to explain the learned behavior of our DL models by localizing and highlighting a discriminative area within an input image. Our multi-channel models achieve significantly better performance overall (accuracy exceeding 93%), compared to single-channel DL models that only take one specific directional view of 2D projected image as an input. In addition, CRM visually demonstrates that a DL model based on the sagittal-left view of 2D images outperforms those based on other directional 2D images.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,,
44,Statistical Image Restoration for Low-Dose CT using Convolutional Neural Networks,,,1303-1306,"Choi Kihwan,Kim Sungwon","Choi K,Kim S",Choi K,,Korea Institute of Science & Technology (KIST),"Deep learning has recently attracted widespread interest as a means of reducing noise in low-dose CT (LDCT) images. Deep convolutional neural networks (CNNs) are typically trained to transfer high-quality image features of normaldose CT (NDCT) images to LDCT images. However, existing deep learning approaches for denoising LDCT images often overlook the statistical property of CT images. In this paper, we propose an approach to statistical image restoration for LDCT using deep learning (StatCNN). We introduce a loss function to incorporate the noise property in the image domain derived from the noise statistics in the sinogram domain. In order to capture the spatially-varying statistics of axial CT images, we increase the receptive fields of the proposed network to cover full-size CT slices. In addition, the proposed network utilizes z-directional correlation by taking multiple consecutive CT slices as input. For performance evaluation, the proposed network was thoroughly trained and tested by leave-one-out cross-validation with a dataset consisting of LDCT-NDCT image pairs. The experimental results showed that the denoising networks successfully reduced the noise level and restored the image details without adding artifacts. This study demonstrates that the statistical deep learning approach can transfer the image style from NDCT images to LDCT images without loss of anatomical information.","BEAM COMPUTED-TOMOGRAPHY,RECONSTRUCTION",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,"BEAM,COMPUTED-TOMOGRAPHY,RECONSTRUCTION",,,
45,Metal Artifacts Reduction in CT Scans using Convolutional Neural Network with Ground Truth Elimination,,,1319-1322,"Mai Qi,Wan Justin W. L.","Mai Q,Wan JWL",Mai Q,,University of Waterloo,"Metal artifacts are very common in CT scans since metal insertion or replacement is performed for enhancing certain functionality or mechanism of patient's body. These streak artifacts could degrade CT image quality severely, and consequently, they could influence clinician's diagnosis. Many existing supervised learning methods approaching this problem assume the availability of clean images data, images free of metal artifacts, at the part with metal implant. However, in clinical practices, those clean images do not usually exist. Therefore, there is no support for the existing supervised learning based methods to work clinically. We focus on reducing the streak artifacts on the hip scans and propose a convolutional neural network based method to eliminate the need of the clean images at the implant part during model training. The idea is to use the scans of the parts near the hip for model training. Our method is able to suppress the artifacts in corrupted images, highly improve the image quality, and preserve the details of surrounding tissues, without using any clean hip scans. We apply our method on clinical CT hip scans from multiple patients and obtain artifact-free images with high image quality.","Computed Tomography (CT),Convolutional Neural Network (CNN),machine learning,ground truth",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,,
46,Predicting Local Failure after Stereotactic Radiation Therapy in Brain Metastasis using Quantitative CT and Machine Learning,,,1323-1326,"Jaberipour Majid,Sahgal Arjun,Soliman Hany,Sadeghi-Naini Ali","Jaberipour M,Sahgal A,Soliman H,Sadeghi-Naini A",Jaberipour M,,University of Toronto,"Despite recent advances in cancer treatment, the prognosis of patients diagnosed with brain metastasis is still poor. The median survival is limited to months even for patients undergoing treatment. Radiation therapy is a main component of treatment for brain metastasis. However, radiotherapy cannot control local progression in up to 20% of the metastatic brain tumours. An early prediction of radiotherapy outcome for individual patients could facilitate therapy adjustments to improve its efficacy. This study investigated the potential of quantitative CT biomarkers in conjunction with machine learning methods to predict local failure after radiotherapy in brain metastasis. Volumetric CT images were acquired for radiation treatment planning from 120 patients undergoing stereotactic radiotherapy. Quantitative features characterizing the morphology and texture were extracted from different regions of each lesion. A feature reduction/selection framework was adapted to define a quantitative CT biomarker of radiotherapy outcome. Different machine learning methods were applied and evaluated to predict the local failure outcome at pre-treatment. The optimum biomarker consisting of two features in conjunction with an AdaBoost with decision tree could predict the local failure outcome with 71% accuracy on an independent test set (20 patients, 31 lesions). This study is a step forward towards prediction of radiotherapy outcome in brain metastasis using quantitative imaging and machine learning.",TUMOR-CELL DEATH,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,"TUMOR-CELL,DEATH",,,
47,Lumbar Vertebrae Synthetic Segmentation in Computed Tomography Images Using Hybrid Deep Generative Adversarial Networks,,,1327-1330,"Malinda Vania,Lee Deukhee","Malinda V,Lee D",Malinda V,,Korea Institute of Science & Technology (KIST),"The lumbar vertebrae segmentation in Computed tomography (CT) is challenging due to the scarcity of the labeled training data that we define as paired training data for the deep learning technique. Much of the available data is limited to the raw CT scans, unlabeled by radiologists. To handle the scarcity of labeled data, we utilized a hybrid training system by combining paired and unpaired training data and construct a hybrid deep segmentation generative adversarial network (Hybrid-SegGAN). We develop a total automatic approach for lumbar vertebrae segmentation in CT images using Hybrid-SegGAN for synthetic segmentation. Our network receives paired and unpaired data, discriminates between the two sets of data, and processes each through separate phases. We used CT images from 120 patients to demonstrate the performance of the proposed method and extensively evaluate the segmentation results against their ground truth by using 12 performance measures. The result analysis of the proposed method suggests its feasibility to improve the capabilities of deep learning segmentation without demanding the time-consuming annotation procedure for labeled and paired data.",DIAGNOSIS,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,DIAGNOSIS,,,
48,Evaluating Deep Learning Algorithms in Pulmonary Nodule Detection,,,1335-1338,"Traore Abdarahmane,Ly Abdoulaye O.,Akhloufi Moulay A.","Traore A,Ly AO,Akhloufi MA",Traore A,,University of Moncton,"Lung cancer is considered the deadliest cancer worldwide. In order to detect it, radiologists need to inspect multiple Computed Tomography (CT) scans. This task is tedious and time consuming. In recent years, promising methods based on deep learning object detection algorithms were proposed for the automatic nodule detection and classification. With those techniques, Computed Aided Detection (CAD) software can be developed to alleviate radiologist's burden and help speed-up the screening process. However, among available object detection frameworks, there are just a limited number that have been used for this purpose. Moreover, it can be challenging to know which one to choose as a baseline for the development of a new application for this task. Hence, in this work we propose a benchmark of recent state-of-the-art deep learning detectors such as Faster-RCNN, YOLO, SSD, RetinaNet and EfficientDet in the challenging task of pulmonary nodule detection. Evaluation is done using automatically segmented 2D images extracted from volumetric chest CT scans.",IMAGES,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,IMAGES,,,
49,An innovative radiomics approach to predict response to chemotherapy of liver metastases based on CT images,,,1339-1342,"Giannini Valentina,Defeudis Arianna,Rosati Samanta,Cappello Giovanni,Mazzetti Simone,Panic Jovana,Regge Daniele,Balestra Gabriella","Giannini V,Defeudis A,Rosati S,Cappello G,Mazzetti S,Panic J,Regge D,Balestra G",Giannini V,,IRCCS Fondazione del Piemonte per l'Oncologia,"Liver metastases (mts) from colorectal cancer (CRC) can have different responses to chemotherapy in the same patient. The aim of this study is to develop and validate a machine learning algorithm to predict response of individual liver mts. 22 radiomic features (RF) were computed on pre-treatment portal CT scans following a manual segmentation of mts. RFs were extracted from 7x7 Region of Interests (ROIs) that moved across the image by step of 2 pixels. Liver mts were classified as non-responder (R-) if their largest diameter increased more than 3 mm after 3 months of treatment and responder (R+), otherwise. Features selection (FS) was performed by a genetic algorithm and classification by a Support Vector Machine (SVM) classifier. Sensitivity, specificity, negative (NPV) and positive (PPV) predictive values were evaluated for all lesions in the training and validation sets, separately. On the training set, we obtained sensitivity of 86%, specificity of 67%, PPV of 89% and NPV of 61%, while, on the validation set, we reached a sensitivity of 73%, specificity of 47%, PPV of 64% and NPV of 57%. Specificity was biased by the low number of R-lesions on the validation set. The promising results obtained in the validation dataset should be extended to a larger cohort of patient to further validate our method.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,,
50,An Approach for Asbestos-related Pleural Plaque Detection,,,1343-1346,"Sousa Azael Melo,Castelo-Fernandez Cesar,Osaku Daniel,Bagatin Ericson,Reis Fabiano,Falcao Alexandre X.","Sousa AM,Castelo-Fernandez C,Osaku D,Bagatin E,Reis F,Falcao AX",Sousa AM,,Universidade Estadual de Campinas,"Asbestos is a toxic ore widely used in construction and commercial products. Asbestos tends to dissolve into fibers and after years inhaling them, these fibers calcify and form plaques on the pleura. Despite being benign, pleural plaques may indicate an immunologic deficiency or dysfunctional lung areas. We propose a pipeline for asbestos-related pleural plaque detection in CT images of the human thorax based on the following operations: lung segmentation, 3D patch selection along the pleura, a convolutional neural network (CNN) for feature extraction, and classification by support vector machines (SVM). Due to the scarcity of publicly available and annotated datasets of pleural plaques, the proposed CNN relies on architecture learning with random weights obtained by a PCA-based approach instead of using traditional filter learning by backpropagation. Experiments show that the proposed CNN can outperform its counterparts based on backpropagation for small training sets.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,,
51,Unsupervised 3D End-to-end Deformable Network for Brain MRI Registration,,,1355-1359,"Zhu Zhenyu);,Cao Yiqin);,Qin Chenchen);,Rao Yi);,Ni Dong);,Wang Yi","Zhu ZY,Cao YQ,Qin CC,Rao Y,Ni D,Wang Y",Wang Y,,Shenzhen University,"Volumetric medical image registration has important clinical significance. Traditional registration methods may be time-consuming when processing large volumetric data due to their iterative optimizations. In contrast, existing deep learning-based networks can obtain the registration quickly. However, most of them require independent rigid alignment before deformable registration; these two steps are often performed separately and cannot be end-to-end. Moreover, registration ground-truth is difficult to obtain for supervised learning methods. To tackle the above issues, we propose an unsupervised 3D end-to-end deformable registration network. The proposed network cascades two subnetworks; the first one is for obtaining affine alignment, and the second one is a deformable subnetwork for achieving the non-rigid registration. The parameters of the two subnetworks are shared. The global and local similarity measures are used as loss functions for the two subnetworks, respectively. The trained network can perform end-to-end deformable registration. We conducted experiments on brain MRI datasets (LPBA40, Mindboggle101, and IXI) and experimental results demonstrate the efficacy of the proposed registration network.",IMAGE REGISTRATION,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,"IMAGE,REGISTRATION",,,
52,Multi-scale U-net with Edge Guidance for Multimodal Retinal Image Deformable Registration,,,1360-1363,"Tian Yuntong,Hu Yan,Ma Yuhui,Hao Huaying,Mou Lei,Yang Jianlong,Zhao Yitian,Liu Jiang","Tian YT,Hu Y,Ma YH,Hao HY,Mou L,Yang JL,Zhao YT,Liu J",Liu J,,Chinese Academy of Sciences,"Registration of multimodal retinal images is of great importance in facilitating the diagnosis and treatment of many eye diseases, such as the registration between color fundus images and optical coherence tomography (OCT) images. However, it is difficult to obtain ground truth, and most existing algorithms are for rigid registration without considering the optical distortion. In this paper, we present an unsupervised learning method for deformable registration between the two images. To solve the registration problem, the structure achieves a multi-level receptive field and takes contour and local detail into account. To measure the edge difference caused by different distortions in the optics center and edge, an edge similarity (ES) loss term is proposed, so loss function is composed by local cross-correlation, edge similarity and diffusion regularizer on the spatial gradients of the deformation matrix. Thus, we propose a multi-scale input layer, U-net with dilated convolution structure, squeeze excitation (SE) block and spatial transformer layers. Quantitative experiments prove the proposed framework is best compared with several conventional and deep learning-based methods, and our ES loss and structure combined with U-net and multi-scale layers achieve competitive results for normal and abnormal images.","Multimodal Registration,Deep Learning,Deformable Registration,Color Fundus,Optical Coherence Tomography",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,,
53,Lung CT Image Registration through Landmark-constrained Learning with Convolutional Neural Network,,,1368-1371,"Hu Ruxue,Wang Hongkai,Ristaniemi Tapani,Zhu Wentao,Sun Xiaobang","Hu RX,Wang HK,Ristaniemi T,Zhu WT,Sun XB",Wang HK,,Dalian University of Technology,"Accurate registration of lung computed tomography (CT) image is a significant task in thorax image analysis. Recently deep learning-based medical image registration methods develop fast and achieve promising performance on accuracy and speed. However, most of them learned the deformation field through intensity similarity but ignored the importance of aligning anatomical landmarks (e.g., the branch points of airway and vessels). Accurate alignment of anatomical landmarks is essential for obtaining anatomically correct registration. In this work, we propose landmark constrained learning with a convolutional neural network (CNN) for lung CT registration. Experimental results of 40 lung 3D CT images show that our method achieves 0.93 in terms of Dice index and 3.54 mm of landmark Euclidean distance on lung CT registration task, which outperforms state-of-the-art methods in registration accuracy.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,,
54,Effectiveness of GAN-based Synthetic Samples Generation of Minority Patterns in HEp-2 Cell Images,,,1376-1379,"Gupta Krati,Thapar Daksh,Bhavsar Arnav,Sao Anil K.","Gupta K,Thapar D,Bhavsar A,Sao AK",Gupta K,,Indian Institute of Technology System (IIT System),"In this paper, we present a framework to address the augmentation of images for the rare and minor appearance of mitotic type staining patterns, for Human Epithelium Type-2 (HEp-2) cell images. The identification of mitotic patterns among non-mitotic/interphase patterns is important in the process of diagnosis of various autoimmune disorders. This task leads to a pattern classification problem between mitotic v/s interphase. However, among the two classes, typically, the number of mitotic cells are relatively very less. Thus, in this work, we propose to generate synthetic mitotic samples, which can be used to augment the number of mitotic samples and balance the samples of mitotic and interphase patterns in classification paradigm. An effective feature representation is used, to validate the usefulness of the synthetic samples in classification task, along with a subjective validation done by a medical expert. The results demonstrate that the approach of generating and mingling synthetic samples with existing training data works well and yields good performance, with 0.98 balanced class accuracy (BcA) in one case, over a public dataset, i.e., UQ-SNP I3A Task-3 mitotic cell identification dataset.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,,
55,Carcino-Net: A Deep Learning Framework for Automated Gleason Grading of Prostate Biopsies,,,1380-1383,"Lokhande Avinash,Bonthu Saikiran,Singhal Nitin","Lokhande A,Bonthu S,Singhal N",Singhal N,,"AIRA MATRIX, Mumbai, Maharashtra, India.","Gleason scoring for prostate cancer grading is a subjective examination and suffers from suboptimal interobserver and intraobserver variability. To overcome these limitations, we have developed an automated system to grade prostate biopsies. We present a novel deep learning architecture Carcino-Net, which improves semantic segmentation performance. The proposed network is a modified FCN8s with ResNet50 backbone. Using Carcino-Net, we not only report best performance in separating the different grades, we also offer greater accuracy over other state-of-the-art frameworks. The proposed system could expedite the pathology workflow in diagnostic laboratories by triaging high-grade biopsies.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,,
56,Vacuole Segmentation and Quantification in Liver Images of Wistar Rat,,,1396-1399,"Deshmukh Sanket,Lokhande Avinash,Wasnik Ratul,Singhal Nitin","Deshmukh S,Lokhande A,Wasnik R,Singhal N",Singhal N,,"AIRA MATRIX, Mumbai, Maharashtra, India.","Accurate detection of macro and microvesicles in rat models of fatty liver disease is crucial in evaluating the progression of liver disease and identifying potential hepatotoxic findings during drug development. In this paper, we present a deep-learning-based framework for the segmentation of vacuoles in liver images of Wistar rat and study the correlation of automated quantification with expert pathologist's manual evaluation. To address the issue of misclassification of lumina (vascular and bile duct) as large vacuoles, we propose a selective tiling technique to generate tiles that include complete lumina and large vacuoles. A binary encoder-decoder convolution neural network is trained to detect individual vacuoles. We report a sensitivity of 85% and specificity of 98%. Furthermore, the diameter and roundness of the segmented vacuoles are estimated with an error of less than 8%, which supports the high potential of our method in drug development process.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,,
57,Supervision and Source Domain Impact on Representation Learning: A Histopathology Case Study,,,1400-1403,"Sikaroudi Milad,Safarpoor Amir,Ghojogh Benyamin,Shafiei Sobhan,Crowley Mark,Tizhoosh H. R.","Sikaroudi M,Safarpoor A,Ghojogh B,Shafiei S,Crowley M,Tizhoosh HR",Tizhoosh HR,,University of Waterloo,"As many algorithms depend on a suitable representation of data, learning unique features is considered a crucial task. Although supervised techniques using deep neural networks have boosted the performance of representation learning, the need for a large sets of labeled data limits the application of such methods. As an example, high-quality delineations of regions of interest in the field of pathology is a tedious and time-consuming task due to the large image dimensions. In this work, we explored the performance of a deep neural network and triplet loss in the area of representation learning. We investigated the notion of similarity and dissimilarity in pathology whole-slide images and compared different setups from unsupervised and semi-supervised to supervised learning in our experiments. Additionally, different approaches were tested, applying few-shot learning on two publicly available pathology image datasets. We achieved high accuracy and generalization when the learned representations were applied to two different pathology datasets.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,http://arxiv.org/pdf/2005.08629,
58,Diabetic Retinopathy (DR) Severity Level Classification Using Multimodel Convolutional Neural Networks,,,1404-1407,"Abidalkareem Ali J.,Abd Moaed A.,Ibrahim Ali K.,Zhuang Hanqi,Altaher Ali Salem,Ali Ali Muhamed","Abidalkareem AJ,Abd MA,Ibrahim AK,Zhuang HQ,Altaher AS,Ali AM",Abidalkareem AJ,,State University System of Florida,"Diabetic retinopathy (DR) is a progressive eye disease that affects a large portion of working-age adults. DR, which may progress to an irreversible state that causes blindness, can be diagnosed with a comprehensive dilated eye exam. With the eye dilated, the Doctor takes pictures of the inside of the eye via a medical procedure called Fluorescein Angiography, in which a dye is injected into the bloodstream. The dye highlights the blood vessels in the back of the eye so they can be photographed. In addition, the Doctor may request an Optical Coherence Tomography (OCT) exam, by which cross-sectional photos of the retina are produced to measure the thickness of the retina. Early prognostication is vital in treating the disease and preventing it from progressing into advanced irreversible stages. Skilled medical personnel and necessary medical facilities are required to detect DR in its five major stages. In this paper, we propose a diagnostic tool to detect Diabetic retinopathy from fundus images by using an ensemble of multi-inception CNN networks. Our inception block consists of three Convolutional layers with kernel sizes of 3x3, 5x5, and 1x1 that are concatenated deeply and forwarded to the max-pooling layer. We experimentally compare our proposed method with two pre-trained models: VGG16 and GoogleNets. The experiment results show that the proposed method can achieve an accuracy of 93.2% by an ensemble of 10 random networks, compared to 81% obtained with transfer learning based on VGG19.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,,
59,Supervised Machine Learning Segmentation and Quantification of Gastric Pacemaker Cells,,,1408-1411,"Mah Sue Ann,Avci Recep,Du Peng,Vanderwinden Jean-Marie,Cheng Leo K.","Mah SA,Avci R,Du P,Vanderwinden JM,Cheng LK",Cheng LK,,University of Auckland,"Interstitial Cells of Cajal (ICC) are specialized pacemaker cells that generate and actively propagate electrophysiological events called slow waves. Slow waves regulate the motility of the gastrointestinal tract necessary for digesting food. Degradation in the ICC network structure has been qualitatively associated to several gastrointestinal motility disorders. ICC network structure can be obtained using confocal microscopy, but the current limitations in imaging and segmentation techniques have hindered an accurate representation of the networks. In this study, supervised machine learning techniques were applied to extract the ICC networks from 3D confocal microscopy images. The results showed that the Fast Random Forest classification method using Trainable WEKA Segmentation outperformed the Decision Table and Naive Bayes classification methods in sensitivity, accuracy, and F-measure. Using the Fast Random Forest classifier, 12 gastric antrum tissue blocks were segmented and variations in ICC network thickness, density and process width were quantified for the myenteric plexus ICC network (the primary pacemakers). Our findings demonstrated regional variation in ICC network density and thickness along the circumferential and longitudinal axis of the mouse antrum. An inverse relationship was observed in the distal and proximal antrum for density (proximal: 9.8 +/- 4.0% vs distal: 7.6 +/- 4.6%) and thickness (proximal: 15 +/- 3 mu m vs distal: 24 +/- 10 mu m). Limited variation in ICC process width was observed throughout the antrum (5 +/- 1 mu m).","INTERSTITIAL-CELLS,SLOW WAVES,CAJAL,PROPAGATION,ORIGIN,GUT",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,"INTERSTITIAL-CELLS,SLOW,WAVES,CAJAL,PROPAGATION,ORIGIN,GUT",,,
60,Deep Learning Model based Ki-67 Index estimation with Automatically Labelled Data,,,1412-1415,"Lakshmi S.,Ritwik Kotra Venkata Sai,Vijayasenan Deepu,David Sumam S.,Sreeram Saraswathy,Suresh Pooja K.","Lakshmi S,Ritwik KVS,Vijayasenan D,David SS,Sreeram S,Suresh PK",Lakshmi S,,National Institute of Technology (NIT System),"Ki-67 labelling index is a biomarker which is used across the world to predict the aggressiveness of cancer. To compute the Ki-67 index, pathologists normally count the tumour nuclei from the slide images manually; hence it is time-consuming and is subject to inter pathologist variability. With the development of image processing and machine learning, many methods have been introduced for automatic Ki-67 estimation. But most of them require manual annotations and are restricted to one type of cancer. In this work, we propose a pooled Otsu's method to generate labels and train a semantic segmentation deep neural network (DNN). The output is post-processed to find the Ki-67 index. Evaluation of two different types of cancer (bladder and breast cancer) results in a mean absolute error of 3.52%. The performance of the DNN trained with automatic labels is better than DNN trained with ground truth by an absolute value of 1.25%.","NEUROENDOCRINE TUMORS,QUANTIFICATION",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,"NEUROENDOCRINE,TUMORS,QUANTIFICATION",,,
61,Recognizing Magnification Levels in Microscopic Snapshots,,,1416-1419,"Zaveri Manit,Kalra Shivam,Babaie Morteza,Shah Sultaan,Damskinos Savvas,Kashani Hany,Tizhoosh H. R.","Zaveri M,Kalra S,Babaie M,Shah S,Damskinos S,Kashani H,Tizhoosh HR",Zaveri M,,University of Waterloo,"Recent advances in digital imaging has transformed computer vision and machine learning to new tools for analyzing pathology images. This trend could automate some of the tasks in the diagnostic pathology and elevate the pathologist workload. The final step of any cancer diagnosis procedure is performed by the expert pathologist. These experts use microscopes with high level of optical magnification to observe minute characteristics of the tissue acquired through biopsy and fixed on glass slides. Switching between different magnifications, and finding the magnification level at which they identify the presence or absence of malignant tissues is important. As the majority of pathologists still use light microscopy, compared to digital scanners, in many instance a mounted camera on the microscope is used to capture snapshots from significant field-of-views. Repositories of such snapshots usually do not contain the magnification information. In this paper, we extract deep features of the images available on TCGA dataset with known magnification to train a classifier for magnification recognition. We compared the results with LBP, a well-known handcrafted feature extraction method. The proposed approach achieved a mean accuracy of 96% when a multi-layer perceptron was trained as a classifier.","Microscope,pathology,magnification,deep learning",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,CLASSIFICATION,,http://arxiv.org/pdf/2005.03748,
62,SEGMENTATION OF TAU STAINED ALZHEIMERS BRAIN TISSUE USING CONVOLUTIONAL NEURAL NETWORKS,,,1420-1423,"Wurts Alexander,Oakley Derek H.,Hyman Bradley T.,Samsi Siddharth","Wurts A,Oakley DH,Hyman BT,Samsi S",Wurts A,,Worcester Polytechnic Institute,"Alzheimers disease is characterized by complex changes in brain tissue including the accumulation of tau-containing neurofibrillary tangles (NFTs) and dystrophic neurites (DNs) within neurons. The distribution and density of tau pathology throughout the brain is evaluated at autopsy as one component of Alzheimers disease diagnosis. Deep neural networks (DNN) have been shown to be effective in the quantification of tau pathology when trained on fully annotated images. In this paper, we examine the effectiveness of three DNNs for the segmentation of tau pathology when trained on noisily labeled data. We train FCN, SegNet and U-Net on the same set of training images. Our results show that using noisily labeled data, these networks are capable of segmenting tau pathology as well as nuclei in as few as 40 training epochs with varying degrees of success. SegNet, FCN and U-Net are able to achieve a DICE loss of 0.234, 0.297 and 0.272 respectively on the task of segmenting regions of tau. We also apply these networks to the task of segmenting whole slide images of tissue sections and discuss their practical applicability for processing gigapixel sized images.","ConvNets,Segmentation,Tauopathy,DNN",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,"IMAGE-ANALYSIS,CANCER",,,
63,Visualizing Functional Network Connectivity Difference between Healthy Control and Major Depressive Disorder Using an Explainable Machine-learning Method,,,1424-1427,"Chun Ji Ye,Sendi Mohammad S. E.,Sui Jing,Zhi Dongmei,Calhoun Vince D.","Chun JY,Sendi MSE,Sui J,Zhi DM,Calhoun VD",Chun JY,,University System of Georgia,"Major depressive disorder (MDD) is a complex mental disorder characterized by a persistent sad feeling and depressed mood. Recent studies reported differences between healthy control (HC) and MDD by looking to brain networks including default mode and cognitive control networks. More recently there has been interest in studying the brain using advanced machine learning-based classification approaches. However, interpreting the model used in the classification between MDD and HC has not been explored yet. In the current study, we classified MDD from HC by estimating whole-brain connectivity using several classification methods including support vector machine, random forest, XGBoost, and convolutional neural network. In addition, we leveraged the SHapley Additive exPlanations (SHAP) approach as a feature learning method to model the difference between these two groups. We found a consistent result among all classification method in regard of the classification accuracy and feature learning. Also, we highlighted the role of other brain networks particularly visual and sensory motor network in the classification between MDD and HC subjects.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,,
64,Combining Deep Learning with Handcrafted Features for Cell Nuclei Segmentation,,,1428-1431,"Narotamo Hemaxi,Sanches J. Miguel,Silveira Margarida","Narotamo H,Sanches JM,Silveira M",Narotamo H,,Universidade de Lisboa,"Segmentation of cell nuclei in fluorescence microscopy images provides valuable information about the shape and size of the nuclei, its chromatin texture and DNA content. It has many applications such as cell tracking, counting and classification. In this work, we extended our recently proposed approach for nuclei segmentation based on deep learning, by adding to its input handcrafted features. Our handcrafted features introduce additional domain knowledge that nuclei are expected to have an approximately round shape. For round shapes the gradient vector of points at the border point to the center. To convey this information, we compute a map of gradient convergence to be used by the CNN as a new channel, in addition to the fluorescence microscopy image. We applied our method to a dataset of microscopy images of cells stained with DAPI. Our results show that with this approach we are able to decrease the number of missdetections and, therefore, increase the F1-Score when compared to our previously proposed approach. Moreover, the results show that faster convergence is obtained when handcrafted features are combined with deep learning.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,,
65,Interphase Cell Cycle Staging using Deep Learning,,,1432-1435,"Narotamo Hemaxi,Fernandes M. Sofia,Sanches J. Miguel,Silveira Margarida","Narotamo H,Fernandes MS,Sanches JM,Silveira M",Narotamo H,,Universidade de Lisboa,"The progression of cells through the cell cycle is a tightly regulated process and is known to be key in maintaining normal tissue architecture and function. Disruption of these orchestrated phases will result in alterations that can lead to many diseases including cancer. Regrettably, reliable automatic tools to evaluate the cell cycle stage of individual cells are still lacking, in particular at interphase. Therefore, the development of new tools for a proper classification are urgently needed and will be of critical importance for cancer prognosis and predictive therapeutic purposes. Thus, in this work, we aimed to investigate three deep learning approaches for interphase cell cycle staging in microscopy images: 1) joint detection and cell cycle classification of nuclei patches; 2) detection of cell nuclei patches followed by classification of the cycle stage; 3) detection and segmentation of cell nuclei followed by classification of cell cycle staging. Our methods were applied to a dataset of microscopy images of nuclei stained with DAPI. The best results (0.908 F1-Score) were obtained with approach 3 in which the segmentation step allows for an intensity normalization that takes into account the intensities of all nuclei in a given image. These results show that for a correct cell cycle staging it is important to consider the relative intensities of the nuclei. Herein, we have developed a new deep learning method for interphase cell cycle staging at single cell level with potential implications in cancer prognosis and therapeutic strategies.","SEGMENTATION,TRACKING",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,"SEGMENTATION,TRACKING",,,
66,A V-Net Based Deep Learning Model for Segmentation and Classification of Histological Images of Gastric Ablation,,,1436-1439,"Aghababaie Zahra,Jamart Kevin,Chan Chih-Hsiang Alexander,Amirapu Satya,Cheng Leo K.,Paskaranandavadivel Niranchan,Avci Recep,Angeli Timothy R.","Aghababaie Z,Jamart K,Chan CHA,Amirapu S,Cheng LK,Paskaranandavadivel N,Avci R,Angeli TR",Angeli TR,,University of Auckland,"Gastric motility disorders are associated with bioelectrical abnormalities in the stomach. Recently, gastric ablation has emerged as a potential therapy to correct gastric dysrhythmias. However, the tissue-level effects of gastric ablation have not yet been evaluated. In this study, radiofrequency ablation was performed in vivo in pigs (n=7) at temperature-control mode (55-80 degrees C, 5-10 s per point). The tissue was excised from the ablation site and routine H&E staining protocol was performed. In order to assess tissue damage, we developed an automated technique using a fully convolutional neural network to segment healthy tissue and ablated lesion sites within the muscle and mucosa layers of the stomach. The tissue segmentation achieved an overall Dice score accuracy of 96.18 +/- 1.0 %, and Jacquard score of 92.77 +/- 1.9 %, after 5-fold cross validation. The ablation lesion was detected with an overall Dice score of 94.16 +/- 0.2 %. This method can be used in combination with high-resolution electrical mapping to define the optimal ablation dose for gastric ablation.","INTERSTITIAL-CELLS,CAJAL",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,"INTERSTITIAL-CELLS,CAJAL",,,
67,High-Fidelity Accelerated MRI Reconstruction by Scan-Specific Fine-Tuning of Physics-Based Neural Networks,,,1481-1484,"Hosseini Seyed Amir Hossein,Yaman Burhaneddin,Moeller Steen,Akcakaya Mehmet","Hosseini SAH,Yaman B,Moeller S,Akcakaya M",Hosseini SAH,,University of Minnesota System,"Long scan duration remains a challenge for high-resolution MRI. Deep learning has emerged as a powerful means for accelerated MRI reconstruction by providing data-driven regularizers that are directly learned from data. These data-driven priors typically remain unchanged for future data in the testing phase once they are learned during training. In this study, we propose to use a transfer learning approach to fine-tune these regularizers for new subjects using a self-supervision approach. While the proposed approach can compromise the extremely fast reconstruction time of deep learning MRI methods, our results on knee MRI indicate that such adaptation can substantially reduce the remaining artifacts in reconstructed images. In addition, the proposed approach has the potential to reduce the risks of generalization to rare pathological conditions, which may be unavailable in the training data.","IMAGING RECONSTRUCTION,PARALLEL,SENSE",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,"IMAGING,RECONSTRUCTION,PARALLEL,SENSE",,http://arxiv.org/pdf/2005.05550,
68,Aberrant Functional Network Connectivity Transition Probability in Major Depressive Disorder,,,1493-1496,"Zendehrouh Elaheh,Sendi Mohammad. S. E.,Sui Jing,Fu Zening,Zhi Dongmei,Lv Luxian,Ma Xiaohong,Ke Qing,Li Xianbin,Wang Chuanyue","Zendehrouh E,Sendi MSE,Sui J,Fu ZN,Zhi DM,Lv LX,Ma XH,Ke Q,Li XB,Wang CY",Zendehrouh E,,University System of Georgia,"Major depressive disorder (MDD) is a common and serious mental disorder characterized by a persistent negative feeling and tremendous sadness. In recent decades, several studies used functional network connectivity (FNC), estimated from resting state functional magnetic resonance imaging (fMRI), to investigate the biological signature of MDD. However, the majority of them have ignored the temporal change of brain interaction by focusing on static FNC (sFNC). Dynamic functional network connectivity (dFNC) that explores temporal patterns of functional connectivity (FC) might provide additional information to its static counterpart. In the current study, by applying k-means clustering on dFNC of MDD and healthy subjects (HCs), we estimated 5 different states. Next, we use the hidden Markov model as a potential biomarker to differentiate the dFNC pattern of MDD patients from HCs. Comparing MDD and HC subjects' hidden Markov model (HMM) features, we have highlighted the role of transition probabilities between states as potential biomarkers and identified that transition probability from a lightly-connected state to highly connected one reduces as symptom severity increases in MDD subjects.","Major depressive disorder,Dynamic functional network connectivity,Machine learning,Resting-state functional magnetic resonance imaging,Hidden Markov model",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,"DEFAULT-MODE,NETWORK",,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8233065,
69,Anxiety and Depression Diagnosis Method Based on Brain Networks and Convolutional Neural Networks,,,1503-1506,"Xie Yunlong,Yang Banghua,Lu Xi,Zheng Minmin,Fan Cunxiu,Bi Xiaoying,Zhou Shu,Li Yingjie","Xie YL,Yang BH,Lu X,Zheng MM,Fan CX,Bi XY,Zhou S,Li YJ",Yang BH,,Shanghai University,"At present, only professional doctors can use the professional scales to diagnose depression and anxiety in clinical practice. In recent years, the problems of detecting the presence of anxiety or depression using Electroencephalography (EEG) has received attention as a way to implement assistant diagnosis, and some researchers explored that there are differences in the degree of prefrontal lateralization and functional connectivity of brain networks between patients with anxiety and depression and normal people. In this paper, we proposed a new approach that combines functional connectivity of brain networks and convolutional neural networks ( CNN) for EEG-based anxiety and depression recognition. EEG data are collected from subjects consisting ten healthy controls and ten patients with anxiety or depression. In this way, we achieved 67.67% classification accuracy. It points out the way to further explore the application of functional connectivity of brain networks and deep learning technology in EEG about patients with anxiety and depression.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,,
70,An Autoencoder-based Approach to Predict Subjective Pain Perception from High-density Evoked EEG Potentials,,,1507-1511,"Wang Jiahao,Wei Mengying,Zhang Li,Huang Gan,Liang Zhen,Li Linling,Zhang Zhiguo","Wang JH,Wei MY,Zhang L,Huang G,Liang Z,Li LL,Zhang ZG",Zhang ZG,,Shenzhen University,"Pain is a subjective experience and clinicians need to treat patients with accurate pain levels. EEG has emerged as a useful tool for objective pain assessment, but due to the low signal-to-noise ratio of pain-related EEG signals, the prediction accuracy of EEG-based pain prediction models is still unsatisfactory. In this paper, we proposed an autoencoder model based on convolutional neural networks for feature extraction of pain-related EEG signals. More precisely, we used EEGNet to build an autoencoder model to extract a small set of features from high-density pain-evoked EEG potentials and then establish a machine learning models to predict pain levels ( high pain vs. low pain) from extracted features. Experimental results show that the new autoencoder-based approach can effectively identify pain-related features and can achieve better classification results than conventional methods.","EEG,pain,deep learning,autoencoder,laser-evoked potentials",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,,
71,Deep Learning for Neuroimaging Segmentation with a Novel Data Augmentation Strategy,,,1516-1519,"Wu Wenshan,Lu Yuhao,Mane Ravikiran,Guan Cuntai","Wu WS,Lu YH,Mane R,Guan CT",Wu WS,,Nanyang Technological University & National Institute of Education (NIE) Singapore,"Brain insults such as cerebral ischemia and intracranial hemorrhage are critical stroke conditions with high mortality rates. Currently, medical image analysis for critical stroke conditions is still largely done manually, which is time-consuming and labor-intensive. While deep learning algorithms are increasingly being applied in medical image analysis, the performance of these methods still needs substantial improvement before they can be widely used in the clinical setting. Among other challenges, the lack of sufficient labelled data is one of the key problems that has limited the progress of deep learning methods in this domain. To mitigate this bottleneck, we propose an integrated method that includes a data augmentation framework using a conditional Generative Adversarial Network (cGAN) which is followed by a supervised segmentation with a Convolutional Neural Network (CNN). The adopted cGAN generates meaningful brain images from specially altered lesion masks as a form of data augmentation to supplement the training dataset, while the CNN incorporates depth-wise-convolution based X-blocks as well as Feature Similarity Module (FSM) to ease and aid the training process, resulting in better lesion segmentation. We evaluate the proposed deep learning strategy on the Anatomical Tracings of Lesions After Stroke (ATLAS) dataset and show that this approach outperforms the current state-of-art methods in task of stroke lesion segmentation.",DISEASE,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,DISEASE,,,
72,Deep learning approaches for bone and bone lesion segmentation on (18)FDG PET/CT imaging in the context of metastatic breast cancer,,,1532-1535,"Moreau Noemie,Rousseau Caroline,Fourcade Constance,Santini Gianmarco,Ferrer Ludovic,Lacombe Marie,Guillerminet Camille,Campone Mario,Colombie Mathilde,Rubeaux Mathieu","Moreau N,Rousseau C,Fourcade C,Santini G,Ferrer L,Lacombe M,Guillerminet C,Campone M,Colombie M,Rubeaux M",Moreau N,,Centre National de la Recherche Scientifique (CNRS),"(18)FDG PET/CT imaging is commonly used in diagnosis and follow-up of metastatic breast cancer, but its quantitative analysis is complicated by the number and location heterogeneity of metastatic lesions. Considering that bones are the most common location among metastatic sites, this work aims to compare different approaches to segment the bones and bone metastatic lesions in breast cancer.
Two deep learning methods based on U-Net were developed and trained to segment either both bones and bone lesions or bone lesions alone on PET/CT images. These methods were cross-validated on 24 patients from the prospective EPICUREseinmeta metastatic breast cancer study and were evaluated using recall and precision to measure lesion detection, as well as the Dice score to assess bones and bone lesions segmentation accuracy.
Results show that taking into account bone information in the training process allows to improve the precision of the lesions detection as well as the Dice score of the segmented lesions. Moreover, using the obtained bone and bone lesion masks, we were able to compute a PET bone index (PBI) inspired by the recognized Bone Scan Index (BSI). This automatically computed PBI globally agrees with the one calculated from ground truth delineations.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,https://hal.archives-ouvertes.fr/hal-02591573/file/EMBC20_Noemie_FinalSubmission.pdf,
73,Combining Superpixels and Deep Learning Approaches to Segment Active Organs in Metastatic Breast Cancer PET Images,,,1536-1539,"Fourcade Constance,Ferrer Ludovic,Santini Gianmarco,Moreau Noemie,Rousseau Caroline,Lacombe Marie,Guillerminet Camille,Colombie Mathilde,Campone Mario,Mateus Diana","Fourcade C,Ferrer L,Santini G,Moreau N,Rousseau C,Lacombe M,Guillerminet C,Colombie M,Campone M,Mateus D",Fourcade C,,Centre National de la Recherche Scientifique (CNRS),"Semi-automatic measurements are performed on (18)FDG PET-CT images to monitor the evolution of metastatic sites in the clinical follow-up of metastatic breast cancer patients. Apart from being time-consuming and prone to subjective approximation, semi-automatic tools cannot make the difference between cancerous regions and active organs, presenting a high (18)FDG uptake.
In this work, we combine a deep learning-based approach with a superpixel segmentation method to segment the main active organs (brain, heart, bladder) from full-body PET images. In particular, we integrate a superpixel SLIC algorithm at different levels of a convolutional network. Results are compared with a deep learning segmentation network alone. The methods are cross-validated on full-body PET images of 36 patients and tested on the acquisitions of 24 patients from a different study center, in the context of the ongoing EPICUREseinmeta study. The similarity between the manually defined organ masks and the results is evaluated with the Dice score. Moreover, the amount of false positives is evaluated through the positive predictive value (PPV).
According to the computed Dice scores, all approaches allow to accurately segment the target organs. However, the networks integrating superpixels are better suited to transfer knowledge across datasets acquired on multiple sites (domain adaptation) and are less likely to segment structures outside of the target organs, according to the PPV.
Hence, combining deep learning with superpixels allows to segment organs presenting a high (18)FDG uptake on PET images without selecting cancerous lesion, and thus improves the precision of the semi-automatic tools monitoring the evolution of breast cancer metastasis.",LOCALIZATION,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,LOCALIZATION,,https://hal.archives-ouvertes.fr/hal-02565092/document,
74,Enhanced Capsule Network for Medical image classification,,,1544-1547,"Zhang Zhe,Ye Shiwei,Liao Pan,Liu Yan,Su Guiping,Sun Yi","Zhang Z,Ye SW,Liao P,Liu Y,Su GP,Sun Y",Zhang Z,,Chinese Academy of Sciences,"Nowadays, cancer has become a major threat to people's lives and health. Convolutional neural network (CNN) has been used for cancer early identification, which cannot achieve the desired results in some cases, such as images with affine transformation. Due to robustness to rotation and affine transformation, capsule network can effectively solve this problem of CNN and achieve the expected performance with less training data, which are very important for medical image analysis. In this paper, an enhanced capsule network is proposed for medical image classification. For the proposed capsule network, the feature decomposition module and multi-scale feature extraction module are introduced into the basic capsule network. The feature decomposition module is presented to extract richer features, which reduces the amount of calculation and speeds up the network convergence. The multi-scale feature extraction module is used to extract important information in the low-level capsules, which guarantees the extracted features to be transmitted to the high-level capsules. The proposed capsule network was applied on PatchCamelyon (PCam) dataset. Experimental results show that it can obtain good performance for medical image classification task, which provides good inspiration for other image classification tasks.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,,
75,Classification of Aortic Stenosis Using ECG by Deep Learning and its Analysis Using Grad-CAM,,,1548-1551,"Hata Erika,Seo Chanjin,Nakayama Masafumi,Iwasaki Kiyotaka,Ohkawauchi Takaaki,Ohya Jun","Hata E,Seo C,Nakayama M,Iwasaki K,Ohkawauchi T,Ohya J",Hata E,,Waseda University,"This paper proposes an automatic method for classifying Aortic valvular stenosis (AS) using ECG (Electrocardiogram) images by the deep learning whose training ECG images are annotated by the diagnoses given by the medical doctor who observes the echocardiograms. Besides, it explores the relationship between the trained deep learning network and its determinations, using the Grad-CAM.
In this study, one-beat ECG images for 12-leads and 4-leads are generated from ECG's and train CNN's (Convolutional neural network). By applying the Grad-CAM to the trained CNN's, feature areas are detected in the early time range of the one-beat ECG image. Also, by limiting the time range of the ECG image to that of the feature area, the CNN for the 4-lead achieves the best classification performance, which is close to expert medical doctors' diagnoses.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,,
76,A Study on Tuberculosis Classification in Chest X-ray Using Deep Residual Attention Networks,,,1552-1555,"Zhang Ran,Duan Huichuan,Cheng Jiezhi,Zheng Yuanjie","Zhang R,Duan HC,Cheng JZ,Zheng YJ",Duan HC; Zheng YJ,,Shandong Normal University,"The introduction of deep learning techniques for the computer-aided detection scheme has shed a light for real incorporation into the clinical workflow. In this work, we focus on the effect of attention in deep neural networks on the classification of tuberculosis x-ray images. We propose a Convolutional Block Attention Module (CBAM), a simple but effective attention module for feed-forward convolutional neural networks. Given an intermediate feature map, our module infers attention maps and multiplied it to the input feature map for adaptive feature refinement. It achieves high precision and recalls while localizing objects with its attention. We validate the performance of our approach on a standard-compliant data set, including a dataset of 4990 x-ray chest radiographs from three hospitals and show that our performance is better than the models used in previous work.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,,
77,Autofluorescence Bronchoscopy Video Analysis for Lesion Frame Detection,,,1556-1559,"Chang Qi,Bascom Rebecca,Toth Jennifer,Ahmad Danish,Higgins William E.","Chang Q,Bascom R,Toth J,Ahmad D,Higgins WE",Higgins WE,,Pennsylvania Commonwealth System of Higher Education (PCSHE),"Because of the significance of bronchial lesions as indicators of early lung cancer and squamous cell carcinoma, a critical need exists for early detection of bronchial lesions. Autofluorescence bronchoscopy (AFB) is a primary modality used for bronchial lesion detection, as it shows high sensitivity to suspicious lesions. The physician, however, must interactively browse a long video stream to locate lesions, making the search exceedingly tedious and error prone. Unfortunately, limited research has explored the use of automated AFB video analysis for efficient lesion detection. We propose a robust automatic AFB analysis approach that distinguishes informative and uninformative AFB video frames in a video. In addition, for the informative frames, we determine the frames containing potential lesions and delineate candidate lesion regions. Our approach draws upon a combination of computer-based image analysis, machine learning, and deep learning. Thus, the analysis of an AFB video stream becomes more tractable. Using patient AFB video, 99.5%/90.2% of test frames were correctly labeled as informative/uninformative by our method versus 99.2%/47.6% by ResNet. In addition, >= 97% of lesion frames were correctly identified, with false positive and false negative rates <= 3%.","EARLY LUNG-CANCER,CLASSIFICATION,ENDOSCOPY",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,"EARLY,LUNG-CANCER,CLASSIFICATION,ENDOSCOPY",,,
78,A Multi-Label Deep Learning Model with Interpretable Grad-CAM for Diabetic Retinopathy Classification,,,1560-1563,"Jiang Hongyang,Xu Jie,Shi Rongjie,Yang Kang,Zhang Dongdong,Gao Mengdi,Ma He,Qian Wei","Jiang HY,Xu J,Shi RJ,Yang K,Zhang DD,Gao MD,Ma H,Qian W",Xu J,,Capital Medical University,"The characteristics of diabetic retinopathy (DR) fundus images generally consist of multiple types of lesions which provided strong evidence for the ophthalmologists to make diagnosis. It is particularly significant to figure out an efficient method to not only accurately classify DR fundus images but also recognize all kinds of lesions on them. In this paper, a deep learning-based multi-label classification model with Gradient- weighted Class Activation Mapping (Grad-CAM) was proposed, which can both make DR classification and automatically locate the regions of different lesions. To reducing laborious annotation work and improve the efficiency of labeling, this paper innovatively considered different types of lesions as different labels for a fundus image so that this paper changed the task of lesion detection into that of image classification. A total of five labels were pre-defined and 3228 fundus images were collected for developing our model. The architecture of deep learning model was designed by ourselves based on ResNet. Through experiments on the test images, this method acquired a sensitive of 93.9% and a specificity of 94.4% on DR classification. Moreover, the corresponding regions of lesions were reasonably outlined on the DR fundus images.","Diabetic Retinopathy,Deep Learning,Multi-label Classification,Grad-CAM",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,,
79,Multiple Slice k-space Deep Learning for Magnetic Resonance Imaging Reconstruction,,,1564-1567,"Du Tianming,Zhang Yanci,Shi Xiaotong,Chen Shuang","Du TM,Zhang YC,Shi XT,Chen S",Du TM,,Beijing University of Posts & Telecommunications,"Magnetic resonance imaging (MRI) has been one of the most powerful and valuable imaging methods for medical diagnosis and staging of disease. Due to the long scan time of MRI acquisition, k-space under-samplings is required during the acquisition processing. Thus, MRI reconstruction, which transfers undersampled k-space data to high-quality magnetic resonance imaging, becomes an important and meaningful task. There have been many explorations on k-space interpolation for MRI reconstruction. However, most of these methods ignore the strong correlation between target slice and its adjacent slices. Inspired by this, we propose a fully data-driven deep learning algorithm for k-space interpolation, utilizing the correlation information between the target slice and its neighboring slices. A novel network is proposed, which models the inter-dependencies between different slices. In addition, the network is easily implemented and expended. Experiments show that our methods consistently surpass existing image-domain and k-space-domain magnetic resonance imaging reconstructing methods.",FRAMEWORK,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,FRAMEWORK,,,
80,Unsupervised stratification in neuroimaging through deep latent embeddings,,,1568-1571,"Dimitri Giovanna Maria,Spasov Simeon,Duggento Andrea,Passamonti Luca,Lio' Pietro,Toschi Nicola","Dimitri GM,Spasov S,Duggento A,Passamonti L,Lio' P,Toschi N",Dimitri GM,,University of Cambridge,"There is growing evidence that the use of stringent and dichotomic diagnostic categories in many medical disciplines (particularly `brain sciences' as neurology and psychiatry) is an oversimplification. Although clear diagnostic boundaries remain useful for patients, families, and their access to dedicated NHS and health care services, the traditional dichotomic categories are not helpful to describe the complexity and large heterogeneity of symptoms across many and overlapping clinical phenotypes. With the advent of 'big' multimodal neuroimaging databases, data-driven stratification of the wide spectrum of healthy human physiology or disease based on neuroimages is theoretically become possible. However, this conceptual framework is hampered by severe computational constraints. In this paper we present a novel, deep learning based encode-decode architecture which leverages several parameter efficiency techniques generate latent deep embedding which compress the information contained in a full 3D neuroimaging volume by a factor 1000 while still retaining anatomical detail and hence rendering the subsequent stratification problem tractable. We train our architecture on 1003 brain scan derived from the human connectome project and demonstrate the faithfulness of the obtained reconstructions. Further, we employ a data driven clustering technique driven by a grid search in hyperparameter space to identify six different strata within the 1003 healthy community dwelling individuals which turn out to correspond to highly significant group differences in both physiological and cognitive data. Indicating that the well-known relationships between such variables and brain structure can be probed in an unsupervised manner through our novel architecture and pipeline. This opens the door to a variety of previously inaccessible applications in the realm of data driven stratification of large cohorts based on neuroimaging data.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,,
81,Generating X-ray Images from Point Clouds Using Conditional Generative Adversarial Networks,,,1588-1591,"Haiderbhai Mustafa,Ledesma Sergio,Navab Nassir,Fallavollita Pascal","Haiderbhai M,Ledesma S,Navab N,Fallavollita P",Haiderbhai M,,University of Ottawa,"Simulating medical images such as X-rays is of key interest to reduce radiation in non-diagnostic visualization scenarios. Past state of the art methods utilize ray tracing, which is reliant on 3D models. To our knowledge, no approach exists for cases where point clouds from depth cameras and other sensors are the only input modality. We propose a method for estimating an X-ray image from a generic point cloud using a conditional generative adversarial network (CGAN). We train a CGAN pix2pix to translate point cloud images into X-ray images using a dataset created inside our custom synthetic data generator. Additionally, point clouds of multiple densities are examined to determine the effect of density on the image translation problem. The results from the CGAN show that this type of network can predict X-ray images from points clouds. Higher point cloud densities outperformed the two lowest point cloud densities. However, the networks trained with high-density point clouds did not exhibit a significant difference when compared with the networks trained with medium densities. We prove that CGANs can be applied to image translation problems in the medical domain and show the feasibility of using this approach when 3D models are not available. Further work includes overcoming the occlusion and quality limitations of the generic approach and applying CGANs to other medical image translation problems.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,,
82,3-To-1 Pipeline: Restructuring Transfer Learning Pipelines for Medical Imaging Classification via Optimized GAN Synthetic Images,,,1596-1599,"Choong Ross Zhi Jian,Harding Seth Austin,Tang Bo-yen,Liao Shih-wei","Choong RZJ,Harding SA,Tang BY,Liao SW",Choong RZJ,,National Taiwan University,"The difficulty of applying deep learning algorithms to biomedical imaging systems arises from a lack of training images. An existing workaround to the lack of medical training images involves pre-training deep learning models on ImageNet, a non-medical dataset with millions of training images. However, the modality of ImageNet's dataset samples consisting of natural images in RGB frequently differs from the modality of medical images, consisting largely of images in grayscale such as X-ray and MRI scan imaging. While this method may be effectively applied to non-medical tasks such as human face detection, it proves ineffective in many areas of medical imaging. Recently proposed generative models such as Generative Adversarial Networks (GANs) are able to synthesize new medical images. By utilizing generated images, we may overcome the modality gap arising from current transfer learning methods. In this paper, we propose a training pipeline which outperforms both conventional GAN-synthetic methods and transfer learning methods.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,,
83,Generating Hyperspectral Skin Cancer Imagery using Generative Adversarial Neural Network,,,1600-1603,"Annala Leevi,Neittaanmaki Noora);,Paoli John,Zaar Oscar,Polonen Ilkka","Annala L,Neittaanmaki N,Paoli J,Zaar O,Polonen I",Annala L,,University of Jyvaskyla,"In this study we develop a proof of concept of using generative adversarial neural networks in hyperspectral skin cancer imagery production. Generative adversarial neural network is a neural network, where two neural networks compete. The generator tries to produce data that is similar to the measured data, and the discriminator tries to correctly classify the data as fake or real. This is a reinforcement learning model, where both models get reinforcement based on their performance. In the training of the discriminator we use data measured from skin cancer patients. The aim for the study is to develop a generator for augmenting hyperspectral skin cancer imagery.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,https://jyx.jyu.fi/bitstream/123456789/72130/1/root.pdf,
84,X-ray2Shape: Reconstruction of 3D Liver Shape from a Single 2D Projection Image,,,1608-1611,"Tong Fei,Nakao Megumi,Wu Shuqiong,Nakamura Mitsuhiro,Matsuda Tetsuya","Tong F,Nakao M,Wu SQ,Nakamura M,Matsuda T",Tong F,,Kyoto University,"Computed tomography (CT) and magnetic resonance imaging (MRI) scanners measure three-dimensional (3D) images of patients. However, only low-dimensional local two-dimensional (2D) images may be obtained during surgery or radiotherapy. Although computer vision techniques have shown that 3D shapes can be estimated from multiple 2D images, shape reconstruction from a single 2D image such as an endoscopic image or an X-ray image remains a challenge. In this study, we propose X-ray2Shape, which permits a deep learning-based 3D organ mesh to be reconstructed from a single 2D projection image. The method learns the mesh deformation from a mean template and deep features computed from the individual projection images. Experiments with organ meshes and digitally reconstructed radiograph (DRR) images of abdominal regions were performed to confirm the estimation performance of the methods.",BEAM COMPUTED-TOMOGRAPHY,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,"BEAM,COMPUTED-TOMOGRAPHY",,https://repository.kulib.kyoto-u.ac.jp/dspace/bitstream/2433/265386/1/EMBC44109.2020.9176655.pdf,
85,AEC-Net: Attention and Edge Constraint Network for Medical Image Segmentation,,,1616-1619,"Wang Jingyi,Zhao Xu,Ning Qingtian,Qian Dahong","Wang JY,Zhao X,Ning QT,Qian DH",Zhao X,,Shanghai Jiao Tong University,"Semantic segmentation is a fundamental and challenging problem in medical image analysis. At present, deep convolutional neural network plays a dominant role in medical image segmentation. The existing problems of this field are making less use of image information and learning few edge features, which may lead to the ambiguous boundary and inhomogeneous intensity distribution of the result. Since the characteristics of different stages are highly inconsistent, these two cannot be directly combined. In this paper, we proposed the Attention and Edge Constraint Network (AEC-Net) to optimize features by introducing attention mechanisms in the lower-level features, so that it can be better combined with higher-level features. Meanwhile, an edge branch is added to the network which can learn edge and texture features simultaneously. We evaluated this model on three datasets, including skin cancer segmentation, vessel segmentation, and lung segmentation. Results demonstrate that the proposed model has achieved state-of-the-art performance on all datasets.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,,
86,Deeply Supervised Active Learning for Finger Bones Segmentation,,,1620-1623,"Zhao Ziyuan,Yang Xiaoyan,Veeravalli Bharadwaj,Zeng Zeng","Zhao ZY,Yang XY,Veeravalli B,Zeng Z",Zeng Z,,Agency for Science Technology & Research (ASTAR),"Segmentation is a prerequisite yet challenging task for medical image analysis. In this paper, we introduce a novel deeply supervised active learning approach for finger bones segmentation. The proposed architecture is fine-tuned in an iterative and incremental learning manner. In each step, the deep supervision mechanism guides the learning process of hidden layers and selects samples to be labeled. Extensive experiments demonstrated that our method achieves competitive segmentation results using less labeled samples as compared with full annotation.",NETWORK,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,NETWORK,,http://arxiv.org/pdf/2005.03225,
87,Adipose Tissue Segmentation in Unlabeled Abdomen MRI using Cross Modality Domain Adaptation,,,1624-1628,"Masoudi Samira,Anwar Syed M.,Harmon Stephanie A.,Choyke Peter L.,Turkbey Baris,Bagci Ulas","Masoudi S,Anwar SM,Harmon SA,Choyke PL,Turkbey B,Bagci U",Harmon SA,,National Institutes of Health (NIH) - USA,"Abdominal fat quantification is critical since multiple vital organs are located within this region. Although computed tomography (CT) is a highly sensitive modality to segment body fat, it involves ionizing radiations which makes magnetic resonance imaging (MRI) a preferable alternative for this purpose. Additionally, the superior soft tissue contrast in MRI could lead to more accurate results. Yet, it is highly labor intensive to segment fat in MRI scans. In this study, we propose an algorithm based on deep learning technique(s) to automatically quantify fat tissue from MR images through a cross modality adaptation. Our method does not require supervised labeling of MR scans, instead, we utilize a cycle generative adversarial network (C-GAN) to construct a pipeline that transforms the existing MR scans into their equivalent synthetic CT (s-CT) images where fat segmentation is relatively easier due to the descriptive nature of HU (hounsfield unit) in CT images. The fat segmentation results for MRI scans were evaluated by expert radiologist. Qualitative evaluation of our segmentation results shows average success score of 3.80/5 and 4.54/5 for visceral and subcutaneous fat segmentation in MR images*.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,http://arxiv.org/pdf/2005.05761,
88,Bladder Wall Segmentation in MRI Images via Deep Learning and Anatomical Constraints,,,1629-1632,"Li Ruikun,Chen Huai,Gong Guanzhong,Wang Lisheng","Li RK,Chen HA,Gong GZ,Wang LS",Wang LS,,Shanghai Jiao Tong University,"Segmenting the bladder wall from MRI images is of great significance for the early detection and auxiliary diagnosis of bladder tumors. However, automatic bladder wall segmentation is challenging due to weak boundaries and diverse shapes of bladders. Level-set-based methods have been applied to this task by utilizing the shape prior of bladders. However, it is a complex operation to adjust multiple parameters manually, and to select suitable hand-crafted features. In this paper, we propose an automatic method for the task based on deep learning and anatomical constraints. First, the autoencoder is used to model anatomical and semantic information of bladder walls by extracting their low dimensional feature representations from both MRI images and label images. Then as the constraint, such priors are incorporated into the modified residual network so as to generate more plausible segmentation results. Experiments on 1092 MRI images shows that the proposed method can generate more accurate and reliable results comparing with related works, with a dice similarity coefficient (DSC) of 85.48%.",NETWORKS,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,NETWORKS,,,
89,MRI-SegFlow: a novel unsupervised deep learning pipeline enabling accurate vertebral segmentation of MRI images,,,1633-1636,"Kuang Xihe,Cheung Jason P. Y.,Wu Honghan,Dokos Socrates,Zhang Teng","Kuang XH,Cheung JPY,Wu HH,Dokos S,Zhang T",Kuang XH,,University of Hong Kong,"Most deep learning based vertebral segmentation methods require laborious manual labelling tasks. We aim to establish an unsupervised deep learning pipeline for vertebral segmentation of MR images. We integrate the sub-optimal segmentation results produced by a rule-based method with a unique voting mechanism to provide supervision in the training process for the deep learning model. Preliminary validation shows a high segmentation accuracy achieved by our method without relying on any manual labelling.
The clinical relevance of this study is that it provides an efficient vertebral segmentation method with high accuracy. Potential applications are in automated pathology detection and vertebral 3D reconstructions for biomechanical simulations and 3D printing, facilitating clinical decision making, surgical planning and tissue engineering.","FRAMEWORK,SPINE",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,"FRAMEWORK,SPINE",,https://discovery.ucl.ac.uk/10131711/1/EMBC20_EMBC_paper_final_amended.pdf,
90,Deep Learning with Skip Connection Attention for Choroid Layer Segmentation in OCT Images,,,1641-1645,"Mao Xiaoqian,Zhao Yitian,Chen Bang,Ma Yuhui,Gu Zaiwang,Gu Shenshen);,Yang Jianlong,Cheng Jun,Liu Jiang","Mao XQ,Zhao YT,Chen B,Ma YH,Gu ZW,Gu SS,Yang JL,Cheng J,Liu J",Mao XQ,,Shanghai University,"Since the thickness and shape of the choroid layer are indicators for the diagnosis of several ophthalmic diseases, the choroid layer segmentation is an important task. There exist many challenges in segmentation of the choroid layer. In this paper, in view of the lack of context information due to the ambiguous boundaries, and the subsequent inconsistent predictions of the same category targets ascribed to the lack of context information or the large regions, a novel Skip Connection Attention (SCA) module which is integrated into the U-Shape architecture is proposed to improve the precision of choroid layer segmentation in Optical Coherence Tomography (OCT) images. The main function of the SCA module is to capture the global context in the highest level to provide the decoder with stage-by-stage guidance, to extract more context information and generate more consistent predictions for the same class targets. By integrating the SCA module into the U-Net and CE-Net, we show that the module improves the accuracy of the choroid layer segmentation.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,,
91,Extracting Membrane Borders in IVUS Images Using a Multi-Scale Feature Aggregated U-Net,,,1650-1653,"Xia Menghua,Yan Wenjun,Huang Yi,Guo Yi,Zhou Guohui,Wang Yuanyuan","Xia MH,Yan WJ,Huang Y,Guo Y,Zhou GH,Wang YY",Wang YY,,Fudan University,"Automatic extraction of the lumen-intima border (LIB) and the media-adventitia border (MAB) in intravascular ultrasound (IVUS) images is of high clinical interest. Despite the superior performance achieved by deep neural networks (DNNs) on various medical image segmentation tasks, there are few applications to IVUS images. The complicated pathological presentation and the lack of enough annotation in IVUS datasets make the learning process challenging. Several existing networks designed for IVUS segmentation train two groups of weights to detect the MAB and LIB separately. In this paper, we propose a multi- scale feature aggregated U-Net (MFAU-Net) to extract two membrane borders simultaneously. The MFAU-Net integrates multi-scale inputs, the deep supervision, and a bi-directional convolutional long short-term memory (BConvLSTM) unit. It is designed to sufficiently learn features from complicated IVUS images through a small number of training samples. Trained and tested on the publicly available IVUS datasets, the MFAU-Net achieves both 0.90 Jaccard measure (JM) for the MAB and LIB detection on 20 MHz dataset. The corresponding metrics on 40 MHz dataset are 0.85 and 0.84 JM respectively. Comparative evaluations with state-of-the-art published results demonstrate the competitiveness of the proposed MFAU-Net.","Intravascular ultrasound,border detection,deep neural networks,ConvLSTM",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,SEGMENTATION,,,
92,Efficient Deep Learning-based Wound-bed Segmentation For Mobile Applications,,,1654-1657,"Ong Ee Ping,Yin Christina Tang Ka,Lee Beng-Hai","Ong EP,Yin CTK,Lee BH",Ong EP,,Agency for Science Technology & Research (ASTAR),"This paper proposes a deep learning image segmentation method for the purpose of segmenting wound-bed regions from the background. Our contributions include proposing a fast and efficient convolutional neural networks (CNN)-based segmentation network that has much smaller number of parameters than U-Net (only 18.1% that of U-Net, and hence the trained model has much smaller file size as well). In addition, the training time of our proposed segmentation network (for the base model) is only about 40.2% of that needed to train a U-Net. Furthermore, our proposed base model also achieved better performance compared to that of the U-Net in terms of both pixel accuracy and intersection-over- union segmentation evaluation metrics. We also showed that because of the small footprint of our efficient CNN-based segmentation model, it could be deployed to run in real-time on portable and mobile devices such as an iPad.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,,
93,Comparison of feature selection in radiomics for the prediction of overall survival after radiotherapy for hepatocellular carcinoma,,,1667-1670,"Fontaine Pierre,Riet Francois-Georges,Castelli Joel,Gnep Khemara,Depeursinge Adrien,De Crevoisier Renaud,Acosta Oscar","Fontaine P,Riet FG,Castelli J,Gnep K,Depeursinge A,De Crevoisier R,Acosta O",Fontaine P,,Institut National de la Sante et de la Recherche Medicale (Inserm),"Hepatocellular carcinoma (HCC) is the sixth more frequent cancer worldwide. This type of cancer has a poor overall survival rate mainly due to underlying cirrhosis and risk of recurrence outside the treated lesion. Quantitative imaging within a radiomics workflow may help assessing the probability of survival and potentially may allow tailoring personalized treatments. In radiomics a large amount of features can be extracted, which may be correlated across a population and very often can be surrogates of the same physiopathology. This issues are more pronounced and difficult to tackle with imbalanced data. Feature selection strategies are therefore required to extract the most informative with the increased predictive capabilities. In this paper, we compared different unsupervised and supervised strategies for feature selection in presence of imbalanced data and optimize them within a machine learning framework. Multi-parametric Magnetic Resonance Images from 81 individuals (19 deceased) treated with stereotactic body radiation therapy (SBRT) for inoperable (HCC) were analyzed. Pre-selection of a reduced set of features based on Affinity Propagation clustering (non supervised) achieved a significant improvement in AUC compared to other approaches with and without feature pre-selection. By including the synthetic minority over-sampling technique (SMOTE) for imbalanced data and Random Forest classification this workflow emerges as an appealing feature selection strategy for survival prediction within radiomics studies.","Radiomics,Feature selection,Liver cancer,survival prediction,SMOTE,Multi-parametric MRI",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,RADIATION-THERAPY,,,
94,Automated Detection of Juvenile Myoclonic Epilepsy using CNN based Transfer Learning in Diffusion MRI,,,1679-1682,"Si Xiaopeng,Zhang Xingjian,Zhou Yu,Sun Yulin,Jin Weipeng,Yin Shaoya,Zhao Xin,Li Qiang,Ming Dong","Si XP,Zhang XJ,Zhou Y,Sun YL,Jin WP,Yin SY,Zhao X,Li Q,Ming D",Si XP,,Tianjin University,"Epilepsy is one of the largest neurological diseases in the world, and juvenile myoclonic epilepsy (JME) usually occurs in adolescents, giving patients tremendous burdens during growth, which really needs the early diagnosis. Advanced diffusion magnetic resonance imaging (MRI) could detect the subtle changes of the white matter, which could be a non-invasive early diagnosis biomarker for JME. Transfer learning can solve the problem of insufficient clinical samples, which could avoid overfitting and achieve a better detection effect. However, there is almost no research to detect JME combined with diffusion MRI and transfer learning. In this study, two advanced diffusion MRI methods, high angle resolved diffusion imaging (HARDI) and neurite orientation dispersion and density imaging (NODDI), were used to generate the connectivity matrix which can describe tiny changes in white matter. And three advanced convolutional neural networks (CNN) based transfer learning were applied to detect JME. A total of 30 participants (15 JME patients and 15 normal controls) were analyzed. Among the three CNN models, Inception_resnet_v2 based transfer learning is better at detecting JME than Inception_v3 and Inception_v4, indicating that the ""short cut"" connection can improve the ability to detect JME. Inception_resnet_v2 achieved to detect JME with the accuracy of 75.2% and the AUC of 0.839. The results support that diffusion MRI and CNN based transfer learning have the potential to improve the automated detection of JME.","transfer learning,convolutional neural network,diffusion MRI,juvenile myoclonic epilepsy",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,,
95,Multi-Shell D-MRI Reconstruction via Residual Learning utilizing Encoder-Decoder Network with Attention (MSR-Net),,,1709-1713,"Jha Ranjeet Ranjan,Nigam Aditya,Bhavsar Arnav,Pathak Sudhir K.,Schneider Walter,Rathish K.","Jha RR,Nigam A,Bhavsar A,Pathak SK,Schneider W,Rathish K",Jha RR,,Indian Institute of Technology System (IIT System),"Contemporary diffusion MRI based analysis with HARDI, which provides more accurate fiber orientation, can be performed using single or multiple b-values (single or multi-shell). Single shell HARDI cannot provide volume fraction for different tissue types, which can produce bias and noisier results in estimation of fiber ODF. Multi-shell acquisition can resolve this issue. However, it requires more scanning time and is therefore not very well suited in clinical setting. Considering this, we propose a novel deep learning architecture, MSR-Net, for reconstruction of diffusion MRI volumes for some b-value using acquisitions at another b-value. In this work, we demonstrate this for b = 2000 s/mm(2) and b = 1000 s/mm(2). We learn such a transformation in the space of spherical harmonic coefficients. The proposed network consists of encoder-decoder along-with an attention module and a feature module. We have considered L2 and Content loss for optimizing and improving the performance. We have trained and validated the network using the HCP data-set with standard qualitative and quantitative performance measures.","Diffusion MRI,Multi-shell HARDI,Encoder-Decoder,Content Loss,Attention module,Feature module",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,,
96,Diagnosis of Myotonic Dystrophy Based on Resting State fMRI Using Convolutional Neural Networks,,,1714-1717,"Kamali Tahereh,Hagerman Katharine A.,Day John W.,Sampson Jacinda,Lim Kelvin O.,Mueller Bryon A.,Wozniak Jeffrey","Kamali T,Hagerman KA,Day JW,Sampson J,Lim KO,Mueller BA,Wozniak J",Kamali T,,Stanford University,"Myotonic dystrophies (DM) are neuromuscular conditions that cause widespread effects throughout the body. There are brain white matter changes on MRI in patients with DM that correlate with neuropsychological functional changes. How these brain alterations causally relate to the presence and severity of cognitive symptoms remains largely unknown. Deep neural networks have significantly improved the performance of image classification of huge datasets. However, its application in brain imaging is limited and not well described, due to the scarcity of labeled training data. In this work, we propose an approach for the diagnosis of DM based on a spatio-temporal deep learning paradigm. The obtained accuracy (73.71%) and sensitivities and specificities showed that the implemented approach based on 4-D convolutional neural networks leads to a compact, discriminative, and fast computing DM-based clinical medical decision support system.","WHITE-MATTER ABNORMALITIES,TYPE-1",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,"WHITE-MATTER,ABNORMALITIES,TYPE-1",,,
97,Brain MRI-based 3D Convolutional Neural Networks for Classification of Schizophrenia and Controls,,,1742-1745,"Hu Mengjiao,Sim Kang,Zhou Juan Helen,Jiang Xudong,Guan Cuntai","Hu MJ,Sim K,Zhou JH,Jiang XD,Guan CT",Hu MJ,,Nanyang Technological University & National Institute of Education (NIE) Singapore,"Convolutional Neural Network (CNN) has been successfully applied on classification of both natural images and medical images but limited studies applied it to differentiate patients with schizophrenia from healthy controls. Given the subtle, mixed, and sparsely distributed brain atrophy patterns of schizophrenia, the capability of automatic feature learning makes CNN a powerful tool for classifying schizophrenia from controls as it removes the subjectivity in selecting relevant spatial features. To examine the feasibility of applying CNN to classification of schizophrenia and controls based on structural Magnetic Resonance Imaging (MRI), we built 3D CNN models with different architectures and compared their performance with a handcrafted feature-based machine learning approach. Support vector machine (SVM) was used as classifier and Voxel-based Morphometry (VBM) was used as feature for handcrafted feature-based machine learning. 3D CNN models with sequential architecture, inception module and residual module were trained from scratch. CNN models achieved higher cross-validation accuracy than handcrafted feature-based machine learning. Moreover, testing on an independent dataset, 3D CNN models greatly outperformed handcrafted feature-based machine learning. This study underscored the potential of CNN for identifying patients with schizophrenia using 3D brain MR images and paved the way for imaging-based individual-level diagnosis and prognosis in psychiatric disorders.",,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,,,http://arxiv.org/pdf/2003.08818,
98,Glioma Growth Prediction via Generative Adversarial Learning from Multi-Time Points Magnetic Resonance Images,,,1750-1753,"Elazab Ahmed,Wang Changmiao,Gardezi Syed Jamal Safdar,Bai Hongmin,Wang Tianfu,Lei Baiying,Chang Chunqi","Elazab A,Wang CM,Gardezi SJS,Bai HM,Wang TF,Lei BY,Chang CQ",Chang CQ,,Shenzhen University,"Gliomas are the most dominant and lethal type of brain tumors. Growth prediction is significant to quantify tumor aggressiveness, improve therapy planning, and estimate patients' survival time. This is commonly addressed in literature using mathematical models guided by multi-time point scans of multi/ single-modal data for the same subject. However, these models are mechanism-based and heavily rely on complicated mathematical formulations of partial differential equations with few parameters that are insufficient to capture different patterns and other characteristics of gliomas. In this paper, we propose a 3D generative adversarial networks (GANs) for glioma growth prediction. Specifically, we stack 2 GANs with conditional initialization of segmented feature maps. Furthermore, we employ Dice loss in our objective function and devised 3D U-Net architecture for better image generation. The proposed method is trained and validated using 3D patch-based strategy on real magnetic resonance images of 9 subjects with 3 time points. Experimental results show that the proposed method can be successfully used for glioma growth prediction with satisfactory performance.","Glioma growth prediction,magnetic resonance images,generative adversarial networks,Dice loss,3D U-Net",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,"INVASION,MODEL",,,
99,U-net combined with CRF and anatomical based spatial features to segment white matter hyperintensities,,,1754-1757,"Zhou PengZheng,Liang Li,Guo Xutao,Lv Haiyan,Wang Tong,Ma Ting","Zhou PZ,Liang L,Guo XT,Lv HY,Wang T,Ma T",Ma T,,Harbin Institute of Technology,"White matter hyperintensities (WMH) are important biomarkers for cerebral small vessel disease and closely associated with other neurodegenerative process. In this paper, we proposed a fully automatic WMH segmentation method based on U-net architecture. CRF were combined with U-net to refine segmentation results. We used a new anatomical based spatial feature produced by brain tissue segmentation based on T1 image, along with intensities of T1 and T2-FLAIR images to train our neural network. We compared 8 forms of automated WMH segmentation methods, range from traditional statistical learnng methods to deep learning based methods, with different architecture and used different features. Results showed our proposed method achieved best performance in terms of most metrics, and the inclusion of anatomical based spatial features strongly increase the segmentation performance.",TOOL,Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,TOOL,,,
100,Training Deep Neural Networks for Small and Highly Heterogeneous MRI Datasets for Cancer Grading,,,1758-1761,"Wodzinski Marek,Banzato Tommaso,Atzori Manfredo,Andrearczyk Vincent,Cid Yashin Dicente,Mueller Henning","Wodzinski M,Banzato T,Atzori M,Andrearczyk V,Cid YD,Muller H",Wodzinski M,,AGH University of Science & Technology,"Using medical images recorded in clinical practice has the potential to be a game-changer in the application of machine learning for medical decision support. Thousands of medical images are produced in daily clinical activity. The diagnosis of medical doctors on these images represents a source of knowledge to train machine learning algorithms for scientific research or computer-aided diagnosis. However, the requirement of manual data annotations and the heterogeneity of images and annotations make it difficult to develop algorithms that are effective on images from different centers or sources (scanner manufacturers, protocols, etc.). The objective of this article is to explore the opportunities and the limits of highly heterogeneous biomedical data, since many medical data sets are small and entail a challenge for machine learning techniques. Particularly, we focus on a small data set targeting meningioma grading. Meningioma grading is crucial for patient treatment and prognosis. It is normally performed by histological examination but recent articles showed that it is possible to do it also on magnetic resonance images (MRI), so non-invasive. Our data set consists of 174 T1-weighted MRI images of patients with meningioma, divided into 126 benign and 48 atypical/anaplastic cases, acquired using 26 different MRI scanners and 125 acquisition protocols, which shows the enormous variability in the data set. The performed preprocessing steps include tumor segmentation, spatial image normalization and data augmentation based on color and affine transformations. The preprocessed cases are passed to a carefully trained 2D convolutional neural network. Accuracy above 74% was obtained, with the high-grade tumor recall above 74%. The results are encouraging considering the limited size and high heterogeneity of the data set. The proposed methodology can be useful for other problems involving classification of small and highly heterogeneous data sets.","deep learning,classification,grading,small data set,meningioma",Proceedings Paper,"IEEE, 345 E 47TH ST, NEW YORK, NY 10017 USA",Engineering,,,MENINGIOMAS,,,
